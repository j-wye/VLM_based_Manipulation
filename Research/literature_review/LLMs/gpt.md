알겠습니다. Jetson Orin AGX 64GB 기반 시스템에서 rigid body(산업용 부품 및 일상용품)을 대상으로, 6DOF 단순형 그리퍼(예: parallel-jaw gripper)를 사용하고, 전체 파이프라인이 최소 10fps 수준의 실시간 성능을 보장할 수 있는 open source 기반 grasping 알고리즘에 대한 최신 연구 자료를 정리하겠습니다. 특히 입력 데이터로 depth image, depth pointcloud, RGB + pointcloud 조합을 사용하는 알고리즘들의 성능 차이와 특성을 비교하고, pointcloud upsampling을 포함하는 방식이 실제 grasping 품질 향상에 기여할 수 있는지에 대해서도 조사하겠습니다.

조사가 완료되면 보고서 형식으로 정리하여 공유드리겠습니다.


# Jetson Orin에서의 실시간 6자유도 그립 포즈 추정 최신 연구 동향

## 개요 및 배경

실세계의 **6자유도 병렬 그리퍼 파지(Grasp) 자세 추정** 문제는 산업용 부품이나 일상용품과 같이 형태가 다양한 강체 객체를 로봇으로 잡기 위해 중요한 연구 분야입니다. 특히 단순한 2D 평면 상의 탑다운(top-down) 그립이 아닌, 임의의 공간 방향에서 \*\*병렬 양쪽 손가락(평행 그리퍼)\*\*으로 물체를 파지할 수 있는 6자유도(grasp의 위치 3축 + 방향 3축) 그립 포즈를 예측해야 합니다. 최근 2018년 이후 딥러닝을 활용한 데이터 기반 접근법들이 등장하여 미지의 물체에 대해서도 **심층 신경망을 통해 센서 데이터만으로 파지 자세를 산출**하는 방향으로 발전하고 있습니다. 이러한 접근은 복잡한 기하 모델링 없이도 다양한 물체에 일반화된 성능을 보이지만, **실시간성**과 **물체 간 잡기 충돌 회피** 등의 측면에서 도전과제가 있습니다.

본 보고서에서는 **Jetson Orin AGX (64GB)** 수준의 엣지 디바이스에서 동작 가능한 실시간 6자유도 파지 인식 시스템 구현을 위해, 최신 연구 동향을 다음 기준에 따라 정리합니다:

* **입력 데이터 유형**: 깊이 이미지(depth image), 3D 포인트클라우드(point cloud), RGB+D 등 **네트워크 입력으로 사용하는 데이터 종류**를 명확히 언급.
* **알고리즘 성능**: 2018년 이후 발표된 논문 중 **추론 속도 10fps 이상**의 실시간성을 보이거나, 모바일/엣지 사용을 고려하여 **경량화된 모델**을 제안한 사례.
* **오픈 소스 공개 여부**: 논문과 함께 코드가 공개되어 **GitHub 링크**를 제공하는 알고리즘만 선별.
* **논문 등급**: 가능하면 ICRA, IROS, CoRL, RSS, CVPR 등 **최상위 학회/저널**에 발표된 연구 위주.
* **핵심 처리 방식 및 특징**: 각 방법의 **그립 포즈 산출 방식**(예: 표본 샘플링+평가, 단일 단계 회귀 등)과 **사용하는 특징**(예: RGB 피처, 표면 법선/곡률 등의 기하 특징) 및 **멀티모달 융합 기법** 활용 여부.
* **엣지 디바이스 적용 가능성**: Jetson Orin AGX와 유사한 **임베디드 GPU 환경에서의 실험이나 언급**이 있는 경우 가산.

추가적으로, **포인트클라우드 기반 알고리즘의 경우 입력 밀도가 낮을 때의 성능 저하를 완화하기 위한 방법**으로 **점군 업샘플링(point cloud upsampling)** 기법(예: PU-Net, PU-GAN 등) 적용에 관한 연구 동향도 함께 조사합니다.

아래에서는 위 조건을 충족하는 대표적인 6자유도 그립 포즈 추정 알고리즘들을 개별적으로 요약하고, **특징과 성능 비교**, **엣지 디바이스에서의 구현 가능성**, **업샘플링 활용에 대한 논의**를 제공합니다.

## 주요 6자유도 그립 포즈 추정 알고리즘

### Volumetric Grasping Network (VGN, CoRL 2020)

* **입력 및 특징:** VGN은 **씬(scene)의 3D 부피 정보를 TSDF**(Truncated Signed Distance Function) 형태로 입력받는 접근법입니다. TSDF 볼륨이란 한 씬의 공간을 격자로 나누고 각 격자(voxel)에 최근접 표면까지의 부호 있는 거리를 저장한 것으로, 다수의 깊이 프레임을 통합해 **노이즈를 줄이면서** 물체 표면의 3차원 구조를 표현합니다. 별도의 RGB 정보 없이 **깊이에 기반한 기하학적 특징**만 활용하며, TSDF 자체가 표면의 존재 여부와 거리를 담고 있어 **표면 법선 등의 정보를 간접적으로 포함**합니다.

* **추론 방식:** 3D CNN 기반 **풀컨볼루션 네트워크**를 사용하여 입력 TSDF 격자와 동일 해상도의 **그립 품질(성공 확률), 그립 방향(쿼터니언), 그립 너비** 값을 출력합니다. 즉, **격자 공간상의 각 위치마다** 그립을 실행할 경우의 품질을 예측하므로, 장점은 **씬 전체에서 잠재적인 그립 지점을 한 번에 평가**한다는 점입니다. 네트워크 출력 후에는 \*\*비최대 억제(NMS)\*\*를 통해 인접 격자들 중 최고 점수를 가진 후보들만 추출하고, 충돌 가능성이 높은 그립은 걸러냅니다. 이러한 **전면 탐색식(voxel-wise) 접근**은 사전에 후보를 샘플링하는 단계를 없애 연산을 간소화하고, 3D CNN이 **격자 형태로 통합된 다중 뷰 정보를 학습**함으로써 추가적 충돌 검사 없이 **충돌 없는 파지 자세**를 바로 제시하는 장점을 갖습니다.

* **성능:** VGN의 큰 특징은 **실시간 속도**입니다. 시뮬레이션 및 실제 로봇 실험 결과, **GPU 환경에서 추론에 약 10ms**(0.01초)밖에 걸리지 않아 매우 빠른 속도를 달성했습니다. 이는 이전의 점군 기반 방법인 GPD가 동일 환경에서 **약 1.2초** 소요된 것과 대비되는 수치로, VGN은 GPD 대비 **100배 이상의 속도 개선**을 보였습니다. 이처럼 **약 100fps에 달하는 추론 속도**는 센서 피드백에 기반한 **클로즈드 루프 실시간 파지 제어**도 가능하게 합니다. 성능 면에서, 물체가 많은 난잡한 더미(clutter) 속에서도 실험을 통해 **92%의 물체를 성공적으로 제거**할 수 있음을 보였습니다.

* **평가 및 환경:** VGN은 시뮬레이션 데이터셋으로 학습한 후 실제 로봇에 적용하여 **별도 튜닝 없이도** 물체 잡기에 성공함을 보였고, 범용 GPU가 장착된 워크스테이션에서 실험되었습니다. Jetson Orin과 같은 엣지에서도 3D CNN 추론이 가능하다면, 수십 ms 수준의 추론 시간으로 **산업용 실시간 파지**에 충분히 적용 가능할 것으로 예상됩니다. 다만 TSDF 볼륨 생성을 위해 여러 뷰(depth 스트림 통합) 시 **추가 연산**이 필요하지만, 이는 ROS 등에서 병렬 처리로 보완 가능합니다.

* **오픈 소스:** VGN 알고리즘의 소스코드는 공개되어 있으며, **GitHub: `ethz-asl/vgn`** 레포지토리에서 제공됩니다. (CoRL 2020 논문 발표와 함께 코드와 시뮬레이션/로봇 실험 스크립트가 배포되었습니다.)

### PointNetGPD (ICRA 2019)

* **입력 및 특징:** PointNetGPD는 **단일 뷰의 3D 포인트클라우드**를 입력으로 사용합니다. 이때 물체 전체 점군이 아니라, 후보 그립 위치마다 **그립 너비 사이에 포획되는 국소 영역의 점군**을 고려합니다. 기본적인 파이프라인은 **기하학적 휴리스틱 기반 후보 생성 + 신경망 평가**로 구성되는데, 3D 센서로 획득한 점군에서 무작위 및 격자기반 표본 추출, 법선 추정 등을 통해 다수의 **그립 후보 자세**를 먼저 생성합니다. 그런 다음 각 후보의 **로컬 점군 패치**(그립 위치 주변의 점들)를 **PointNet** 계열 네트워크에 통과시켜 해당 그립의 성공 가능성을 **분류/평가**합니다. 이 접근은 후보가 주어졌을 때 **직접 3D 점셋을 입력**으로 처리하므로, 별도로 2D 이미지로 변환하거나 손수 설계한 특징을 계산할 필요가 없습니다. 특히 **PointNet 구조**의 이점으로 **입력 점의 순서에 불변**하며, 포인트 간 **대칭성과 3차원 기하 관계를 학습**하여 **접촉 영역의 복잡한 형상도 파악**할 수 있습니다.

* **추론 방식:** PointNetGPD의 핵심은 **학습 기반 그립 평가 모델**입니다. 즉, 그립 후보의 local 점군을 보고 그립이 성공적일지를 **이진 분류**하거나 점수화합니다. 논문에서는 이 평가망을 **경량 PointNet**으로 설계하여 파라미터 수가 약 **160만 개** 수준으로 작고, **입력 점군이 매우 희소한 경우에도** 접촉 영역 형태를 잘 포착한다고 보고했습니다. 이러한 평가 네트워크가 있다면, 생성된 수백\~수천개의 후보 중 **네트워크 판별 점수가 가장 높은** 자세를 최종 그립으로 선택합니다. PointNetGPD는 **표면 법선이나 곡률 등의 수작업 특징 없이**, **원시 점 좌표**만으로 학습했습니다. 단, 후보 생성 단계에서 **물체와 충돌하거나 테이블을 잡는 후보**는 사전에 제거하여 효율을 높입니다.

* **성능:** 정확도 측면에서 PointNetGPD는 기존 GPD 대비 향상된 성능을 보였으며, YCB 객체 세트 대상의 대규모 **실제 점군 데이터 35만 개**로 학습하여 **이질적인 새로운 물체에도 잘 일반화**함을 입증했습니다. 특히 **포인트클라우드 밀도가 낮거나 센서 오차가 있는 경우**에도 의미있는 평가를 수행하여, 불완전한 관측 하의 파지 계획에 강건함을 확인했습니다. 속도 측면에서는, 정확한 수치는 논문에 명시되지 않았으나 GPD의 경우 CPU 기반 1\~2초 수준이었던 데 반해, PointNetGPD는 **GPU 병렬처리**로 다수 후보를 동시에 평가하여 **수백 밀리초 수준으로 단축**할 수 있습니다. 예를 들어 후속 연구에서는 GPD가 20초 이상 걸리던 장면에서 PointNetGPD는 약 10초로 줄였다고 보고되었으나, 이는 Python 구현 및 CPU 후보생성 포함 시간이라 실제 최적화된 모델은 훨씬 빠르게 동작 가능합니다. 네트워크 자체는 1회 입력에 수 ms 정도이므로, **10Hz 이상의 처리율**도 달성 가능할 것으로 추정됩니다.

* **평가 및 환경:** 이 알고리즘은 시뮬레이터뿐 아니라 실제 Universal Robotics 암(robotic arm)으로 잡기 실험을 수행하여 성공률 향상을 보였습니다. 다만 **병렬 그리퍼의 길이/너비 등의 파라미터**에 민감하므로, 학습 시 사용한 그리퍼(예: 로봇 손가락 간 간격)에 맞춰야 합니다. Jetson Orin과 같은 엣지 디바이스에서는, **네트워크 추론은 비교적 가볍지만** 후보 생성의 병렬화와 점군 처리 최적화가 필요합니다. Orin의 GPU를 활용해 \*\*후보 생성(PCL 등의 라이브러리)\*\*과 \*\*평가망 추론(TensorRT 최적화)\*\*을 수행한다면 실시간 구현도 가능할 것으로 보입니다.

* **오픈 소스:** PointNetGPD의 소스코드와 학습 데이터 생성기가 공개되어 있으며 **GitHub: `lianghongzhuo/PointNetGPD`** 레포지토리에서 제공됩니다. 해당 코드에는 시뮬레이션 환경 및 YCB 객체 데이터셋으로 학습된 모델 가중치도 포함되어 있습니다.

### 6-DoF GraspNet (ICCV 2019, NVIDIA)

* **입력 및 특징:** 6-DoF GraspNet은 임의 물체의 \*\*부분 점군(단일 깊이 카메라로 본 점군)\*\*을 입력으로 받아 **다양한 그립 포즈를 생성 및 평가**하는 방식을 제안했습니다. 특징적으로 \*\*딥러닝 기반 **생성 모델** 개념을 도입하여, 그립 후보들을 네트워크가 **샘플링**하도록 합니다. 이를 위해 **변분 오토인코더(VAE)** 구조를 활용하여 학습된 **Grasp Sampler 네트워크**가 잠재 공간에서 다수의 그립 포즈를 생성합니다. 이렇게 생성된 후보들은 추가로 **Grasp Evaluator 네트워크**를 통해 **품질 평가 및 세부 refinement**를 거쳐 최종 출력됩니다. 두 네트워크 모두 **관측된 3D 점군**을 입력으로 사용하며, Sampler는 다양한 그립을 **확률적으로 샘플링**하고 Evaluator는 각 그립을 **성공 가능도 스코어**로 점수화하거나 미세 조정합니다.

* **추론 방식:** Grasp Sampler는 **잠재 벡터를 샘플링하여** 그에 대응하는 다수의 6자유도 그립을 일괄 생성합니다. 한 번의 forward pass로 **복수의 후보**를 뽑을 수도 있지만, 다양성 확보를 위해 여러 번 샘플링하기도 합니다. 그 후 Grasp Evaluator가 각 후보에 대하여 **충분히 물체를 파지할 수 있는지** 분류하거나 점수를 부여합니다. 경우에 따라 evaluator의 출력 및 gradient를 이용해 sampler가 생성한 포즈를 **미세하게 수정**(refine)하는 피드백을 거쳐 정확도를 높이기도 합니다. 이처럼 **“생성 + 평가” 이단 구조**를 통해 **탐색(space of grasps)과 평가 기능을 분리**하였으며, 복잡한 6자유도 출력 공간에서 **생성망이 다양한 초기값**을 제시하면 **평가망이 걸러내는** 형태로 효율을 높였습니다. 입력 특징으로는 주로 **점의 3D 좌표**를 사용하고, PointNet++ 등의 **ポイント클라우ド 임베딩**을 내부에서 활용한 것으로 추정됩니다 (세부 구현은 TensorFlow로 되어 있습니다).

* **성능:** 시뮬레이션 기반으로 학습된 모델을 실제 로봇에 적용한 결과, **88%의 그립 성공률**을 보였으며 다양한 모양, 크기, 무게의 물체에 대응할 수 있음을 검증했습니다. 특히 **도메인 랜덤화** 등을 통해 **시뮬레이션만으로 학습하고 실제에서 추가 학습 없이도 동작**하는 점을 강조하였습니다. 다만 속도 면에서는, 논문에서 구체적인 추론 시간은 언급되지 않았습니다. 두 개의 신경망과 다수 샘플링이 필요하므로, **단일 GPU 기준 수백 ms 정도**로 추정됩니다. 실제 NVIDIA 연구진이 개발한 만큼 고성능 GPU에서는 수 초당 여러 grasp를 평가 가능하지만, **Jetson급에서는 최적화 없이는 10fps에 못 미칠 가능성**이 있습니다.

* **평가 및 환경:** 이 연구는 **ICCV 2019**에 발표되며, 컴퓨터 비전 커뮤니티에서도 주목을 받았습니다. 실험에는 프랑카(Franka) 연구용 로봇팔이 사용되었고, 물체 집기 성공률을 측정했습니다. Jetson Orin과 같은 엣지 디바이스에 적용하려면 두 네트워크를 경량화하거나 TensorRT 최적화가 필요하며, 특히 **TensorFlow 1.x 기반** 구현을 이식하는 작업이 선행되어야 합니다. Orin의 64GB 메모리는 충분하나, **연산량이 많아** 실시간 요구사항(>=10Hz)을 만족하려면 **샘플링 횟수를 줄이거나** 앙상블을 축소하는 등의 트레이드오프가 필요할 수 있습니다.

* **오픈 소스:** 6-DoF GraspNet의 공식 구현은 \*\*GitHub: `NVlabs/6dof-graspnet`\*\*으로 공개되어 있습니다. 해당 저장소에는 학습된 모델과 실험에 사용된 데이터 준비 방법이 포함되어 있어, 연구자들이 재현 실험을 해볼 수 있습니다.

### Contact-GraspNet (ICRA 2021, NVIDIA)

* **입력 및 특징:** Contact-GraspNet은 **단일 깊이 맵으로부터 얻은 3D 점군**을 입력으로 사용하며, 복잡한 장면(clutter)에서도 **효율적으로 다수의 6자유도 그립을 직접 예측**하는 알고리즘입니다. 이 방법은 **“컨택(contact) 기반” 표현**을 도입하여 그립을 정의하는 차원을 줄였는데, **물체 표면 상의 한 접촉점**을 기준으로 그립을 파라미터화함으로써 6자유도 문제를 실질적으로 **4자유도**로 감소시켰다고 설명합니다. 즉, 한쪽 그리퍼 손가락이 닿을 표면 접촉점을 결정하면, 그 점에서의 법선 방향을 따라 접근하도록 그립 자세 일부가 결정되고, 남은 회전 자유도와 그립 폭만 최적화하면 되므로 탐색 공간이 작아집니다. Contact-GraspNet은 **신경망이 직접 이러한 접촉점과 대응 그립 자세 분포를 예측**하도록 학습됩니다.

* **추론 방식:** 네트워크는 입력 점군을 받아 **충분히 잡기 좋은 접촉점들의 분포와 그립 방향**을 출력합니다. 구체적인 아키텍처로는 PointNet++ 계열의 포인트 기반 신경망을 활용하여, **포인트클라우드 전체에 대한 임베딩**을 구한 후 다수의 접촉 후보를 추출하는 것으로 알려져 있습니다. 이후 **fine-tuning** 단계 없이도 곧바로 다양한 그립 포즈를 얻을 수 있으며, 물체간 **충돌 여부는 학습 데이터 상에서 간접적으로 고려**되었으나 더욱 **dense한 후보를 얻기 위해 한 장면에 대해 여러 회의 forward 연산**을 수행하기도 합니다. Contact-GraspNet의 또다른 실용적인 특징은 **물체 분할(sementation) 정보가 주어질 경우** 이를 활용해 **물체별로 지역 점군을 잘라내어** 그립을 필터링할 수 있다는 점입니다. 이는 만일 장면에 여러 물체가 쌓여 있을 때, 배경이나 다른 물체를 관통하는 그립을 제거하여 **물체별 유효한 그립셋**을 구하는 데 유용합니다.

* **성능:** ICRA 2021 발표 당시 이 방법은 **대규모 합성 데이터셋(ACRONYM 등)으로 학습**되어 다양한 물체에서 높은 grasp 성공률을 보였으며, 특히 **복잡한 적재 환경에서 기존 방법들보다 안정적인 성능**을 달성했습니다. 정확한 수치는 논문에서 90% 이상의 성공률, 80% 이상의 완료율 등을 보고하였습니다. **속도 측면**에서는 “Efficient”란 명칭에도 불구하고, GPU에서 동작 시 한 장면당 **수백 ms 수준**으로 추정됩니다. 실제 GitHub 예제에서 하나의 장면에 대해 기본 **5회 forward pass**를 수행하도록 되어 있는데, 만약 1회 연산이 100ms이라면 5회로 0.5초 가량 소요될 수 있습니다. 다만 이는 다양한 grasp 후보 생성을 위한 것이며, **속도가 필요하면 forward pass 횟수를 줄이거나 물체 분할을 통해 국소 영역만 입력**하여 가속할 수 있습니다. 또한 후속 연구에서 \*\*최적화된 변형(vMF-Contact 등)\*\*을 통해 10Hz 이상의 속도를 보인 사례도 언급되는 등, 추론 시간은 지속 개선되고 있습니다.

* **평가 및 환경:** Contact-GraspNet 역시 시뮬레이션 후 실제 로봇 실험으로 검증되었으며, **물체 분할 네트워크(UOIS)와 결합한 시스템 데모**를 통해 난잡한 상자더미에서 물체를 집어내는 시연을 보였습니다. Jetson Orin과 같은 엣지 환경에서는, 이 모델이 TensorFlow 2 기반이므로 호환성 문제를 해결하고 연산 커널을 최적화해야 하지만, **8GB 이상의 VRAM에서 구동 가능**하고 일부 포인트 연산은 CUDA 가속을 사용하므로 Orin에서도 동작은 가능합니다. 다만 실시간으로 사용하려면 \*\*추론 엔진 변환(TensorRT)\*\*과 **배치 감소** 등이 필요할 수 있습니다.

* **오픈 소스:** Contact-GraspNet은 \*\*GitHub: `NVlabs/contact_graspnet`\*\*으로 공식 구현이 공개되어 있습니다. 제공된 코드에는 학습된 모델 체크포인트, 테스트 데이터, 그리고 Segmentation을 활용한 후처리 예시 등이 포함되어 있어, 연구자들이 손쉽게 활용할 수 있습니다.

### REGNet (ICRA 2021, REgion-based Grasp Network)

* **입력 및 특징:** REGNet은 **단일 시점(point of view)의 점군**만으로 **엔드투엔드 6자유도 파지 추정**을 구현한 방법으로, 이름처럼 **영역(region) 단위의 그립 후보 생성** 전략을 도입했습니다. 입력은 깊이 카메라 한 대로 얻은 물체들의 부분 점군이며, 별도 RGB 정보 없이 **기하학적 입력**만 이용합니다. 네트워크는 크게 세 단계로 구성되는데: (1) **점 별 스코어 예측(SN: Score Network)** – 각 3D 점이 좋은 그립 위치인지 신뢰도를 회귀하여 **유망한 점들을 선택**하고, (2) **그립 영역 예측(GRN: Grasp Region Network)** – 선택된 상위 점들마다 그 주변의 **그립 후보 여러 개**를 예측하며(각 점을 중심으로 여러 orientation anchor 기반 후보 산출), (3) **자세 정련(RN: Refine Network)** – 생성된 후보들의 자세를 미세 조정 및 평가하여 최종 결과를 산출합니다. 이러한 **스코어+영역+정련 3단계** 구조는, 전부 딥러닝으로 통합돼있지만 모듈별로 기능이 나뉘어 **효과적으로 탐색과 평가를 수행**합니다. 특징적으로 **Grasp Anchor**라는 개념을 도입하여, 미리 정해둔 여러 그립 방향(예: 법선 방향을 기준으로 다양한 각도)을 **anchor**로 삼아 각 점에서 그립 후보를 생성함으로써, 학습을 안정화하고 **다양한 방향의 grasp**을 놓치지 않도록 했습니다.

* **추론 방식:** REGNet의 추론은 단일 신경망처럼 동작하지만 내부적으로 위 단계들을 거칩니다. 입력 점군에 대해 **PointNet++ 기반** 임베딩을 추출한 후, SN 헤드에서 **점별 grasp score**를 예측하고 상위 N개 점을 고릅니다. 이어 그 점들에 대해 GRN 헤드가 \*\*각각 M개의 grasp 후보 (anchor 수만큼)\*\*의 방향과 품질을 예측합니다. 마지막으로 RN이 그 후보들을 입력으로 받아 **정교화된 grasp 자세와 최종 점수**를 출력합니다. 이 과정에서 **로컬 표면 특징**이 자연스럽게 고려되며, anchor 설정으로 인해 **법선이나 곡률 등의 명시적 계산 없이**도 여러 방향을 탐색할 수 있습니다. 결과적으로 **collision-free하고 성공률이 높은 그립 포즈**들을 산출하며, 복잡한 장면에서도 한 번의 전방산출로 다수의 후보를 얻을 수 있습니다.

* **성능:** REGNet은 ICRA 2021에서 **기존 SOTA 대비 뛰어난 성능**을 보고했습니다. 실제 난잡한 물체 더미 환경에서 \*\*성공률 79.34%, 완료율 96%\*\*를 달성하여, 동시대의 GPD, PointNetGPD, S4G 등을 크게 상회했습니다. 특히 **다양한 물체 범주에 일반화**가 잘 되고, 그립 품질 평가도 정밀하여 부적절한 그립 자세를 걸러내는 능력이 우수함을 실험적으로 보였습니다. 속도 측면에서는 논문에서 언급이 없으나, 내부 단계가 세 번의 네트워크 추론으로 볼 수도 있어 다소 무거울 수 있습니다. 다만 **불필요한 점들을 초기에 거르는 메커니즘**으로 효율을 높였으므로, **최적 구현 시 수백 ms 이내**의 추론도 가능할 것으로 추정됩니다. (후속 연구 HGGD에서 비교한 결과에 따르면 REGNet 계열 기존 방법들은 수십\~수백 ms 수준인 반면 HGGD는 28ms였다고 언급되어 있어, REGNet은 아마 수백 ms대였을 가능성이 있습니다.)

* **평가 및 환경:** REGNet은 **물체 인스턴스에 사전 지식이 없는** 상황에서 동작하므로, 범용적인 bin-picking 등에 응용될 수 있습니다. 실제 실험에서는 다양한 가정용 물체를 무작위로 쌓은 더미에서 로봇팔이 물체를 집어내도록 테스트하여 높은 완료율(96%)을 보고했습니다. 이처럼 **클러터 환경**에 특화된 성능은 Jetson Orin과 같은 엣지 로봇에도 매력적입니다. 다만 구현 난이도로는, 세 모듈을 합친 신경망이 다소 크고 복잡하여 엣지에서 구동 시 **메모리 요구량**과 **실시간성** 모두 고려해야 합니다. Orin 64GB라 메모리는 충분하나, 추론을 3단계 모두 순차 실행해야 하므로 TensorRT 최적화 등을 통해 파이프라인 지연을 최소화하는 것이 중요합니다.

* **오픈 소스:** REGNet의 코드는 공개되어 있으며, **GitHub** 링크가 논문에 명시되어 있습니다. (실제 저장소 명: `zhaobinglei/REGNet` 로 알려져 있음.) 공개된 코드에는 학습된 모델과 점군 입출력 예제가 포함되어 있어 재현이 가능합니다.

### Efficient Heatmap-Guided Grasp Detection (HGGD, RA-L 2023)

* **입력 및 특징:** HGGD는 **RGB-D 센서 입력을 모두 활용**하는 대표적인 **멀티모달 융합** 기반 6-DoF 파지 추정입니다. 특히 **전역 RGB(D) 이미지**로부터 **grasp heatmap**(픽셀 단위 그립 가능성 맵)을 예측하고, 이를 가이드로 **로컬 3D 포인트클라우드**를 이용해 최종 그립 포즈를 산출하는 **Global-to-Local** 두 단계 접근을 제안했습니다. 즉, 먼저 **딥러닝 기반 2D CNN**이 **컬러 또는 깊이 이미지 상에서** 물체들의 grasp하기 좋은 영역을 히트맵으로 표시하고, 이 히트맵을 기반으로 3D 점군 중 해당 영역의 점들만 **지역 후보**로 모아 그 부분의 파지를 집중적으로 분석합니다. 이때 2D 히트맵 예측에는 **Gaussian 인코딩 및 그리드 기반 전략**을 사용하여 인접 픽셀들의 grasp 확률을 부드럽게 표현하고, 작은 영역 단위로 안정적인 응답을 얻도록 했습니다. 결과적으로 전역 단계에서는 **물체의 대략적 위치와 형태(“semantic”) 정보**를 얻고, 국소 단계에서는 해당 부위의 **정밀한 기하학적 특징**(점군)을 활용하게 됩니다.

* **추론 방식:** ① **Global 단계:** 입력 RGB-D 이미지 → ResNet 등의 CNN을 통해 **그립 히트맵** 산출. 이 히트맵은 이미지 상의 각 영역(그리드 셀)에 대해 그 안에 grasp 가능한 지점이 있을 확률를 Gaussian처럼 퍼지게 표시합니다. ② **Local 단계:** 히트맵 상에서 상위 확률을 보이는 영역에 대응하는 3D 점들을 원본 점군에서 추출. 각 영역별로 별도의 **로컬 그립 생성기** 네트워크에 입력하여 해당 점 집합으로부터 가능한 그립들을 예측. 이때 **anchor 기반 회전 샘플링**을 도입하여, 각 지역에 대해 다양한 방향의 후보를 생성하고 분류합니다. **Non-uniform anchor sampling**이라 명명된 기법으로, 히트맵 확률이 높은 영역일수록 더 다양한 각도의 grasp를 시도하여 **정확도와 다양성**을 높입니다. 마지막으로 각 후보에 품질 점수를 부여하고 상위 grasp들을 출력합니다. 전역 이미지 피처와 로컬 점군 피처는 필요한 경우 **초기 히트맵 예측 시 결합**되기도 하고, 또는 두 단계 결과를 **최종 결합하여 ranking**하는 방식으로 활용됩니다.

* **특징 융합:** 이 방식의 핵심은 **이미지 기반 전역语의 “semantic” 정보와 점군 기반 국소 기하 정보의 융합**입니다. 이미지로부터 얻는 히트맵은 단순히 깊이만으로는 파악하기 어려운 **물체 구분**이나 **윤곽 정보**를 줄 수 있어, 복잡한 장면에서 **잡을 물체 영역을 신속히 좁히는 역할**을 합니다. 이후 3D 점군 국소 분석은 **정밀한 surface geometry** (예: 물체의 구멍이나 미세한 돌출 등)까지 고려하여 최적 그립자세를 찾습니다. 논문에서는 이러한 **멀티스케일 피처 융합** 덕분에, **센서 노이즈나 입력 누락에 대한 강인성**이 높아짐을 보였습니다. 실제로 **추가적인 가우시안 노이즈**를 깊이와 점군에 주는 실험에서, 전역-로컬 융합을 한 HGGD는 성능 저하가 적었던 반면 융합이 없는 모델들은 성능이 크게 떨어졌다고 보고됩니다.

* **성능:** HGGD는 **고속성과 정확도를 동시에** 달성한 점이 돋보입니다. 논문에 따르면 제안한 모델은 **추론에 약 28ms** 정도 소요되어 \*\*실시간 처리(초당 약 35장면)\*\*가 가능하며, 이는 이전 최신 방법들보다 월등히 빠른 속도입니다. 그럼에도 불구하고 GraspNet-1B 등 대형 데이터셋 및 실제 로봇 실험에서 **94%의 grasp 성공률과 100%의 완료율**을 기록하여 정확도에서도 최고 수준임을 증명했습니다. 즉, 쌓여있는 모든 물체를 모두 집어내는 \*\*완료율 100%\*\*를 달성한 것은, 빠른 추론과 더불어 **최적의 그립 제안 품질**이 뛰어나다는 것을 시사합니다. 이러한 성과는 히트맵 가이드 전략이 **전역적으로 효율적인 탐색**을 돕고, anchor+로컬 포인트 분석으로 **세밀한 평가**를 했기에 가능했습니다. HGGD는 IROS 2023 최우수 레터(RA-L)에 선정되기도 하며, 최신 SOTA로 평가받고 있습니다.

* **평가 및 환경:** 실제 로봇 실험은 깊이 카메라와 로봇팔로 구성된 시스템에서 수행되었으며, 다양한 가정용 물체 여러 개를 무작위로 흩뿌린 장면에서 연속 파지하여 **모든 물체를 성공적으로 집는 시연**을 보였습니다. Jetson Orin과 같은 엣지 환경에도 비교적 적합한데, 이유는 \*\*모델 경량화(총 파라미터 11.6M 정도)\*\*와 단계별 효율성 덕분입니다. 실제로 이와 유사한 PointNet++ Grasping(ICRA 2020) 모델은 지극히 낮은 사양 GPU(GeForce 840M)에서도 102ms에 처리되었는데, Orin AGX의 GPU 성능을 고려하면 HGGD 또한 엣지에서 수십 Hz 실시간 구현이 충분히 가능할 것으로 보입니다. 다만 이미지 CNN과 포인트 기반 모듈의 이원화로 구현 복잡도는 다소 있으나, CUDA 가속과 병렬 파이프라이닝으로 지연을 줄일 수 있습니다.

* **오픈 소스:** HGGD의 공식 구현은 \*\*GitHub: `THU-VCLab/HGGD`\*\*에서 제공되며, 학습된 모델 및 GraspNet-1Billion 데이터셋을 활용한 평가 코드도 함께 공개돼 있습니다. 이를 통해 연구자들은 손쉽게 해당 모델을 테스트하거나, 자신의 실시간 로봇 시스템에 통합할 수 있습니다.

## 희소 점군 보완을 위한 업샘플링 및 형태 보완 연구

저가형 RGB-D 카메라나 단일 뷰 관측에서는 **점군 밀도가 낮거나 물체 뒷면이 보이지 않는 문제**로 인해, 그립 후보를 잘못 산출하거나 성공률이 떨어질 수 있습니다. 이러한 **불완전한 점군(incomplete point cloud)** 문제를 해결하기 위한 접근으로 **점군 업샘플링(point cloud upsampling)** 또는 **점군 보완(point completion)** 기술의 활용이 주목받고 있습니다. 업샘플링이란 주어진 희소 점군으로부터 **더 촘촘한 점들을 보간**해내는 것으로, 딥러닝 기반의 **PU-Net(CVPR 2018)**, **PU-GAN(CVPR 2019)** 등이 대표적입니다. 단순 업샘플링은 동일 표면 위에 점 밀도를 높여 **노이즈 완화**나 **곡률 부분 보강**에는 도움이 되지만, **보이지 않는 영역의 점을 창출하지는 못한다**는 한계가 있습니다.

이 한계를 넘어, **Point Completion** 기법은 **관측되지 않은 물체 뒷면이나 측면의 점들을 예측**하여 **물체의 전체 형태를 추정**하는데 초점을 둡니다. 예를 들어, **PCN, FoldingNet** 등으로 대표되는 점군 완성 모델들은 부분 점군을 입력받아 해당 물체의 완전한 표면 점군을 생성해낼 수 있습니다. 최근에는 이러한 완성된 점군을 grasp 인식에 활용하려는 시도가 나타나고 있습니다. 2025년 발표된 **PCF-Grasp(Point Completion to Feature for Grasp)** 연구는 **점군 완성 기술을 통해 얻은 예측 점들을 그대로 grasp 후보로 사용하지 않고**, **그 정보를 특징으로 인코딩하여** 실제 관측된 점군의 grasp 예측을 향상시키는 방법을 제안했습니다. 사람은 보이는 부분만으로 물체 전체 모양을 짐작하여 잡을 곳을 판단한다는 점에 착안하여, 완성 네트워크가 생성한 **예상 형태를 일종의 “형상 피처”로 입력 네트워크에 제공**한 것입니다. 이렇게 하면 완성 네트워크의 오류로 엉뚱한 곳에 점이 생겨도 직접 grasp로 이어지지 않아 영향을 줄이고, 대신 유용한 형태 정보는 활용해 **grasp 품질 평가를 정교화**할 수 있습니다.

PCF-Grasp의 결과, **단일 뷰 점군으로만 학습한 기존 SOTA 대비 실험에서 grasp 성공률이 17.8%p 향상**되었다고 보고됩니다. 이는 매우 고무적인 결과로, **점군 완성으로 얻은 잠재 정보가 실제 파지 성능 향상에 크게 기여**함을 보여줍니다. 또한 복잡한 장면에서도 카메라 위치에 덜 민감하게 높은 성공률을 유지할 수 있었다고 합니다. 결국 **업샘플링/완성 기술의 활용**은 sparse한 입력으로 인한 성능 저하 문제에 대한 중요한 대응책으로 떠오르고 있습니다.

다만, 현재까지 **일반적인 점군 업샘플링(PU-Net 등) 기법을 직접 기존 grasp 네트워크 앞단에 적용**한 사례는 많이 보고되지 않았습니다. 왜냐하면 업샘플링으로 **단순 밀도만 올리는 것보다**, 앞서 언급한 **형태 완성처럼 숨겨진 부분을 추론**하는 것이 더 임팩트가 크기 때문입니다. 따라서 최신 경향은 \*\*“불완전한 입력 → 보완 네트워크 → 보완정보 활용 grasp 네트워크”\*\*의 파이프라인으로 진화하고 있습니다. Jetson Orin과 같은 엣지에서도, 만약 실시간성이 허용된다면 간단한 업샘플링을 전처리로 넣어 **국소 표면을 매끈하게** 하는 방법을 고려해볼 수 있습니다. 예를 들어 LiDAR 등 매우 희소한 점군의 경우 업샘플링으로 데이터 증강을 시도할 수 있습니다. 그러나 대부분의 RGB-D 카메라 출력 밀도가 수만\~수십만 점 이상으로 그립 인식에 충분하므로, **연산 자원과 지연을 늘려가며 업샘플링을 할 실익은 크지 않을 수 있습니다**. 그 대신, \*\*필요하다면 멀티뷰 통합(예: TSDF)\*\*이나 **포인트 보완 기법**으로 **센서 가시성 한계를 보완**하는 쪽이 더 효과적입니다. 예컨대 앞서 소개한 VGN처럼 여러 프레임을 통합하거나, PCF-Grasp처럼 보이지 않는 면의 형상을 추론하여 네트워크가 참고하도록 하는 방식이 실제 성능 향상으로 이어집니다.

## 알고리즘 비교 및 결론

아래 표는 본 보고서에서 다룬 주요 알고리즘들을 요약 비교한 것입니다. 각각의 **입력 데이터 형태**, **핵심 처리 방식**, **특징 융합 여부**, **실시간 추론 성능**, **오픈소스 제공** 여부를 정리하여, Jetson Orin과 같은 엣지 디바이스에의 적용 적합성을 함께 평가합니다.

| 알고리즘 (연도)                           | 입력 데이터             | 처리 방식 및 특징                                                       | 융합 기법                        | 추론 성능                               | 오픈 소스 및 비고                                                                   |
| ----------------------------------- | ------------------ | ---------------------------------------------------------------- | ---------------------------- | ----------------------------------- | ---------------------------------------------------------------------------- |
| **VGN** (CoRL 2020)                 | 다중 뷰 깊이 → TSDF 볼륨  | 3D CNN이 voxel 단위 그립 품질/방향 출력<br>격자 전체를 탐색하여 collision-free 후보 산출 | 단일 센서 (깊이)<br>데이터 통합         | **10ms** (100fps)<br>GPU (데스크탑) 기준  | **예** (GitHub `ethz-asl/vgn`)<br>실로 초고속, 폐루프 제어 가능                           |
| **PointNetGPD** (ICRA 2019)         | 단일 뷰 점군 (부분)       | 휴리스틱 후보 생성 + PointNet 평가망<br>로컬 접촉점 주변 점군으로 성공률 분류               | 단일 센서 (깊이)<br>               | 수백 ms (추정)<br>GPU 병렬처리시 수십 Hz 가능    | **예** (GitHub `lianghongzhuo/PointNetGPD`)<br>경량 모델(1.6M param), 후보생성 최적화 필요 |
| **6-DoF GraspNet** (ICCV 2019)      | 단일 뷰 점군            | VAE 기반 다수 grasp 샘플링 + 평가망<br>생성망이 후보 제시, 평가망이 필터링                | 단일 센서 (깊이)                   | \~ 수백 ms - 1s (추정)<br>다중 샘플링으로 가변   | **예** (GitHub `NVlabs/6dof-graspnet`)<br>다단계 모델, 정확도 높음. 실시간엔 최적화 필요         |
| **Contact-GraspNet** (ICRA 2021)    | 단일 뷰 점군            | 표면 **접촉점** 기반 6-DoF 그립 예측<br>점군→접촉 후보 분포→다양한 방향 그립 출력            | 단일 센서 (깊이)<br>(세그멘테이션 결합 가능) | 수백 ms (기본 5-pass)<br>→ 10Hz까지 개선 보고 | **예** (GitHub `NVlabs/contact_graspnet`)<br>접촉 기반 표현으로 효율↑, TF 구현            |
| **REGNet** (ICRA 2021)              | 단일 뷰 점군            | 점별 스코어→영역별 후보→자세 정련<br>3단계 PointNet++ 기반 엔드투엔드 추정                | 단일 센서 (깊이)                   | 100\~200ms+ (추정)<br>HGGD 대비 느림      | **예** (GitHub 공개)<br>성공률 높음(79%↑), 다소 높은 복잡도                                 |
| **HGGD** (RA-L 2023)                | **RGB-D** 이미지 + 점군 | **전역 CNN 히트맵 + 로컬 PointNet++**<br>이미지로 grasp 영역 인퍼, 점군으로 자세 산출   | **RGB+D 융합**                 | **28ms** (35fps)<br>GPU (RTX) 기준    | **예** (GitHub `THU-VCLab/HGGD`)<br>멀티모달 융합으로 정확도 최고(94%)                     |
| **PointNet++ Grasping** (ICRA 2020) | 단일 뷰 점군            | 단일 단계 PointNet++로 모든 grasp 직접 회귀<br>샘플링 없이 한 번에 예측               | 단일 센서 (깊이)                   | 102ms (⬇️840M GPU)<br>→ 실측, 매우 경량   | **부분 공개** (데이터셋 공개)<br>End-to-end 단일망, 향후 공개 기대                              |
| **PCF-Grasp** (ArXiv 2025)          | 단일 뷰 점군 (+완성)      | **점군 완성**→형상 특징 추출→기존 grasp망 강화<br>보이지 않는 표면 정보 활용               | 단일 센서 (깊이)<br>+완성 네트워크       | n/a (미공개)<br>추가 연산(완성망) 발생          | **미정** (추후 공개 예상)<br>성공률 +17.8%p 향상, 새로운 접근                                  |

**비교 및 결론:** 위 비교에서 알 수 있듯이, **HGGD, VGN** 등 최신 기법들은 **실시간 처리와 높은 성공률을 동시에 만족**하여 Jetson Orin 같은 엣지에서도 유망한 후보입니다. **HGGD**는 RGB-D 융합을 통해 정확도를 극한으로 끌어올리면서도 경량화를 이뤘고, **VGN**은 3D CNN으로 단순화한 구조 덕분에 **압도적인 속도**를 보입니다. **PointNetGPD**와 **REGNet**도 각각 경량모델과 다단계모델의 대표로서 의미있지만, 전자는 후보 생성 최적화가 필요하고 후자는 실시간성 면에서 추가 개선 여지가 있습니다. **Contact-GraspNet**과 **6-DoF GraspNet**은 NVIDIA가 제안한 강력한 방법들이나, 현 시점에선 속도나 구현 면에서 **엣지 보다는 서버급 GPU에 적합**할 수 있습니다. 다만 Contact-GraspNet은 개념적으로 **접촉 기반의 효율성**이 있으므로, 향후 최적화를 거치면 엣지에서도 돌아갈 수 있을 것입니다.

**업샘플링/보완 기법의 활용** 측면에서, **PCF-Grasp** 등의 최신 연구가 시사하듯 **점군의 보이지 않는 부분을 추론하여 파지 인식에 활용**하는 것이 새로운 방향으로 주목받고 있습니다. Jetson Orin과 같은 현장 로봇에서는 멀티뷰 통합이나 간단한 업샘플링으로도 입력 개선을 시도해볼 수 있지만, **추가 연산으로 인한 지연과 효과 사이의 트레이드오프**를 고려해야 합니다. 만약 센서가 충분히 양질의 점군을 제공한다면, 업샘플링 없이 본 보고서에서 소개한 **최신 실시간 파지 인식 네트워크들을 직접 적용**하는 것으로도 성공률을 높이는 데 충분할 것입니다. 이러한 네트워크 대부분이 **오픈 소스로 공개**되어 있으므로, Jetson 플랫폼에 맞게 **프레임워크 포팅** 및 **TensorRT 가속** 등을 통하여 실시간 로봇 그리핑 시스템 구현이 가능할 것으로 판단됩니다. 앞으로도 **엣지 디바이스 상의 6자유도 파지**를 위한 연구는 모델 경량화, 데이터 효율화, 센서퓨전 측면에서 지속 발전할 것으로 기대됩니다.
