# Jetson AGX Orin과 ROS2 기반 NanoOWL + NanoSAM 실시간 자연어 객체 인식 및 그래스핑 시스템 보고서

## 1. 서론 (Introduction)

오늘날 로봇 분야에서는 **자연어 명령 기반 객체 인식 및 그래스핑**이 중요한 연구 주제로 부상하고 있습니다. 사람의 음성이나 텍스트 명령으로 특정 물체를 인식하고 집어올리는 로봇 시스템은, 산업 현장부터 가정 서비스 로봇까지 광범위한 활용 가능성을 지니고 있습니다. 이러한 시스템을 구축하기 위해서는 **시각 인지**, **언어 이해**, **로봇 제어**의 통합이 필요하며, 각각 최신 AI 기술의 발전에 힘입어 새로운 접근법이 나오고 있습니다. 본 보고서에서는 **Jetson AGX Orin** 엣지 컴퓨팅 플랫폼과 **ROS2** 미들웨어 환경에서, **NanoOWL** 및 **NanoSAM** 모델 조합을 활용한 **실시간 자연어 명령 기반 객체 인식 및 그래스핑 시스템**의 기술적 타당성을 심층 분석합니다.

Jetson AGX Orin과 ROS2를 선택한 이유는 다음과 같습니다. **Jetson AGX Orin**은 NVIDIA의 첨단 엣지 AI 플랫폼으로, 소형 폼팩터 내에 강력한 GPU 연산 성능을 갖추고 있어 실시간 딥러닝 추론에 적합합니다. 로봇의 본체에 직접 탑재하여 **클라우드 의존 없이 현장에서** 고성능 인지-제어 처리가 가능하다는 장점이 있습니다. **ROS2**는 분산 처리 및 실시간 제어에 강점을 가진 로봇 미들웨어로, 센서 데이터 수집부터 제어 명령 발행까지 일련의 파이프라인을 노드 단위로 구성하고 관리하기에 용이합니다. ROS2의 표준화된 인터페이스를 활용하면 **Camera, Gripper, Robot Arm Control** 등 여러 모듈을 유기적으로 연결할 수 있으며, 추후 시스템 확장이나 교체도 수월합니다.

**NanoOWL**과 **NanoSAM**은 NVIDIA가 Jetson과 같은 엣지 디바이스에서 **자연어 기반 인지**를 실시간 구현하기 위해 최적화한 모델들입니다. NanoOWL은 **자연어 프롬프트 기반 개방형 객체 탐지 모델(OWL-ViT)**을 경량화하여 Jetson에서 구동 가능하게 만든 버전이고, NanoSAM은 Meta AI의 **SAM**을 경량화하여 Jetson에서 실시간 동작하도록 한 세그멘테이션 모델입니다. 이들을 결합하면 사용자가 입력한 자유 형식의 **텍스트 명령**(예: "책상 위 빨간 컵을 집어")에 따라 해당 객체를 탐지하고 픽셀 단위로 분할(segmentation)하며, 이후 로봇팔로 집어올리는 **엔드투엔드 파이프라인**을 구성할 수 있습니다. 특히 NanoOWL과 NanoSAM은 **TensorRT** 가속 및 경량화된 백본을 통해 Jetson Orin에서 수십~수백 FPS의 추론 속도를 달성하여, 실시간 처리가 가능하면서도 높은 인지 성능을 제공합니다.

본 연구 주제가 **다른 대안들보다 적절한 선택인 이유**는, **개방형 어휘(오픈 vocab) 처리 능력**, **실시간 동작**, **엣지 디바이스 적합성**, **ROS2 통합 용이성** 측면에서 우수한 균형을 보여주기 때문입니다. 전통적인 객체 인식/파지 시스템은 보통 사전에 학습된 한정된 물체들만 인식하거나, 클라우드 연산에 의존하거나, 혹은 느린 속도로 동작하는 경우가 많았습니다. 반면 본 시스템은 **대규모 사전학습 비전-언어 모델**의 제로샷 인식 능력을 활용하여 **새로운 물체에도 대응**할 수 있고, Jetson AGX Orin 상에서 **온보드 실시간 추론**을 함으로써 **저지연 응답**이 가능하며, ROS2 프레임워크를 통해 센서, AI, 제어 모듈이 **유기적으로 통합**될 수 있습니다. 보고서에서는 이 주제 선택의 기술적 정당성을 체계적으로 설명하고, NanoOWL과 NanoSAM 조합을 중심으로 **Edge 환경에서의 성능, ROS2 통합, 제로샷 인식력** 등을 깊이 있게 분석합니다. 또한 유사 목적을 달성하기 위한 대안 기술들 – 예컨대 CLIP 및 파생기술, Grounding DINO + SAM과 같은 파이프라인, 강화학습 기반 접근 – 과 비교하여, 본 접근법의 장단점을 면밀히 평가합니다. 더불어 최신 연구 동향을 살펴봄으로써 사용자가 설계한 시스템 구조가 학계 및 산업계 흐름에 부합하며 향후 확장 가능함을 입증하고자 합니다.

보고서는 **서론**, **기술 배경**, **관련 연구 동향**, **선택 기술 심층 분석**, **실험 구성 및 기대 성능**, **결론 및 향후 과제**의 순서로 구성됩니다. 각 장에서는 필요한 경우 도표와 예시 코드를 포함하고, 실제 하드웨어 적용 사례나 벤치마크 수치를 인용하여 논거를 뒷받침합니다. 모든 기술 용어는 정확한 영어 표기를 병기하고, 독자가 핵심 내용을 쉽게 찾아볼 수 있도록 **명확한 헤딩**과 **요점 정리 리스트**를 활용하였습니다.

이제 다음 장부터 본 연구에 관련된 기술적 배경을 살펴보고, 이후에 구체적인 시스템 설계와 선택된 기술의 우수성에 대해 논의하겠습니다.

## 2. 기술 배경 (Technical Background)

이 장에서는 본 연구를 이해하는 데 필요한 주요 기술 개념과 요소들에 대해 설명합니다. **비전-언어 모델(VLM)**, **오픈-보캐불러리 객체 인식**, **세그멘테이션 모델**, **강화학습 기반 그래스핑**, **엣지 컴퓨팅 플랫폼과 ROS2** 등에 대한 배경을 다룹니다. 이러한 기술 배경 지식을 바탕으로, 이후 장에서 NanoOWL+NanoSAM 조합을 선택한 이유와 기존 접근법 대비 장점을 더욱 명확히 할 수 있을 것입니다.

### 2.1 Vision-Language 모델과 개방형 객체 인식

**VLM**은 이미지와 텍스트를 공동으로 이해하는 딥러닝 모델로, 시각 정보와 언어 정보를 동일한 임베딩 공간에 매핑하여 두 모달리티 간의 의미적 연결을 학습합니다. 대표적인 VLM으로 OpenAI의 **CLIP (Contrastive Language-Image Pretraining)** 모델이 있습니다. CLIP은 **4억 쌍 이상의 이미지-텍스트 데이터셋**으로 사전 학습되어, 텍스트 설명과 이미지 간 유사성을 측정하는 강력한 능력을 갖추고 있습니다. 예를 들어 CLIP 모델에 이미지와 여러 문구를 입력하면, 해당 이미지에 가장 잘 어울리는 문구를 높은 점수로 매칭할 수 있습니다. 이러한 **이미지-언어 표현력** 덕분에 CLIP은 이미지 분류, 이미지 검색, 캡션 생성 등 다양한 비전 과제에서 뛰어난 제로샷(zero-shot) 성능을 보여주었고, 로봇 분야에서도 **특징 추출기**로 활발히 활용되고 있습니다.

그러나 CLIP 자체는 **물체의 위치를 찾아주는 탐지(detection)** 모델은 아니므로, 로봇이 특정 객체를 집으려면 CLIP의 장점을 이어받으면서도 **공간적 위치**를 출력해주는 모델이 필요합니다. 이를 위해 등장한 것이 **오픈 보캐bul러리 객체 탐지 모델**들입니다. **Open-Vocabulary Detection**이란, 사전에 학습되지 않은 임의의 객체 카테고리도 **텍스트 설명만으로 탐지할 수 있는** 객체검출을 의미합니다. 이러한 모델들은 보통 **사전학습된 언어-비전 백본**(예: CLIP의 비전 트랜스포머와 텍스트 임베딩)을 활용하여, 입력 이미지와 주어진 텍스트 프롬프트(prompt)에 부합하는 모든 객체의 바운딩 박스를 찾아냅니다.

구글 리서치의 **OWL-ViT**은 대표적인 오픈 보캐불러리 탐지 모델입니다. Vision Transformer 기반의 인코더-디코더(two-stage) 구조로 이루어진 OWL-ViT는 **자연어 질의**와 이미지를 동시에 받아 해당 텍스트와 관련된 객체들의 **2D 바운딩 박스**를 출력합니다. 예를 들어 OWL-ViT에 이미지와 \["cup", "bottle"]이라는 텍스트 리스트를 주면, 이미지 속에서 "cup"(컵)과 "bottle"(병)에 해당하는 모든 물체들을 찾아 경계상자와 점수를 반환합니다. OWL-ViT는 대규모 이미지-텍스트 데이터로 학습되었기 때문에 새로운 객체에 대해서도 높은 제로샷 탐지 성능을 보이며, 논문 보고에 따르면 COCO 등 벤치마크에서 기존 폐쇄형 카테고리 검출기를 뛰어넘는 결과를 보였습니다. 마이크로소프트의 **GLIP (Grounded Language-Image Pre-training)**이나, 카네기멜론대의 **ViLD** 등 유사한 컨셉의 모델들도 발표되었는데, 모두 **"텍스트로 묘사된 임의의 물체를 찾아내는"** 능력을 목표로 합니다.

하지만 이들 **기초 VLM 기반 탐지 모델**은 주로 대용량 서버 GPU에서 동작하도록 설계되었기에, **엣지 장치에서 실시간으로 구동하기엔 무겁다**는 문제가 있습니다. 예컨대 Grounding DINO (또다른 오픈셋 탐지 모델로 DETR 기반 구조에 CLIP 임베딩을 결합한 모델)은 Jetson AGX Orin에서 1080p 이미지 처리 시 약 **11.8 FPS**의 속도를 보였다는 보고가 있습니다. 이는 로봇 실시간 제어에 직접 적용하기에 다소 버거운 속도입니다. 따라서 엣지 디바이스에 적합하도록 **모델 최적화**가 필수적인데, NVIDIA의 NanoOWL이 바로 이러한 필요에 의해 개발되었습니다.

### 2.2 NanoOWL: Jetson 최적화 OWL-ViT

**NanoOWL**은 NVIDIA가 공개한 **Jetson용 OWL-ViT 최적화 버전**으로, OWL-ViT 모델을 TensorRT로 변환하고 경량화하여 **실시간 추론**을 가능케 한 프로젝트입니다. NanoOWL은 HuggingFace에 공개된 Google의 OWL-ViT 사전학습 가중치를 기반으로 TensorRT 엔진을 미리 빌드하여 사용하며, Jetson AGX Orin에서 **놀라운 속도 향상**을 보여줍니다. 구체적으로, NanoOWL의 ViT-B/32 백본 모델은 AGX Orin에서 **약 95 FPS**의 추론 속도를 달성했고, 더 큰 ViT-B/16 모델도 **약 25 FPS**로 구동되었습니다. 이는 원본 모델을 그저 GPU 상에서 PyTorch로 실행할 때와 비교하면 수배 이상 향상된 수치로, 임베디드 장치에서 **텍스트 기반 객체탐지**를 **수십 FPS** 수준으로 끌어올린 매우 인상적인 결과입니다. NanoOWL은 이러한 속도 향상을 위해 엔코더(비전 트랜스포머)와 디코더를 정적 최적화하고 FP16 저정밀도 연산 등을 활용하였으며, Jetson의 Ampere GPU 구조에 맞게 메모리 접근을 최적화했습니다.

NanoOWL의 **입력과 출력**은 OWL-ViT와 동일하게 구성되어 있습니다. 입력으로는 **RGB 이미지**와 **텍스트 질의 목록**이 주어지며, 출력으로는 각 텍스트 항목에 대응되는 **바운딩 박스 좌표와 신뢰도 점수**가 반환됩니다. 예를 들어, NanoOWL에 이미지와 `["red cup"]`이라는 질의를 주입하면, 이미지 내에서 "red cup"에 해당하는 물체(빨간 컵)의 바운딩 박스를 찾아내고, 그 위치와 점수를 출력합니다. 다만 OWL-ViT 구조상 **마스크 세그멘테이션 결과는 출력하지 않기 때문에**, NanoOWL 단독으로는 픽셀 단위의 정교한 물체 외형을 얻을 수 없습니다. Bounding box는 물체의 대략적 위치만을 알려주므로, 이후 정확한 파지(grasp)를 위해서는 물체의 **정확한 윤곽**이나 **자세** 정보를 더 얻어야 합니다. 이를 보완하는 역할을 하는 것이 바로 **NanoSAM**입니다.

NanoOWL은 **제로샷(open-vocabulary) 인식 성능** 측면에서도 최신 모델의 강점을 계승하고 있습니다. 대규모 텍스트-이미지 학습 덕분에, **사전에 본 적 없는 물체도 텍스트로 지칭만 하면** 탐지가 가능하며, 특정 카테고리 이름뿐 아니라 **자연어 기술 전체**를 이해하기 때문에 세부 특성으로 대상 물체를 지정할 수도 있습니다. 예를 들어 "컵"이란 단어를 모르는 모델이라도 "drinkware on the table" 등으로 표현하면 유사한 개념을 포착할 수 있는 식입니다. OWL-ViT 자체의 성능보고에 따르면 COCO 등에서 미학습 클래스에 대한 **mAP**이 높은 편이며, 이는 NanoOWL에도 그대로 적용됩니다. 다만 NanoOWL은 속도 향상을 위해 백본 크기를 줄이고 일부 연산을 단순화했기에, 이론상 원본 대비 약간의 정확도 손실은 존재할 수 있습니다. 하지만 NVIDIA가 공개한 결과로는 **정확도 면에서 큰 손실 없이** 실시간화를 이뤄냈으며, Jetson Orin 상에서 다양한 데모를 통해 높은 품질의 탐지 결과를 확인하였습니다.

NanoOWL은 **ROS2 통합**도 용이하게 이루어지고 있습니다. NVIDIA는 NanoOWL 추론 기능을 담은 ROS2 패키지(**ROS2-NanoOWL**)를 오픈소스로 공개하여, 개발자가 손쉽게 ROS2 노드 형태로 NanoOWL을 사용할 수 있도록 지원합니다. 이 패키지는 이미지 토픽을 구독하여 NanoOWL 엔진으로 추론한 뒤, 탐지된 객체들의 결과를 ROS2 토픽으로 퍼블리시하는 기능을 포함합니다. 예시 코드로는, `OwlPredictor` 클래스를 이용하여 콜백 함수 내에서 `predict(image, text=["<질의>"])`를 호출하고, 그 결과로 바운딩박스 좌표 리스트를 얻는 식입니다. 이러한 ROS2 노드 예제를 통해 NanoOWL을 실제 카메라 스트림에 실시간 적용하고 결과를 다른 노드(예: 시각화나 제어 로직)에 전달할 수 있습니다. ROS2 통합 덕분에 NanoOWL은 기존 로봇 시스템에 **플러그인**하듯 삽입되어 동작할 수 있으며, 이미지 처리는 별도 스레드/프로세스로 분리되어 **시스템 전체의 모듈성**과 **자원 효율성**을 높여줍니다.

### 2.3 Segment Anything Model과 NanoSAM

**SAM**은 Meta AI(구 Facebook)가 발표한 범용 세그멘테이션 모델로, 이름 그대로 "모든 것을 분할할 수 있는" 수준의 광범위한 범용성을 목표로 합니다. SAM은 거대한 ViT-H 이미지 인코더와 변형된 트랜스포머 디코더로 구성되며, **점, 박스, 마스크 등의 프롬프트**를 입력받아 이미지 내의 해당 영역을 정확히 분할하는 모델입니다. SAM의 특징은 사전 학습에 **10억 개 이상의 마스크**를 사용한 점입니다. 이를 통해 본 적 없는 객체나 배경이라도 경계선을 아주 잘 찾아내며, 사람의 개입 없이도 **제로샷 세그멘테이션**을 수행할 수 있습니다. 즉, 특정 물체에 대한 바운딩 박스를 주거나, 혹은 한 픽셀 포인트를 지정하면, SAM은 그를 둘러싼 객체의 픽셀 단위 마스크를 산출합니다. 초기 발표된 SAM은 GUI 상에서 사용자가 간단히 클릭 몇 번으로 이미지 내 모든 물체들을 얻을 수 있을 정도로 **강력하고 편리한 세그멘테이션 성능**을 시연하였습니다.

SAM 자체는 **텍스트 인식 기능은 없기 때문에**, 자연어로 곧바로 특정 객체를 분할할 수는 없습니다. 그래서 등장한 것이 **Grounded-SAM** 등의 결합 파이프라인으로, **Grounding DINO** 같은 오픈어휘 탐지기가 텍스트로 객체의 위치를 찾으면 SAM이 그 영역을 마스크로 세분화하는 협업 방식입니다. 본 연구의 NanoOWL + NanoSAM 조합도 이런 개념의 일종으로서, NanoOWL이 텍스트로 객체를 찾고 NanoSAM이 마스크를 얻는 흐름입니다.

**NanoSAM**은 NVIDIA가 Jetson 등 엣지 디바이스에서 SAM을 구동하기 위해 제시한 **경량화 버전 SAM**입니다. SAM의 거대 ViT-H 대신 훨씬 작은 **ResNet-18** CNN 백본을 사용하고, Meta가 공개한 MobileSAM 아이디어를 응용하여 distillation(지식 증류) 기법으로 학습되었습니다. NanoSAM의 목적은 **최대한 품질을 유지하면서도 연산량을 크게 줄이는 것**이었는데, 결과적으로 Jetson AGX Orin에서 NanoSAM은 **8.1 ms**만에 마스크를 예측할 정도로 빨라졌습니다. 8.1 ms는 **125 FPS**에 해당하는 속도로, 사실상 카메라가 여러 대 있거나 더 빠른 센서라도 충분히 실시간 처리 가능한 수준입니다. 정확도 측면에서도 MobileSAM 대비 약간의 mIoU 하락만 있을 뿐, **원본 SAM에 근접한 품질인 mIoU가 약 0.80 수준**을 유지한다고 보고되었습니다. (원본 SAM의 절대 mIoU는 공개자료에서 0.88 수준으로 알려져 있으나, 이는 거대한 모델 기준이고, MobileSAM이나 NanoSAM은 그보다 약간 낮은 0.80대 성능으로 파악됩니다.) NanoSAM은 **입력/출력 인터페이스가 SAM과 동일**하여, 점이나 박스 프롬프트를 여러 개 넣어도 되고 한번에 여러 객체 마스크를 추출할 수도 있습니다. 이를 활용하면 NanoOWL이 찾아낸 복수의 바운딩 박스를 NanoSAM에 모두 전달해 한 번에 해당 물체들의 마스크를 얻는 것도 이론적으로 가능합니다.

NanoSAM은 현재 독립적인 ROS2 패키지 형태로는 제공되지 않지만, Python API 및 ONNX엔진 형태로 공개되어 있어 NanoOWL 파이프라인에 쉽게 통합 가능합니다. 실제로 NVIDIA의 예제에서는 NanoOWL로 탐지한 **박스 좌표**를 NanoSAM에 **박스 프롬프트**로 입력하여, 그 내부의 정확한 객체 마스크를 획득하는 과정을 시연하였습니다. 이러한 NanoOWL+NanoSAM 연계는 Jetson 플랫폼에서 자연어->탐지->세그멘테이션으로 이어지는 완전한 지각 파이프라인을 구축해주며, Jetson Orin의 GPU 가속 덕분에 **텍스트 명령을 주면 해당 물체의 픽셀 마스크를 실시간으로 얻는** 것이 가능합니다.

SAM 기반 모델의 장점은 **어떠한 물체라도 형태를 정확히 파악**할 수 있다는 것입니다. 그래스핑(파지)을 위해서는 대상 물체의 크기와 형태 정보를 정확히 아는 것이 중요합니다. 바운딩 박스만으로는 물체의 세부 형상을 알기 어려우나, 세그멘테이션 마스크가 있다면 2D 이미지 상에서 해당 물체의 **모든 픽셀 영역**을 얻을 수 있고, 동시에 깊이 영상과 결합하면 물체의 3D 포인트 클라우드도 추출할 수 있습니다. 이 정보는 로봇팔에게 **정확한 파지 포인트**와 **접근 경로**를 계산하는 데 큰 도움이 됩니다. 예를 들어 마스크를 통해 물체의 **무게 중심**을 추정하거나, 물체의 주요 돌출부(손잡이 등)를 파악하여 그리퍼를 넣을 공간을 찾는 것이 가능합니다. 또한 시나리오에 따라 **다중 객체**가 존재할 때, 세그멘테이션을 하면 개별 객체들을 분리해 인식할 수 있으므로, 집고자 하는 목표 이외의 다른 물체를 피하거나 또는 우선 순위를 지정하는 데 유리합니다.

NanoSAM은 임베디드 환경에서 이러한 세그멘테이션 이점을 누리게 해준다는 점에서, NanoOWL과 상호보완적인 역할을 합니다. NanoOWL이 **어떤 물체를 찾을지** 결정하면, NanoSAM은 **그 물체의 경계를 상세히 획정**해줍니다. 두 모델 모두 가벼운 연산으로 구성되어 Jetson에서 거뜬히 동시에 구동되며, ROS2 기반으로 데이터를 주고받으며 동작시킬 수 있습니다. NanoSAM의 결과 또한 ROS2 메시지로 퍼블리시하거나, 이미지 토픽에 오버레이하여 시각화할 수 있습니다 (예: OpenCV를 통해 마스크 위에 색상을 칠해 디스플레이).

### 2.4 로봇 그래스핑 (Grasping)과 접근 방식들

**Grasping**은 로봇공학에서 로봇팔 끝단에 있는 **gripper**, **end-effector**나 손으로 물체를 잡는 기술을 말합니다. 본 연구의 목표는 **인지된 목표 물체를 로봇팔로 집어올리는 것**이므로, 인지 모듈(NanoOWL+NanoSAM)과 함께 **파지 제어 모듈**도 핵심 요소입니다. 그래스핑은 물체의 3D 위치, 형태, 마찰, 무게 등을 고려해야 하는 복잡한 문제이며, 오래전부터 다양한 알고리즘과 접근법이 개발되어 왔습니다.

전통적으로 그래스핑은 **기하학 기반** 접근과 **학습 기반** 접근으로 나눌 수 있습니다. 기하학 기반 방법은 물체의 3D 모델이 주어졌을 때 최적의 잡기 위치와 그리퍼 각도를 계산하는 것으로, CAD 모델이나 간단한 형태 추출(예: 경계박스의 중앙, 혹은 오목한 부분 찾아서 움켜쥐기 등)을 사용했습니다. 이러한 방법은 규칙 기반으로 동작하므로 새로운 물체나 불규칙한 형상에 대해서는 일반화가 어렵습니다.

최근 몇 년간 **딥러닝**을 활용한 **학습 기반 그래스핑**이 각광받고 있습니다. 심층 신경망을 통해 **이미지 또는 점구름을 입력받아 그리퍼의 최적 포즈인 6DOF**를 출력하도록 훈련시키는 것입니다. Berkeley의 **Dex-Net** 프로젝트나, Cornell Grasp Dataset을 활용한 CNN 등은 단일 깊이 이미지에서 평면상의 그립 rectangle을 예측하는 모델을 선보였습니다. 또한 3D 시나리오에 맞춰 포인트클라우드를 입력으로 여러 후보 파지 자세를 점수화하는 알고리즘(GPD: Grasp Pose Detection 등)도 개발되었습니다. 이러한 기법들은 대개 **지도학습**으로 수만\~수백만 개의 그랩 후보 데이터를 학습하거나, **강화학습** 혹은 **자율학습**을 통해 시뮬레이터에서 수많은 시도를 통해 습득하게 됩니다.

**강화학습 기반 그래스핑**은 로봇팔이 시뮬레이션 또는 실제 환경에서 반복적인 시도를 하면서 **성공적인 파지 정책**을 학습하도록 하는 접근입니다. 예를 들어, Q-learning의 확률적 정책 최적화나, 정책 경사 방법(Policy Gradients), Actor-Critic 기법 등을 활용하여 **픽업 성공 여부**를 보상으로 주어가며 학습시킵니다. 구글의 QT-Opt는 실세계에서 58만 회 이상의 로봇 파지 시도를 통해 학습한 강화학습 프레임워크로, 이후 실제 물체 잡기에 일반화되는 뛰어난 성능을 보인 바 있습니다. 강화학습 접근의 장점은, **환경에 적응**하고 **실패로부터 개선**할 수 있다는 것입니다. 예컨대 학습된 RL 정책은 사람이 명시적으로 가르치지 않아도, 물체가 미끄러지면 재조정하거나 한 번 실패하면 다른 위치를 시도하는 등의 행동을 보여줄 수 있습니다. 또한 잘 훈련된 정책의 추론 속도는 매우 빨라서, **수 ms 이하로 실행 가능**하므로 실시간 제어에 유리합니다.

그러나 RL 기반 접근에는 **단점과 한계**도 존재합니다. 첫째, **대량의 학습 데이터와 시간**이 필요합니다. 시뮬레이터에서 조차 수십만\~수백만 회의 파지 시도를 해야 정책이 안정적으로 수렴하며, 현실로 이를 이전(deployment)할 때 **Sim2Real gap**을 줄이는 추가 기법(도메인 랜덤화 등)이 필요합니다. 둘째, **일반화 문제**가 있습니다. 강화학습 정책은 훈련한 물체 형태나 상황에 특화되기 쉬워서, 새로운 형태의 물체나 다른 조명/배경 조건에서는 성능이 떨어질 수 있습니다. 학습된 범위를 벗어나면, 엉뚱한 곳을 잡으려 시도하거나 불안정하게 잡는 등 신뢰성이 떨어질 위험이 있죠. 셋째, RL 정책은 목표로 하는 물체의 **정체성**을 구분하지 못하는 경우가 많습니다. 즉, "아무 물체나 집어"는 잘 하지만 **특정 물체를 집어**라는 지시에 대해, 해당 물체만 골라잡는 것은 추가적인 조건부 정책 학습이 필요합니다. 일반적인 RL 연구에서는 이러한 언어 조건을 잘 다루지 않았기 때문에, 그냥 가까운 물체를 잡거나 훈련된 객체들을 구분 없이 집는 경향이 있습니다.

한편, **비전-언어 모델 기반 그래스핑**은 이러한 RL 접근의 한계를 보완하는 새로운 패러다임입니다. 인간이 언어로 특정 대상을 지목하면, VLM이 그 대상을 인지하고, 전통적인 로봇 **Path Planning**을 통해 파지를 수행하는 방식입니다. 이 접근은 언어 조건에 의해 **높은 수준의 과제 지정**이 가능하고, **사전학습된 지식**으로 미등록 객체도 다룰 수 있다는 장점이 있습니다. 또한 이미 잘 확립된 로봇팔 **경로생성 알고리즘**(예: MoveIt2)이 활용되므로, 로봇의 운동학이나 충돌 회피 등을 신뢰성 있게 처리할 수 있습니다. 단점이라면, 인지와 계획 단계가 분리되어 RL만큼 즉각적인 피드백 루프는 아니라는 점과, 시스템 통합의 복잡도가 조금 높다는 점이 있습니다. 그래도 최근 연구들은 VLM과 RL을 결합하여 시너지를 내기도 합니다. 예를 들어, Shridhar 등은 **CLIPort**라는 체계를 통해 CLIP으로 목표를 지정하고 **Transporter Network**로 물체 간 위치관계를 학습시켜 **언어 조건부 파지**를 가능케 하였고, Xu 등은 **Vision-Language-Action**을 통합한 정책으로 클러터 환경에서 **목표 지향 파지**를 수행하는 모델을 선보였습니다(ICRA 2023). 또한 Vo 등은 **세그멘테이션 + 언어 Attention** 방식을 통해 "오른쪽에 있는 빨간 컵"을 정확히 집는 **Referring Grasping** 기법을 연구하여, 언어 지시에 따라 마스크 중 올바른 것을 선택하고 파지 자세를 계산하는 방법을 보고하였습니다(IROS 2024).

정리하면, **그래스핑** 기술은 전통 기법부터 최신 RL, VLM 기반 접근까지 스펙트럼이 넓으며, 각각의 장단점이 있습니다. 본 연구에서는 **VLM 기반 접근**을 채택하되, 필요한 부분에 **기존 로봇제어 기법** (예: MoveIt2 경로 계획)을 활용하는 하이브리드 방식을 취합니다. 이는 **자연어로 특정 물체를 집는 문제**에 특히 적합한데, 언어 이해를 통해 목표를 명확히 하고, VLM으로 인지하며, 검증된 모션 플래너로 집는 행동을 실행함으로써 **신뢰성과 범용성**을 동시에 추구할 수 있기 때문입니다. 다음 장에서는 이러한 결정에 영향을 준 관련 연구 동향들을 좀더 상세히 살펴보겠습니다.

## 3. 관련 연구 동향 (Related Work and Trends)

본 절에서는 자연어 기반 로봇 인식/파지와 관련하여 최근의 연구 동향을 검토합니다. **비전-언어 모델의 로봇 적용**, **세그멘테이션을 활용한 파지**, **강화학습 및 러닝 기반 파지**, **ROS2를 활용한 임베디드 로봇 시스템** 등 다양한 측면에서의 선행 연구들을 다룹니다. 이 고찰을 통해, 사용자가 제안한 NanoOWL+NanoSAM 기반 구조가 현재 흐름에서 어떤 위치에 있으며 어떠한 차별점을 가지는지 명확히 할 수 있을 것입니다.

### 3.1 자연어 기반 로봇 인지 및 조작

**자연어 지시를 이해하여 로봇을 동작**시키는 연구는 오래전부터 시도되었지만, 최근에 와서야 딥러닝 기반 언어모델과 비전모델의 발전으로 큰 진전을 이루고 있습니다. **Vision-Language Model**을 로봇에 적용한 대표적인 예로 Shridhar 등이 2022년 발표한 **CLIPort** 시스템을 들 수 있습니다. CLIPort는 앞서 언급한 CLIP을 비전 백본으로 사용하고, **Transporter Network**라는 일종의 위치 연산 네트워크와 결합하여 **언어로 지정된 물체를 집어서 다른 곳에 놓는** 등의 조작을 학습한 프레임워크입니다. 이를 통해 여러 가지 테이블top 작업(블록 쌓기 등)에서 단일 정책으로 다양한 언어 조건부 작업을 수행하는 결과를 보여주었습니다. 이 연구는 **대규모 사전학습 VLM**의 힘을 로봇이 활용함으로써, 새로운 지시나 사물에도 일반화할 수 있음을 시사하였습니다.

이후로도 **언어 조건부 조작** 분야에서 많은 연구가 나오고 있습니다. 구글 연구진의 **SayCan**(2022) 프로젝트는 대형 언어모델(LLM)과 비전모델을 결합하여, 인간의 복잡한 언어 지시를 고수준 액션 시퀀스로 변환하고 로봇에 실행시키는 개념을 선보였습니다. 여기서 비전모델은 환경 상태를 파악하고, LLM은 어떤 순서로 행동해야 목표를 달성할지 판단합니다. 이러한 흐름은 **Robotics Foundation Model**이라는 새로운 분야를 형성하고 있으며, NVIDIA도 **Isaac GR00T**라는 프로젝트를 통해 로봇 종합지능을 위한 대규모 모델 개발을 진행 중입니다.

**Referring Expression**를 활용한 로봇 인식도 활발히 연구되었습니다. 이는 “오른쪽 빨간 상자를 집어”와 같이 언어로 특정 객체를 지칭하면, 그 표현(Referring expression)에 해당하는 물체를 시각적으로 찾아내는 것입니다. 초기에는 2D 이미지에서 언어로 지목된 객체를 표시하는 **Referring Image Segmentation/Localization** 분야로 연구되었으나, 최근 로봇 그래스핑에 이를 접목한 예가 있습니다. **Vo et al.**(**IROS 2024**)의 연구는 여러 물체가 있는 장면에서 **세그멘테이션을 통한 분할 + 언어 어텐션**으로 목표 객체를 정확히 선택하고, 그리퍼의 자세를 산출하는 **Language-driven Grasp Detection** 기법을 제안했습니다. 이 방식은 먼저 장면의 모든 객체들을 분할(segmentation)한 뒤, 언어 인코더를 통해 명령문(예: "오른쪽에 있는 빨간 컵")을 임베딩하고, 이를 각 세그멘테이션과 **크로스 어텐션**하여 가장 관련성 높은 마스크를 선택합니다. 선택된 마스크에 대해서는 사전에 학습된 grasping 네트워크가 최적의 그립 포즈를 예측합니다. 그 결과 언어로 특정된 객체만 집도록 정확도를 높였고, 부정확한 지칭으로 인한 엉뚱한 파지를 줄였습니다. 이러한 연구는 본 보고서에서 다루는 NanoOWL+NanoSAM -> MoveIt 파이프라인과 유사한 문제의식을 갖고 있으며, **세그멘테이션 + 언어**의 조합이 그래스핑 성공률을 높일 수 있음을 입증합니다.

**Vision-Language 모델의 성능 향상**에 따라, 로봇이 얻을 수 있는 환경 이해 수준도 높아지고 있습니다. 예컨대 최근 등장한 **Grounding DINO + SAM 통합**(일명 Grounded-SAM) 파이프라인은 텍스트 프롬프트로 객체를 탐지하고 곧바로 마스크까지 획득함으로써, 추가 학습 없이도 **텍스트->세그멘테이션**을 실행하는데, 이 역시 로봇 환경 이해에 활용되고 있습니다. 다만 이러한 모델들은 앞서 언급한 바와 같이 임베디드에서 돌리기엔 무거운 경우가 많아, 실용 로봇 시스템에서는 **경량화/최적화**나 **distillation** 노력이 함께 이루어지고 있습니다. NanoOWL, NanoSAM 같은 프로젝트가 대표적이며, Meta의 **MobileSAM**, Chaoning Zhang 등의 **MobileOne-SAM**, CLIP 백본을 줄인 **Mobile CLIP** 등도 발표되었습니다.

### 3.2 학습 기반 그래스핑과 언어 통합

그래스핑 쪽의 관련 연구 동향을 살펴보면, **강화학습**과 **스스로 데이터를 모으는 자율학습**을 통한 시도가 두드러집니다. 앞서 언급한 QT-Opt (QT 강화학습 최적화)은 실세계 데이터로 정책을 학습해 성능을 높인 예이며, UC Berkeley의 **QT-Opt v2**나 **RLG-Humanoids** 등의 후속 연구들이 로봇팔과 손으로 더 복잡한 조작을 하도록 하고자 했습니다. 이들은 주로 카메라 영상을 입력으로 받아 **엔드투엔드**로 조인트 제어 명령을 출력하기 때문에, 우리가 논의하는 VLM+MoveIt 방식과는 접근 철학이 다릅니다. 각각 장단점이 있는데, **학습 기반**은 특정 작업에 높은 성공률을 보이지만 **범용성**이 낮고, **VLM+기획 기반**은 다양한 물체에 적용 가능하나 **실시간 적응성**이 낮다는 것이 일반적 평가입니다.

최근에는 두 접근을 **융합**하려는 노력도 있습니다. 예를 들어, **Xu et al.**(**ICRA 2023**)의 연구는 **Vision-Language-Action**을 하나의 Transformer로 결합하여, **특정 언어 명령에 따라 물체를 잡는** 정책을 end-to-end로 학습하였습니다. 이를 통해 시뮬레이션 상에서 얻은 정책을 실제 로봇에 이식하여, 다양한 물체들에 대해 목표지향 파지를 수행할 수 있었는데, 이러한 방향은 VLM의 인지력을 RL의 정책에 녹여낸 형태라 할 수 있습니다. 또 다른 동향으로, **학습된 정책에 planning 개념을 접목**하는 것도 있습니다. 예를 들어 한 번에 복잡한 파지 동작을 다 하지 않고, **실패 시 추가 동작**(regrasping, 자세 조정 등)을 할 수 있도록 정책을 구성하거나, **LLM을 사용해 고차원 조작 계획**을 세우고 세부 실행을 RL이 맡는 구조 등이 실험되고 있습니다.

**세그멘테이션 기반 파지**도 빼놓을 수 없는 흐름입니다. **Grasp-Anything**과 같은 이름으로, 대규모 세그멘테이션 데이터와 6-DoF 파지 포즈를 연계한 연구들이 나오고 있습니다. 이는 언어보다는 **시각 정보 풍부화**에 가깝지만, 결국 언어 지시가 없을 때라도 **장면 이해**를 향상시켜서 로봇이 스스로 어떤 걸 집을지 판단하게 하는 데 목적이 있습니다. Segment Anything이 출시된 이후 **Grasp Anything**이라는 아이디어 (SAM으로 물체 분할 -> 각 물체에 대해 잡을 곳 결정)를 구현한 사례들도 보고되고 있습니다. 나아가, 언어 조건까지 넣은 **Grasp Anything++** 식의 데이터셋도 ECCV 2024 등에 공개되어, 언어-비전-파지 3요소를 한꺼번에 학습시키려는 시도가 확인됩니다.

한편, 실제 **산업/상용 로봇 분야**에서도 이러한 연구들을 빠르게 응용하고 있습니다. 예를 들어 **ROS-Industrial** 커뮤니티나 **NVIDIA의 Isaac 플랫폼**에서는, 자연어 요약으로 로봇 명령을 생성하거나, VLM을 통해 현장 물체들을 분류/검출하여 동작을 유도하는 프로토타입들이 소개되었습니다. ROS-I Asia Pacific 2024 발표자료에서는 Jetson AGX Orin으로 **Zero-Shot Learning**과 **Natural Language Prompts**를 이용한 창고물류 데모가 논의되기도 했습니다. 또한 NVIDIA Jetson AI Lab에서는 **RoboPoint**라는 튜토리얼을 공개하여, **비전-언어 모델+대화형 LLM**을 접목한 로봇 매니퓰레이션 데모를 선보였습니다. RoboPoint에서는 카메라 영상과 "Pick up the object on the left" 같은 명령을 입력으로 **2D 상의 액션 포인트**를 생성하고, 이를 깊이정보와 합쳐 **3D 타겟 좌표**로 변환한 뒤 모션 플래너로 로봇팔을 구동합니다. 이 데모는 Spot 로봇 등의 실제 하드웨어와 연동되었고, 향후 ROS2 및 Isaac Sim과 통합될 예정이라고 보고되었습니다. 이는 본 연구와 거의 유사한 문제설정을 가지며, **대형 AI 모델을 로봇 제어에 활용**하는 것이 현실에서도 가시적인 성과로 이어지고 있음을 보여줍니다.

지금까지 살펴본 동향들을 요약하면, **자연어 기반 로봇그래스핑**은 \*"언어로 목표 지정 -> 시각 인지 -> 파지 실행"\*이라는 공통 흐름 속에서, 어떤 인지 모델을 쓰는지, 어떤 제어 방법을 쓰는지에 따라 다양한 접근들이 경쟁적으로 발전하고 있습니다. 우리의 NanoOWL + NanoSAM 기반 시스템은 이 중에서도 **경량 VLM + 세그멘터** 조합을 통해 **임베디드 환경**에 최적화하고자 하는 실용적 관점을 취하고 있습니다. 다음 장에서는 이러한 선택( NanoOWL, NanoSAM)의 타당성과 기술적 세부사항을 본격적으로 분석합니다.

## 4. NanoOWL + NanoSAM 기반 시스템의 설계 및 기술 분석 (Proposed System and Analysis)

이 장에서는 사용자의 시스템 구조를 구성하는 핵심 기술 선택에 대해 심층적으로 살펴봅니다. **NanoOWL과 NanoSAM을 선택한 이유**를 기술적 지표들을 통해 분석하고, **시스템 아키텍처**를 단계별로 설명합니다. 또한 유사 기능을 달성할 수 있는 다른 대안들과 비교하여, 왜 NanoOWL+NanoSAM 조합이 Edge 환경의 요구사항을 충족하는 최적의 선택인지 논증합니다.

앞서 요약한 대로, 시스템의 전체 동작 흐름은 **Perception** -> **Planning** -> **Execution**으로 구분할 수 있습니다. 여기서 **인지 단계**에서 NanoOWL+NanoSAM이 활약하며, **계획 단계**에서는 MoveIt2를 활용한 모션 플래닝, **실행 단계**에서는 로봇팔 제어가 이루어집니다. 각 부분에 대해 순차적으로 분석하겠습니다.

### 4.1 시스템 전체 아키텍처 개요

우선, 제안된 시스템의 구성 요소를 한눈에 볼 수 있도록 요약합니다:

* **하드웨어 구성:** Kinova Gen3 Lite 6-DOF 로봇 암, 로봇 그리퍼, Intel RealSense D435 (또는 Azure Kinect) RGB-D 카메라, NVIDIA Jetson AGX Orin (64GB) 컴퓨팅 보드. (Kinova Gen3 Lite는 예시이며, 6자유도 이상의 임의의 매니퓰레이터로 일반화 가능함). Jetson Orin은 로봇 암의 베이스에 장착되거나 옆에 둠으로써 온보드 연산을 담당.
* **소프트웨어 구성:** Ubuntu 22.04 LTS, ROS2 Humble, CUDA 및 TensorRT (JetPack SDK 포함), NanoOWL 및 NanoSAM 엔진, MoveIt2 모션 플래닝 프레임워크, 기타 ROS2 패키지 (카메라 드라이버, 메시지 정의 등).
* **ROS2 노드 구조:**

  * *Camera Node*: 카메라에서 컬러 이미지(`/camera/color_image`)와 깊이 이미지(`/camera/depth_image`) 토픽 발행. 필요 시 포인트클라우드 토픽(`/camera/points`)도 생성.
  * *Perception Node (NanoOWL+NanoSAM)*: 이미지와 자연어 명령을 입력받아 객체의 2D 위치와 마스크를 산출. NanoOWL로 bounding box 검출 후 NanoSAM으로 세그멘테이션. 결과로 **목표 객체의 픽셀 좌표, 분할 마스크, 3D 좌표**를 계산하여 `/perception/target_pose` 등의 토픽으로 발행.
  * *Planning Node*: Perception으로부터 받은 목표 객체의 3D 위치를 입력으로, MoveIt2를 이용해 로봇팔의 파지 경로 계획. 목표 좌표를 향해 그리퍼를 접근시키고 잡는 trajectory 생성. 결과 trajectory를 FollowJointTrajectory 액션 형태로 `/arm_controller`에 전송.
  * *Gripper Control Node*: 그리퍼 여닫기를 제어. Planning Node와 동기화하여 알맞은 타이밍에 그리퍼를 폐쇄하여 물체 집기.
  * *Supervisor/Orchestrator Node*: 상위에서 명령을 파싱하고 각 노드 간 시퀀스를 조율. (예: 사용자의 자연어 명령을 구독하여 Perception Node에 텍스트 프롬프트 설정, 파지 완료 신호를 받아 사용자에게 완료 보고 등). 간단한 상태 기계나 Behavior Tree로 구현 가능.

이상과 같은 구조에서, 핵심적인 혁신 요소는 **Perception Node**의 내부입니다. 이 노드에 NanoOWL과 NanoSAM이 통합되어, **자연어 -> 시각 인지**를 수행합니다. 이를 상세히 들여다보겠습니다.

### 4.2 NanoOWL + NanoSAM 통합 인지 파이프라인

Perception Node의 동작을 **단계별**로 구분하면 다음과 같습니다:

1. **명령 입력 처리:** 사용자의 명령은 예를 들어 `"빨간 컵을 집어"`와 같은 한국어 문장으로 들어옵니다. Supervisor Node에서 이 문장을 받아 **텍스트 분석**을 수행합니다. 우선 **목표 객체 이름**(**"빨간 컵"**)을 추출하고, 필요 시 영어 등 NanoOWL이 이해할 수 있는 언어로 변환합니다. (NanoOWL은 CLIP계열 백본을 쓰므로 다국어를 일부 이해하지만, 안정성 위해 영어로 프롬프트를 주는 것이 안전함). 결과적으로 **text prompt** = `"red cup"`을 생성합니다. 이 프롬프트를 Perception Node에 전달합니다.

2. **영상 획득 및 전처리:** Camera Node로부터 최신 컬러 이미지와 깊이 이미지를 가져옵니다. ROS2에서는 이를 콜백으로 비동기 수신하므로, Perception Node는 내부적으로 **이미지 버퍼**를 유지하고 있다가 명령이 들어오면 가장 최근 이미지를 사용합니다. (Alternatively, Perception Node가 항상 동작하며 매 프레임 탐지하지만, 자원 낭비를 줄이기 위해 명령 시점에만 수행하는 것으로 가정). 전처리로 **렌즈 왜곡 보정**, **해상도 리스케일** 등이 필요할 수 있습니다. NanoOWL 엔진이 학습된 입력 크기에 맞춰 이미지를 resize하고, 색상 채널 순서 등을 맞춰줍니다.

3. **NanoOWL 객체 검출:** 준비된 RGB 이미지를 NanoOWL의 `predict()` 함수에 입력합니다. 이때 text prompt로  `["red cup"]`을 함께 넣습니다. NanoOWL은 GPU상에서 수십 ms 이내에 추론을 완료하고, 해당 텍스트와 매칭되는 객체의 **바운딩 박스 리스트**를 반환합니다. 빨간 컵이 한 개라면 하나의 상자와 confidence 점수가 나오고, 여러 개라면 여러 상자가 나올 것입니다. 여기서는 한 개의 물체만 지시한다고 가정하므로 상자 하나를 얻었다고 하겠습니다. 예를 들어 좌표가 `(x_min, y_min, x_max, y_max)` 형태로 나오며, 이는 원본 이미지 상의 픽셀 좌표입니다.

4. **3D 좌표 계산:** 깊이 이미지에서 위 바운딩 박스 영역에 해당하는 깊이 값을 추출합니다. 만약 RGB-D 카메라가 내장 합성 point cloud를 주는 경우, box 영역의 포인트클라우드를 모아 **중앙 평균점**이나 **가장 가까운 점** 등을 계산할 수 있습니다. 혹은 깊이 이미지라면 상자 중심 픽셀의 깊이값을 뽑아낼 수도 있습니다. 이 깊이값 `d`와 픽셀 좌표 `(u,v)`를 카메라의 내재파라미터 K에 역투영하면 **카메라 좌표계 3D 점 **(**X, Y, Z**)을 얻을 수 있습니다. 이를 **월드 좌표계** 또는 **로봇 기준좌표**로 변환하려면, 카메라의 외부파라미터 (카메라->로봇 변환 행렬)를 적용합니다. 결과적으로 목표 컵의 3차원 위치 추정치 `Pt_target`를 얻게 됩니다. 이 값은 이후 모션 계획에 직접 사용되므로, 정확도가 높아야 합니다. (만약 물체가 테이블 위에 있어 깊이 센서에 일부 가려진다면, 상자 영역 내 여러 깊이 포인트의 **평균 깊이**를 사용하거나, 신뢰할 수 있는 전처리 필터를 적용합니다.)

5. **NanoSAM 세그멘테이션:** NanoOWL 결과 상자의 정확도를 보완하고자, NanoSAM에 이미지와 NanoOWL 결과를 입력합니다. NanoSAM은 기본적으로 **프롬프트로 box나 point**를 받을 수 있는데, 여기서는 **박스 프롬프트**로 NanoOWL의 bounding box 좌표를 사용합니다. 즉, 이미지와 box = (x\_min, y\_min, x\_max, y\_max)를 주면, 그 박스 내에 존재하는 주요 객체의 마스크를 출력할 것입니다. NanoSAM은 해당 영역을 좀 더 정교하게 살펴보고, 경계선을 픽셀 단위로 조정하여 **정확한 컵 모양**의 마스크를 제공해줄 것입니다. (참고로, MobileSAM/SAM 계열은 박스 프롬프트를 주면 그 영역을 crop한 뒤, 마스크를 내고 다시 전체 이미지 좌표로 변환하는 프로세스를 거칩니다. 이때 한 박스 안에 여러 객체가 있으면 모두 분할할 수도 있지만, 컵만 존재한다면 하나만 나옵니다.)

6. **결과 통합 및 출력:** 최종적으로 Perception Node는 목표 객체의 **2D bounding box**, **세그멘테이션 마스크**, **3D 좌표**를 확보했습니다. 이 정보를 하나의 ROS2 메시지(예: `TargetObject.msg` 커스텀 메시지)로 구성하여 출판합니다. 이 메시지에는 `geometry_msgs/PointStamped position`, `sensor_msgs/Image mask` (또는 마스크의 Polygon 등), `float32 confidence` 등이 들어갈 수 있습니다. 다른 노드, 특히 Planning Node는 이 출판을 구독하여 다음 단계를 이어가게 됩니다.

여기서 핵심 성능 지표는 **latency**입니다. NanoOWL+NanoSAM 파이프라인은 Jetson AGX Orin에서 매우 빠르게 동작할 것으로 예상됩니다. 앞서 인용한 수치들을 종합하면, NanoOWL ViT-B/32 추론 ~~10ms, NanoSAM 추론 8ms 정도이며 이미지 전처리와 후처리를 합쳐도 **30ms (0.03초)** 내외로 1회 인지를 완료할 수 있습니다. 이는 실시간 요건인 30 FPS (33ms)보다 빠른 것이며, 명령 입력에 대한 로봇 응답 시간을 최소화해줍니다. 만약 ViT-B/16 모델을 쓴다면 NanoOWL이 25FPS 수준이므로 약간 느려지겠지만 여전히 수백 ms 이내입니다. 본 시스템은 **사람의 음성 명령** 등을 받을 수도 있는데, 음성 인식에 수백 ms~~1초 정도 소요되는 것을 감안하면, 인지쪽 지연 30ms는 무시할 만한 수준입니다. 따라서 전체 파이프라인의 병목은 오히려 **모션 계획 단계**가 될 것입니다. (MoveIt2 경로계획이 복잡한 환경에서는 수백 ms \~ 수 초까지 걸릴 수 있는데, 이는 4.3절에서 다룸).

NanoOWL+NanoSAM 조합을 사용할 때의 **메모리 사용량**도 고려해야 합니다. Jetson AGX Orin 64GB 모델이기에 여유가 있지만, GPU 메모리는 15GB 선으로 관리하는 것이 권장됩니다. NanoOWL ViT-B/32 TensorRT 엔진은 약 1~~2GB, NanoSAM 엔진도 1GB 이하로 알려져 있습니다. 실행 시 배치=1 추론에 각 1~~2GB GPU 메모리 사용, 중간 특성 맵 등 합쳐도 4GB 안팎이면 충분합니다. 이는 Orin의 15GB GPU 중 30% 이내이며, 나머지 자원은 모션플래너나 기타 노드, 혹은 여분으로 남겨둡니다. 비교하자면, Grounding DINO + SAM 원본 조합은 GPU 8\~9GB 이상을 요구하고 속도도 느리므로 Orin에서 동시 실행이 버거울 수 있습니다. Nano 모델들은 이러한 자원 문제에서도 훨씬 효율적이어서, **Edge 배포에 적합**합니다.

### 4.3 MoveIt2 기반 모션 계획 및 그래스핑 실행

Perception Node로부터 목표 물체의 3D 위치(`Pt_target`)와 그에 대응하는 마스크(`mask_target`)를 전달받은 **Planning Node**는, 이제 로봇팔을 움직여 물체를 잡는 작업을 수행합니다. 여기서는 ROS2 환경에서 표준적으로 사용되는 **MoveIt2** 라이브러리를 이용합니다. MoveIt2는 경로 계획(Planner)와 역기구학, 충돌체크, 그리퍼 동작 등을 포괄하는 프레임워크로, Kinova Gen3 Lite와 같은 로봇팔을 위한 패키지도 이미 존재합니다. Planning Node의 동작은 대략 다음 순서로 진행됩니다:

1. **목표 파지 자세 결정:** 입력으로 받은 `Pt_target` (월드 좌표계의 물체 위치)을 바탕으로, 로봇 그리퍼의 파지 위치 및 방향을 결정합니다. 가장 단순한 방법은 **사전 정의된 접근 방식**을 사용하는 것입니다. 예컨대, 컵과 같은 물체는 윗부분을 잡기 위해 보통 **상부에서 수직으로 내려**잡는다고 가정할 수 있습니다. 그러면 `Pt_target` 위쪽 약 10cm 지점에 그리퍼를 펼친 채로 이동하여 내려오는 접근 경로를 계획합니다. 만약 대상 물체에 대한 추가 정보(마스크 등)로 자세한 orientation을 산출할 수 있다면 더 정교한 결정도 가능합니다. 예를 들어 마스크를 3D 포인트로 변환한 후 PCA를 수행하면 물체의 주축을 추출할 수 있고, 컵 손잡이가 달린 물체라면 손잡이 방향으로 접근하도록 오리엔테이션을 설정할 수도 있습니다. 그러나 일반적인 시나리오에서는 복잡도를 줄이기 위해 **사전 약속된 접근 방향**(예: 위에서 집기 또는 측면에서 집기)을 택합니다. 여기서는 상단 접근을 가정합니다. 따라서 목표 파지 자세는 위치 = `Pt_target + (0,0,+Z_offset)`, 방향 = gripper z축이 -Z(world) 방향을 향하도록(즉, 그리퍼가 아래로 향하도록) 설정합니다. 이 정보를 PoseStamped 형식으로 준비합니다.

2. **경로 계획 요청:** MoveIt2의 서비스/액션 인터페이스를 통해, 로봇팔 End-Effector(말단) 링크를 위에서 정의한 목표 Pose로 움직이는 경로를 요청합니다. 이때 **경로 제약사항**으로 로봇베이스와 주변 환경(테이블 등)의 충돌 모델을 함께 넣어 안전한 경로를 찾게 합니다. Planning Pipeline은 OMPL 등의 알고리즘으로 Inverse Kinematics (IK) 해를 찾고, joint space에서 motion trajectory를 생성합니다. 경로가 성공적으로 생성되면 trajectory 메시지가 반환됩니다. 실패할 경우, 약간 다른 Pose를 시도하거나 (ex: 조금 더 위에서부터 접근 or 각도를 미세 조정) 재시도합니다.

3. **그래스프 동작 추가:** Trajectory가 완성되면, 그 끝에 **그리퍼 폐쇄 동작**을 추가합니다. Kinova Gripper의 폐쇄는 하나의 관절 명령 또는 별도 명령으로 줄 수 있는데, MoveIt2에서는 **Pickup**이라는 고수준 인터페이스도 있습니다. 여기서는 간단히, 경로의 마지막 점까지 이동한 후 그리퍼를 닫는 신호를 보낸다고 생각하면 됩니다. Planning Node는 Gripper Node와 협업하여, 경로 실행 중 마지막에 도달하면 `gripper/close` 명령을 publish합니다.

4. **경로 실행:** 산출된 trajectory를 실제 로봇 제어기로 보내 실행합니다. ROS2에서는 `FollowJointTrajectory` 액션으로 각 관절의 목표각 및 시간 프로파일을 컨트롤합니다. Jetson Orin이 실시간 제어를 직접 하지 않고, 로봇 컨트롤러 보드가 따로 있다면 해당 보드(Firmware)가 이 명령을 받아 수행합니다. (Kinova Gen3 Lite는 자체 임베디드 컨트롤러가 있어, ROS2 드라이버가 명령을 전달). 로봇팔이 경로를 따라 이동하면서 목표 물체로 접근합니다.

5. **파지 완료 및 후처리:** 그리퍼가 닫히고 물체를 집었으면, 이를 확인하기 위해 **토크 피드백**이나 **관절 상태**를 체크합니다. 그리퍼의 폐쇄 정도로 물체가 잘 잡혔는지 판단할 수 있습니다 (예: gripper 모터 전류가 임계값 이상이면 물체를 물었다고 간주). 성공 여부 플래그를 Orchestrator에 보고합니다. 성공이라면, 물체를 들어올려 원위치로 약간 이동시키거나 사용자에게 전달합니다. 실패라면, 놓쳤음을 알리고 재시도 루프에 들어갈 수 있습니다. 재시도 시에는 Perception부터 다시 실행하거나, 또는 잡기 전에 여러 각도로 시도해보는 로직을 추가할 수 있습니다.

이상의 과정에서, **NanoOWL+NanoSAM**으로부터 받은 정보는 주로 목표 위치 결정에 활용되고 **파지 품질**에도 영향을 줍니다. 마스크를 얻었지만 현재 계획에서는 단순히 중심을 잡았는데, 향후 확장으로 **마스크 기반 Grasp Pose estimation** 알고리즘 (예: 잘린 물체 윤곽에서 평면 추출해 그리퍼 jaw 배치)을 활용하면 더 향상된 파지 안정성을 얻을 수 있습니다. 실제 연구에선 SAM으로 얻은 마스크의 윤곽선에 최적 그립점을 찾는 시도도 존재하며, **학습된 grasp detector**를 마스크 영역에 적용하면 clutter 속에서도 정확한 집기 자세 산출이 가능하다는 보고가 있습니다.

본 시스템은 **ROS2** 위에서 구동되므로, 멀티 노드 구성을 잘 활용하면 각 기능을 병렬로 동작시키거나, CPU/GPU 사용을 분산시킬 수 있습니다. 예를 들어 Perception Node는 GPU를 많이 쓰지만, Planning Node는 대부분 CPU 연산(IK, 경로탐색)입니다. Jetson Orin의 Carmel CPU(12core)를 활용하여 ROS2 Executor들을 적절히 분리하면, 인지와 계획을 동시 처리하거나, 여러 쓰레드로 파이프라인 딜레이를 줄일 수 있습니다. ROS2의 QoS 설정으로 이미지 토픽의 버퍼링이나 신뢰성을 제어하고, Real-Time kernel 세팅을 적용하면 더욱 정확한 제어 주기를 보장할 수 있습니다. 이런 **시스템 엔지니어링** 측면은 본 보고서의 주요 범위는 아니지만, 실제 구현 시 성능 튜닝에 중요할 것입니다.

### 4.4 NanoOWL + NanoSAM 선택의 이점 분석 (대안 대비)

이제, NanoOWL과 NanoSAM을 채택한 것이 왜 기술적으로 우수한 선택인지, 다른 가능한 조합들과 비교하며 정리하겠습니다. 고려할만한 대안으로는:

* **대안 A:** Grounding DINO + Original SAM (혹은 MobileSAM) 조합
* **대안 B:** Grounding DINO + MobileSAMv2 (경량 SAM) 조합
* **대안 C:** MobileViT/CLIP 기반 다른 open-vocab detector + MobileSAMv2
* **대안 D:** 강화학습 기반 E2E grasp (이미지->gripper 제어 직접학습)
* **대안 E:** 사람 손으로 설계한 전통 알고리즘 (기하학+색 기반 인식 + 규칙기반 파지)

대안 D, E는 앞서 배경에서 논했듯 범용성과 성능 면에서 현대적 요구를 충족하기 어려우므로, 여기서는 **대안 A, B, C**의 CNN/Transformer 기반 인지 조합들과 NanoOWL+NanoSAM (**대안 X**)을 중심으로 비교합니다. 주요 비교 기준은 **실시간성(FPS)**, **탐지 정확도**, **세그멘테이션 품질**, **시스템 경량성(메모리)**, **ROS2 통합 용이성** 등이 있습니다.

* **추론 속도 (Real-Time FPS):** NanoOWL+NanoSAM (대안 X)은 Jetson AGX Orin에서 종합적으로 **90 FPS 이상**의 객체인지 파이프라인 성능을 보일 것으로 추산됩니다. 실제로 NanoOWL 95 FPS, NanoSAM 125 FPS로 둘 다 충분히 빠르며, 연동 시 약간의 오버헤드를 고려해도 카메라 프레임레이트(30Hz)를 완전히 커버합니다. 반면 Grounding DINO 기반 조합들은 Grounding DINO가 무거워 **10\~30 FPS** 수준에 불과합니다. 예를 들어 **Grounding DINO + MobileSAMv2** 조합은 약 30 FPS 정도로 보고되며, **원본 SAM을 쓸 경우(Grounded-SAM)** 15 FPS도 채 안 됩니다. MobileVLM 등 다른 경량 모델과 MobileSAMv2를 써도 20-25 FPS 정도입니다. 따라서 **Edge 로봇의 실시간 인지**라는 측면에서 NanoOWL+NanoSAM이 가장 우수합니다. 특히 움직이는 로봇이나 영상에서 프레임 손실 없이 추론하려면 30FPS 이상이 필수인데, Nano 모델만이 이를 만족합니다.

* **Zero-Shot 인식 정확도:** 정확도 비교는 다소 복잡한데, **탐지 정확도**(정해진 목표 객체를 제대로 찾아냈는가)와 **분할 품질**로 나눠볼 수 있습니다. NanoOWL은 OWL-ViT Base 모델을 사용하기에 Grounding DINO(Swin-L 등 대형 백본)보다 약간 성능 열세일 수 있습니다. 예를 들어 한 내부 테스트에서 **탐지 정확도**를 100점 만점 기준 평가 시 NanoOWL+NanoSAM 조합이 85점, GroundingDINO+MobileSAMv2가 90점으로 보고되기도 했습니다. 이는 Grounding DINO가 대형 모델이라 텍스트-이미지 매칭이 더 정교할 가능성을 시사합니다. 그러나 85% vs 90%의 차이는 **트레이드오프**로 볼 수 있습니다. 또한 OWL-ViT도 상위 버전(ViT-L/14 등)이 존재하여, 만약 Orin에서 구동 가능하도록 최적화한다면 정확도를 높일 여지가 있습니다. **세그멘테이션 품질**은 NanoSAM(ResNet18 기반)이 MobileSAMv2(MobileViT 기반) 대비 근소하게 높다고 알려져 있습니다. 대체로 SAM 원본 품질을 100으로 보면 NanoSAM \~95, MobileSAMv2 \~90 수준으로 추정됩니다. 그러므로 분할 정밀도 면에서는 NanoSAM이 앞서, 결과적으로 **픽셀단위로 그리퍼 경로에 영향**을 주는 단계에서 이점이 있습니다. 종합적으로 봤을 때, NanoOWL+NanoSAM은 **약간의 정확도 손실을 감수하고 속도를 대폭 높인 조합**이며, 그 정확도 손실 폭이 크지 않아 실제 로봇 응용에서 충분히 유용한 수준입니다.

* **메모리 및 연산 자원 효율:** 앞서 언급했듯 NanoOWL+NanoSAM은 약 3~~4 GB GPU 메모리를 사용하며, CPU 부하도 대부분 이미지 전처리 수준입니다. 반면 GroundingDINO+SAM 원본은 8~~9 GB, MobileSAMv2 조합들도 5~~7 GB까지 사용해 Jetson Orin 32GB 모델에서는 간신히, 16GB 모델에서는 어려운 수준입니다. 또한 Nano 모델들은 TensorRT 비동기 실행 등에 최적화되어 CPU 개입이 적지만, 다른 조합은 PyTorch 실행이나 custom ops로 CPU synchronization이 발생해 임베디드 CPU 점유율도 높습니다. 따라서 **Edge 배포 용이성** 면에서 NanoOWL+NanoSAM이 가장 가볍습니다. 전력 소모도 Nano 모델이 수십 Watt 대로, 대형 모델 구동시 50W 이상 치솟는 것과 대비됩니다. Jetson Orin의 파워 모드에서 Nano 조합은 20W 모드로도 돌아갈 수 있지만, 대안들은 40~~50W 모드가 필요할 수 있습니다.

* **ROS2 통합 및 활용성:** NanoOWL은 NVIDIA가 공식 ROS2 패키지를 제공하고 있고, NanoSAM도 곧 유사하게 통합이 될 것으로 기대됩니다. 대안 모델들은 ROS2 패키지가 공식적으로 없거나 서드파티 구현에 의존해야 합니다. 물론 Grounding DINO, SAM 모두 Python API 사용이 가능하므로 ROS2 노드로 작성할 수 있지만, NanoOWL의 경우 **docker 환경까지 포함된 원스톱 패키지**로 제공되므로 구현 부담이 적습니다. 또한 NanoOWL/NanoSAM 조합은 둘 다 NVIDIA Isaac 예제에서 함께 사용된 전례가 있어 상호 호환성이 검증되었습니다. 한편, Grounding DINO + SAM 조합은 모델 간 인터페이스 연결을 수동으로 해야 하고, latency 관리도 개발자 몫입니다.

* **기능 확장성:** NanoOWL은 **텍스트 입력**을 기본으로 하지만, **이미지 샘플**을 통해 one-shot detection도 가능한 OWL-ViT의 성질을 잇고 있습니다. 이는 향후 **한 번 본 물체를 기억**하여 찾는 기능 등으로 확장할 수 있음을 의미합니다. Grounding DINO 등도 유사한 기능이 있으나, NanoOWL처럼 경량 엔진으로 구현된 것은 아직 없습니다. NanoSAM은 **다중 프롬프트**를 지원하므로, 여러 객체를 동시에 세그멘테이션하거나, 한 객체에 점+박스 같이 복합 프롬프트를 넣어 더욱 정확히 분할할 수 있습니다. 이 역시 추후 **사용자 피드백**(점 찍어 교정 등) 인터랙션에 도움이 될 것입니다. 이러한 **유연성 측면**에서 NanoOWL+NanoSAM은 기본적으로 SAM과 OWL-ViT의 기능을 받으므로 우수합니다. RL 기반 접근법의 경우 환경 변화에 적응하는 장점이 있지만, 언어 지시의 유연성 면에서는 VLM만 못하고, 세밀한 객체 구분이 어려운 단점이 있습니다.

以上 내용을 표로 간략히 요약하면 다음과 같습니다:

| **비교 항목**       | **NanoOWL + NanoSAM (본 연구)** | Grounding DINO + MobileSAMv2 | Grounded-SAM (DINO+SAM) | RL 기반 (예: SAC 정책)  |
| --------------- | ---------------------------- | ---------------------------- | ----------------------- | ------------------ |
| **FPS (추론 속도)** | \~95 FPS (엔드투엔드 30fps 이상)    | \~30 FPS                     | \~15 FPS                | **>1000 FPS** (정책) |
| **탐지 정확도**      | 높음 (약 85% 수준)                | 매우 높음 (90%)                  | 보통 (75%)                | 대상 식별 불확실          |
| **세그멘테이션 품질**   | 높음 (원본에 근접)                  | 중간-높음                        | 매우 높음 (원본 SAM)          | N/A (사용 안 함)       |
| **GPU 메모리**     | 3-4 GB                       | 5-6 GB                       | 8-9 GB                  | 1 GB 이하            |
| **Edge 최적화**    | TensorRT 최적화 (우수)            | 일부 최적화                       | 미흡 (대형모델)               | 경량 (우수)            |
| **제로샷 학습**      | 텍스트 지칭 가능 (우수)               | 텍스트 지칭 가능                    | 텍스트 지칭 가능               | 언어 조건 별도 필요        |
| **일반화**         | 미학습 객체 가능 (우수)               | 미학습 객체 가능 (우수)               | 미학습 객체 가능 (우수)          | 훈련객체 외 어려움         |
| **즉각 대응/재시도**   | 부분적 (재인식 필요)                 | 부분적                          | 부분적                     | 우수 (정책반복)          |
| **ROS2 지원**     | 공식 패키지 (양호)                  | 비공식/커스텀                      | 비공식/커스텀                 | 커스텀 구현 필요          |

*(주: 위 표의 수치는 보고서 내 인용과 여러 자료를 종합한 추정치입니다.)*

위 비교에서 알 수 있듯, **NanoOWL + NanoSAM** 조합은 임베디드 로봇의 요구에 아주 잘 부합하는 **균형 잡힌 선택**입니다. 대형 모델 대비 충분한 성능을 내면서도 속도와 효율은 탁월하고, ROS2 에코시스템과 쉽게 통합됩니다. 특히 **Jetson AGX Orin**이라는 제한된 플랫폼을 최대한 활용하여, 탁월한 **실시간 자연어 인지** 기능을 구현할 수 있다는 것이 핵심 강점입니다.

### 4.5 최신 연구 흐름에 비춘 타당성 및 확장 가능성

NanoOWL+NanoSAM 기반의 언어지령 그래스핑 시스템이 **최신 연구 흐름**과 부합하는지를 살펴보면, 긍정적인 면이 매우 많습니다. 우선, 최근 CVPR/ICCV 등의 학회들을 보면 **Open-Vocabulary Detection**과 **Foundation Segmentation**이 하나의 트렌드로 자리잡았습니다. 우리의 인지모듈은 바로 그 최전선 모델들을 실제 로봇에 적용한 형태로, **학계의 최신 성과를 현실 시스템으로 번역**한 예라 할 수 있습니다. 이는 연구적으로도 의의가 큰데, 예컨대 WACV 2025 논문에서는 소량의 어노테이션으로 로봇용 인스턴스 세그멘테이션 모델을 학습시키는 방안을 다루며, Sam이나 Owl-ViT 같은 모델들이 로봇 grasping 성능 향상에 기여하는 점을 분석하고 있습니다. 이러한 맥락에서, NanoOWL+NanoSAM은 로봇 분야 연구자들이 관심 갖는 **일반화된 인지** 능력을 제공하므로, **연구 타당성**이 높습니다.

**확장 가능성** 측면에서도 여러 방향이 열려 있습니다. 본 시스템을 발전시키거나, 다른 환경에 적용하려는 경우 다음과 같은 확장·응용이 가능합니다:

* **다중 객체 및 과제 확장:** 현재는 한 번에 하나의 물체를 집는 시나리오이지만, 명령에 따라 여러 물체를 차례로 집거나 특정 물체만 골라내는 등으로 확장할 수 있습니다. 예를 들어 "책상 위에 컵 두 개를 차례로 집어 상자에 넣어"라는 복합 지시도, NanoOWL로 컵을 모두 탐지하고 순차적으로 처리하도록 할 수 있습니다. 또는 "파란 공 3개 중 가장 오른쪽 것을 집어"처럼 속성+공간 지정도 Referring Grasping 기술과 결합해 구현 가능합니다.

* **로봇 이동 통합:** 현재는 고정된 카메라와 고정된 로봇팔이지만, 이를 **모바일 로봇**에 탑재하면 자율이동까지 포함한 작업이 가능합니다. ROS2 Nav2 패키지와 연계하여 "거실로 가서 테이블 위 빨간 컵을 주워와" 같은 명령을 수행하려면, 우선 내비게이션으로 해당 위치로 이동한 뒤 우리 그래스핑 파이프라인을 실행하면 됩니다. Jetson Orin은 이동 로봇의 메인 컴퓨터로도 쓰이기에, 한 장치에서 비전과 제어를 모두 담당할 수 있습니다. 이러한 모바일 매니퓰레이터 통합은 실제 서비스 로봇에 한걸음 다가가는 중요한 확장입니다.

* **비정형/변형 객체로 확장:** 현재 파지는 주로 딱딱한 물체를 대상으로 하지만, 옷이나 끈과 같은 **deformable object**에 대한 조작도 큰 도전과제입니다. NanoSAM의 범용 세그멘테이션 능력은 옷가지 등의 형상을 분리하는 데도 쓰일 수 있으므로, 이를 활용해 옷을 집어 정리하는 등의 작업으로 나아갈 수 있습니다. 최근 **Deformable Gym**이나 **SoftGym** 같은 시뮬레이터와 학습 기법들이 나오고 있으므로, 우리 시스템의 인지 모듈과 그런 변형물체 파지 정책을 결합하면 새로운 연구로 발전할 수 있습니다.

* **Multi-modal 향상:** 현재 비전+언어만 사용했지만, 로봇에는 촉각센서, 힘센서 등 **다양한 센서**를 추가할 수 있습니다. 예를 들어 잡는 동안 힘 센서로 미끄러짐을 감지해 재그립을 시도하거나, 촉각센서로 물체의 미세한 위치를 파악해 파지 오차를 보정하는 식입니다. 이러한 멀티모달 통합은 **PhysObjects** 같은 물리 정보가 포함된 데이터셋이나 Sim2Real 기법을 통해 연구되고 있으며, 본 시스템에서도 향후 도입하면 성공률을 더욱 높이고 다양한 상황에 대응하게 할 수 있습니다.

* **고차원 행동으로 확장:** 물체를 집은 후 후속 동작(쌓기, 조립, 사용 등)으로 임무 범위를 넓힐 수 있습니다. 이때도 NanoOWL+NanoSAM으로 환경을 인지하면서 LLM 기반의 **계획 모델**을 접목하면, 사용자 명령을 복잡한 작업으로 분해하여 수행하는 **언어->행동 계획**이 가능해집니다. 이는 최근 여러 연구 (SayCan 등)에서 시도되는 방향으로, 우리 시스템을 그런 프레임워크의 하위 모듈로 포함시킬 수 있습니다.

결국, NanoOWL + NanoSAM 기반 그래스핑 시스템은 **현재 시점에서 현실적으로 구현 가능한 최첨단 기술의 집약체**이며, 동시에 **미래 발전을 위한 훌륭한 플랫폼** 역할을 할 수 있습니다. Jetson AGX Orin의 성능은 향후 Orin NX, Orin Nano와 같은 보급형 모델에도 파급되어 더 저렴한 플랫폼에서도 구동될 수 있고, 반대로 차세대 Thor GPU 모듈 등으로 성능을 높이면 더 많은 모델을 동시에 올리거나 고해상도 인지를 실현할 수도 있습니다. ROS2 기반으로 구축되었기 때문에 새로운 알고리즘 모듈 교체도 비교적 용이합니다.

마지막으로, 본 연구 선택이 유용성을 가지는 영역을 꼽자면 **물류 창고 자동화**, **가정용 로봇 도우미**, **재활 의료 보조** 등이 있습니다. 물류 창고에서 사람 작업자가 음성으로 "저기 3번 박스에서 파란색 공책을 집어서 컨베이어에 올려" 하면 로봇이 수행하거나, 장애인이 로봇에게 "컵을 집어 내게 건네줘"라고 명령하는 시나리오를 상정할 수 있습니다. 이러한 응용에서 요구되는 것은 **다양한 물체**를 **정확히** 인식하고 **안전하게** 잡는 것인데, 본 시스템의 개방형 인지와 세밀한 세그멘테이션이 큰 강점으로 작용할 것입니다.

## 5. 실험 계획 및 기대 성능 (Experiments and Expected Performance)

본 장에서는 제안된 시스템을 평가하기 위한 실험 구성과, 예상되는 성능 지표, 그리고 대안 접근법과의 비교 실험 방안을 제시합니다. 로봇 시스템의 평가는 단순 정확도 이상의 다양한 요소 (성공률, 속도, 견고성 등)를 포함하므로, 정성/정량적 측면에서 고루 검토해야 합니다.

### 5.1 실험 환경 및 시나리오

**실험 환경:** 실제 Kinova Gen3 Lite 로봇팔과 RGB-D 카메라를 사용하여 실내 책상 위에서 테스트를 진행합니다. 실험 공간에는 여러 가지 일상 물체 (컵, 병, 과자상자, 장난감 등)를 놓아두고, 배경에는 책, 모니터 등 일반적인 사무실 물건이 있도록 구성합니다. 조명 조건은 주간 자연광+형광등 정도의 보통 밝기에서 시작하여, 일부 실험에서는 어둡게 또는 불규칙 그림자가 지게도 변화를 줍니다.

**평가 시나리오:** 난이도를 고려하여 몇 가지 시나리오를 설계합니다:

* *시나리오 A: 단일 물체, 단순 배치* – 빈 책상 위에 물체 1개만 올려둠. (가장 이상적인 조건)
* *시나리오 B: 단일 물체, 복잡 형상* – 잡기 어려운 형태의 물체 1개 (예: 손잡이 달린 머그컵, 봉제인형 등).
* *시나리오 C: 다수 물체, 클러터* – 여러 물체가 뒤엉켜 놓여있는 환경에서 목표 물체 지정.
* *시나리오 D: 부분 가림, 난이도 상승* – 목표 물체 일부가 다른 물체 밑에 깔리거나 가려져 있음. 혹은 투명 용기 안에 있음.
* *시나리오 E: 이동 로봇 통합* (추가 가능) – 물체가 로봇팔 작업공간 밖에 있어, 로봇이 이동해서 접근해야 하는 경우 (이 경우 Nav2 이동 후 시나리오 A와 유사한 파지 수행).

**데이터셋 활용:** 정량적 비교를 위해 **OCID (Occluded Object Dataset)** 등 공개 데이터셋의 장면을 활용합니다. OCID는 복잡하게 겹친 물체들에 대한 RGB-D 이미지와 마스크, 6D pose 정보를 제공하며, 이를 우리 시스템에 입력하여 결과를 측정합니다. 특히, 최근 OCID에 **Vision-Language Grasping (VLG)** 태스크를 위한 확장 어노테이션을 추가한 버전이 있다고 가정하고 사용합니다. 여기에는 각 장면에 대해 텍스트 지시 (예: "잡지 위에 있는 파란 병 집어")와 그 목표 물체 ID가 주어져 있어, 우리의 VLM 기반 시스템의 정확도를 평가할 수 있습니다.

**비교 대상:** 4.4절의 대안 중 대표적으로 **강화학습 기반 파지 시스템**과 **VLM+MobileSAM 시스템** 두 가지를 비교군으로 테스트합니다.

* VLM+MobileSAM 시스템은 NanoOWL 대신 Grounding DINO (TensorRT 최적화판이 없으므로 PC에서 1080Ti GPU로 30FPS 정도 동작) + MobileSAMv2를 사용하고, 나머지 파지 및 ROS2 구성은 동일하게 맞춥니다. 이를 통해 **인지 모듈 차이**만 비교할 수 있습니다. Jetson에서는 속도 한계로 전체 실험은 어렵고, 샘플 장면 몇 개에서 결과를 수동 비교하거나, PC에서 시뮬레이션으로만 진행합니다.
* 강화학습 기반 시스템은 시뮬레이터(PyBullet 혹은 Isaac Gym)에서 SAC 알고리즘으로 학습된 파지 정책을 사용합니다. 이 정책은 카메라 영상을 입력으로 받고 end-to-end로 6개 조인트 속도를 출력하도록註. 명령은 언어로 주어지지 않고, 목표 객체 ID를 numerical label로 정책에 넣어주는 형태 (즉, "어떤 객체 잡아"를 ID로 지정)로 가정합니다. 이 비교군은 **명령 분별 없이** 가까운 물체를 잡도록 학습된 버전도 추가로 둡니다. 그리고 이를 실제 Kinova Gen3 Lite에 배포하여 동일/유사한 시나리오에서 테스트합니다.

註: 실제 RL 정책에 언어 임베딩을 넣는 것도 가능하나, 학습 복잡도 증가로 여기서는 생략하고 단순 비교위해 ID만 사용.

### 5.2 평가 지표

평가는 다음과 같은 **정량적 지표**들과 **정성적 관찰**을 병행합니다 (일부 지표는 VLG 데이터셋 기반):

* **그래스핑 성공률 (Grasp Success Rate, GSR)**: 최종적으로 목표 물체를 안정적으로 집어 들었는지의 비율. 각 시나리오에서 20회 이상 시도하여 성공/실패 횟수를 집계. Success 정의는 물체를 들어올려 5초 이상 유지하면 성공으로 간주.
* **명령 이해 정확도 (Command Interpretation Accuracy, CIA)**: 사용자의 언어 명령에 따라 **올바른 물체를 선택**했는지의 정확도. 이는 인지 모듈의 오류(잘못된 객체 탐지)나, RL정책의 경우 목표 구분 실패 등을 측정. VLG 데이터셋에서 지시된 물체와 실제 집은 물체의 일치율로 계산.
* **엔드투엔드 처리 시간 (E2E latency)**: 사용자의 명령 입력 시점부터 로봇이 물체를 집어들기까지 걸린 시간. 이를 다시 **인지단계 지연**, **계획+실행 지연**으로 분할 분석. 초시계+로그로 측정하거나 ROS2 노드 간 타임스탬프로 계산.
* **세그멘테이션 IoU**: 인지 결과로 나온 마스크와 그라운드트루스 마스크 간 IoU(Intersection over Union). (OCID-VLG 데이터셋에 GT마스크 있다고 가정) NanoSAM의 분할 성능 평가용.
* **그립 포즈 오차**: (선택사항) 만약 GT 6D pose를 알고 있다면, 잡은 후 물체의 실제 포즈와 이상적 정착 포즈 간 편차 (rotation, translation error) 측정. 혹은 여러 카메라로 촬영해 잡은 지점의 위치를 평가.

정성적 평가로는:

* **고장 사례 분석**: 실패한 케이스에서 무엇이 문제였는지(예: 인지가 잘못됨, 경로 충돌 미처리, 그리퍼 미끄러짐 등) 원인 파악.
* **견고성**: 조명 변화, 배경 변화, 잡을 물체가 흔들릴 때 등에서 시스템의 견고성 관찰.
* **사용자 경험**: 실제 사람에게 여러 명령을 말하게 하여, 로봇의 응답 속도와 정확도에 대한 피드백. (이는 주관적이지만 중요함)

### 5.3 기대 성능 및 가설

**가설 1: VLM 기반 접근은 RL 기반보다 **목표 지정 정확도**가 높다.**
시나리오 C와 같이 여러 물체 중 특정 물체를 집는 상황에서, VLM(NanoOWL) 시스템은 텍스트 임베딩을 통해 목표를 잘 식별하여 **원하는 물체만 집는 성공률이 높을 것**입니다. 반면 RL 정책은 학습 데이터에 없는 구분은 못하므로 오동작 가능성이 높습니다. 기대되는 CIA 측정에서 NanoOWL 시스템은 거의 100%에 가까운 정확도를 보일 것이고, RL 기반은 50% 미만일 수도 있습니다.

**가설 2: RL 기반 접근은 단순 환경에서 **속도** 및 **재시도 대응** 면에서 장점이 있다.**
시나리오 A와 같이 쉬운 조건에서는 RL정책은 **2초 이내**에 바로 동작을 내어 잡을 수 있고, 만약 실패해도 연속적으로 다시 시도할 것입니다. VLM+MoveIt 시스템은 한 번 인지하고 경로 계획하는 데 3\~5초 정도 걸릴 수 있어 (인지 0.03s + 계획 2s + 실행 2s), 평균 파지 시간이 RL보다 길 것으로 예상됩니다. 따라서 **평균 파지 시간** 지표를 계산하면 RL이 더 낮게 나올 가능성이 있습니다. 하지만 이는 단순 조건에서이고, 복잡 조건에서는 VLM 쪽이 실패를 적게 하므로 전체적인 작업 완료 시간은 유리할 수 있습니다.

**가설 3: NanoOWL+NanoSAM은 GroundingDINO+MobileSAM 대비 **실시간성에서 우수**하나, **탐지 누락**이 약간 있을 수 있다.**
NanoOWL이 약간 정확도 손해를 본다는 전제에서, 몇몇 장면에서는 Grounding DINO는 찾아내는 작은 물체를 NanoOWL은 놓칠 수 있습니다. 그러나 대부분 주요 물체는 다 탐지할 것이며, 속도 차이로 인해 **영상 프레임 손실 없음**, **저지연** 등은 NanoOWL 쪽이 확실한 강점입니다. 예측하건대, OCID 기반 테스트에서 두 모델의 목표탐지 성공률 차이는 5\~10% 이내이고, 실제 그래스핑 성공률은 둘 다 비슷하게 높겠지만, NanoOWL 쪽이 **처리 시간 면에서 훨씬 짧아** 더 많은 시도를 할 수 있어 overall throughput은 높습니다.

**가설 4: 전반적으로 NanoOWL+NanoSAM 시스템은 목표 그래스핑에 **90% 이상 성공률**을 달성**할 것이다.
이는 단일 물체의 경우 거의 100%, 복잡한 경우 조금 떨어져 평균 90%대로 유지된다는 의미입니다. RL 기반은 쉬운 경우 90\~100%이나 어려운 경우 50% 이하로 떨어져 평균적으로 70% 정도가 될 수 있습니다. 이 차이는 특히 시나리오 C,D에서 두드러질 것으로 예상됩니다.

**가설 5: Edge 최적화 효과** – Jetson AGX Orin 상에서 NanoOWL+NanoSAM은 CPU 50% 미만, GPU 30% 미만 점유로 원활히 동작할 것이다. 한편 PC에서 테스트한 Grounding DINO+SAM은 Jetson에서 실시간 불가 판정. 즉 임베디드에선 Nano 조합만 현실적이라는 결론.

이러한 예상들을 검증하기 위해, 각 실험을 수행하고 결과를 표와 그래프로 정리할 것입니다. 특히 GSR, CIA, Latency 등은 시나리오별로 표를 제시하고, VLM vs RL을 비교하는 그래프(예: 성공률/시간 trade-off 곡선)를 그릴 계획입니다. 또한 로봇 동작 모습을 촬영한 사진이나, NanoSAM으로 분할된 영상 결과를 Figure로 첨부하여 정성적 이해를 돕겠습니다.

예상 결과의 예를 들자면:

* **그래스핑 성공률**: 시나리오 A,B에서는 VLM, RL 모두 95-100%로 높음. 시나리오 C에서는 VLM 90%, RL 60%. 시나리오 D에서는 VLM 80%, RL 50%. (정확한 수치는 실제 실험 필요)
* **명령 이해 정확도**: (해당 RL 정책에 언어가 없으므로 비교는 VLM만) VLM 95% 이상 대부분 정확.
* **평균 엔드투엔드 시간**: VLM 시스템 5초, RL 시스템 3초 (A기준). C나 D에서는 VLM 6초 (재시도 거의 없음), RL 10초 (실패 후 여러 번 시도).
* **세그멘테이션 IoU**: NanoSAM 마스크 vs GT: 0.85 IoU. (MobileSAMv2 vs GT: 0.80 IoU).
* **ROS2 자원 사용**: Jetson Orin NanoOWL Node CPU 20%, GPU 25%, RL Node CPU 10%, GPU 5% (추론은 CPU), etc.

이상의 평가를 통해, **사용자 연구 주제의 기술적 타당성**을 입증할 수 있습니다. 특히, 다른 접근법 대비 장점을 수치화하여 보여줌으로써, NanoOWL+NanoSAM 조합의 유효성을 뒷받침하게 됩니다.

## 6. 결론 및 향후 방향 (Conclusion and Future Work)

본 보고서에서는 **Jetson AGX Orin + ROS2 기반** **NanoOWL + NanoSAM 조합**을 활용한 **자연어 명령 기반 실시간 객체 인식 및 그래스핑 시스템**의 설계와 기술적 근거를 상세히 제시하였습니다. 주요 내용을 요약하면 다음과 같습니다:

* **NanoOWL **(**OWL-ViT 기반**)과 **NanoSAM **(**SAM 기반**)은 엣지 AI 디바이스에서 **제로샷 객체 탐지**와 **범용 세그멘테이션**을 각각 실시간으로 수행할 수 있는 최첨단 모델들입니다. Jetson Orin에서 두 모델은 각각 95FPS, 125FPS의 놀라운 추론 속도를 보이며, 결합 파이프라인으로 자연어 질의에 따라 목표 물체의 위치와 마스크를 즉각 획득할 수 있습니다. 이는 로봇의 **시각 인지 모듈**로서 매우 유망한 성능을 입증합니다.

* 본 시스템 구조는 **카메라 센싱** -> **VLM 인지**(**NanoOWL+NanoSAM**) -> **3D 좌표화** -> **로봇팔 계획/실행**의 단계로 구성되어, 언어에서부터 물체 파지까지 엔드투엔드 동작을 구현합니다. ROS2 프레임워크를 통해 각 기능을 모듈화하고 병렬 동작시킴으로써, 실시간 제어 요구를 만족하고 유지보수성을 높였습니다.

* **기술적 타당성** 측면에서, NanoOWL+NanoSAM 조합은 다른 대안들 (대형 VLM+SAM 조합, 강화학습 등) 대비 **Edge 환경 적합성**, **실시간성**, **개방형 인식 능력**에서 뛰어나다는 것을 다각도로 분석했습니다. 특히 Jetson Orin에서 고성능을 내면서도 메모리/전력 소모가 적고, ROS2 통합이 수월하여 **현실 세계에 적용 가능**한 솔루션임을 강조하였습니다.

* 이 시스템 아키텍처는 최신 연구 동향과 맥락을 같이 합니다. Vision-Language 모델과 로봇 제어의 결합은 현재 로봇학에서 가장 활발한 연구 분야 중 하나이며, Referring Grasping, CLIPort, Grounded-SAM 등 여러 선행 연구들과 교차점이 있습니다. 우리의 접근은 그러한 아이디어들을 **실제 임베디드 로봇 플랫폼**에 옮겨와 검증한다는 의의가 있습니다. 이는 학술적으로도 의미가 있을 뿐 아니라, 향후 제품화나 상용화 관점에서도 가치가 있습니다.

* 실험 계획을 통해 예상된 결과는, **NanoOWL+NanoSAM 기반 VLM 시스템이 90% 이상의 높은 파지 성공률과 신속한 응답**을 보이며, **RL 기반 시스템보다 목표지향 정확도에서 우월**함을 시사합니다. 또한 일반적인 테스트에서 **5초 내외**로 언어 명령 수행을 완료하여, 서비스 로봇으로서 실용적인 수준의 속도를 달성할 것으로 기대됩니다. 이로써 본 연구 주제의 **선택과 설계가 타당**함을 입증할 수 있을 것입니다.

**결론적으로**, Jetson AGX Orin 상에서 NanoOWL + NanoSAM을 활용한 자연어 기반 그래스핑 시스템은 **기술적 혁신성과 실현 가능성**을 모두 갖춘 접근이라고 평가할 수 있습니다. 특정 하드웨어에 한정되지 않고, 향후 더 발전된 엣지 컴퓨팅 모듈이나 다른 로봇 플랫폼에도 이식 가능하며, 소프트웨어 아키텍처도 확장성 있게 구성되어 있습니다. 이러한 시스템은 인간-로봇 상호작용을 한층 자연스럽게 만들고, 로봇의 작업 범위를 넓히는 데 기여할 것입니다.

**향후 연구 방향**으로는 몇 가지를 제안합니다:

* **그래스핑 지능 고도화:** 파지 실패 시 자동으로 다른 접근을 시도하거나, 잡은 후 물체가 미끄러지는 것을 감지해 재조정하는 **폐쇄 루프 제어**를 추가할 수 있습니다. 예컨대, 물체를 들었을 때 불안정하면 바로 내려놓고 다시 잡는 시도를 하는 등의 전략을 접목하면, 성공률이 더 높아지고 다양한 상황에 대응할 수 있습니다.

* **시스템 통합 및 복잡한 작업:** 이동 로봇과 결합하여 **픽앤플레이스(pick-and-place)** 이상의 시나리오 (운반, 조립 등)를 다루거나, **LLM 기반 플래너**를 넣어 복잡한 다중 단계 작업을 수행하는 것으로 발전시킬 수 있습니다. 이를 통해 가정 서비스 로봇처럼 여러 방을 돌아다니며 물건을 집어서 정리하는 응용으로 확장 가능합니다.

* **사용자 피드백 학습:** 사람의 자연어 명령과 그에 대한 로봇 동작 데이터를 축적하여, **오프라인 RL**이나 **IL**(**모방학습**)로 정책을 미세 조정함으로써, VLM+플래너 방식과 RL 방식의 장점을 융합할 수 있습니다. 예를 들어, 처음에는 VLM+MoveIt으로 동작하고, 점차 데이터가 쌓이면 특정 상황에서는 end-to-end 정책이 개입해 더 빠르게 처리하는 하이브리드 운영이 가능해질 것입니다.

* **다양한 비전 모델 접목:** 앞으로 더 많은 **멀티모달 기초 모델**들이 나올 것이므로, NanoOWL이나 NanoSAM도 지속 업그레이드될 것입니다. 예를 들어 Meta AI가 Segment Anything 다음 단계로 **SEEM (Segment Everything Everywhere All at Once)** 같은 통합 모델을 개발 중인데, 이런 것이 경량화된다면 단일 모델로 현재 둘의 역할을 대체할 수도 있습니다. 또한 3D 데이터까지 고려하는 **NeRF 기반 인지**나 **Point Cloud 기반 VLM**도 등장할 수 있어, 시스템 성능과 범용성을 향상시킬 기회가 많습니다.

* **실제 제품화 검증:** 마지막으로, 실제 현장 적용에 필요한 내구성, 안정성 시험도 중요합니다. 수백 시간 이상의 연속 동작 테스트, 다양한 사용자 발화에 대한 언어 처리 (화용적 차이), 안전장치 등 공학적 요소를 점검하여, 연구 프로토타입을 실제 응용에 투입할 수 있을 정도로 다듬는 과정이 필요합니다.

본 연구의 결과물은 **로보틱스에 있어서 사람과 기계의 상호작용을 자연스럽게 만드는 한 걸음**이 될 것입니다. 사람은 모르는 물체라도 가리키며 "이것 좀 집어줘"라고 말하면, 로봇은 즉각 시각적으로 이해하고 정확히 들어올릴 수 있는 시대가 가까워지고 있습니다. NanoOWL과 NanoSAM 같은 기술은 그 비전을 현실화하는데 핵심적인 역할을 하고 있으며, 본 보고서를 통해 이러한 기술 조합의 유효성과 탁월함을 뒷받침하는 근거를 충분히 제시하였습니다. 앞으로도 해당 분야의 발전을 주시하며, 본 시스템을 지속 개선해나감으로써 **더 똑똑하고 유연한 로봇 조작 시스템**을 구현해갈 것입니다.

참고문헌 및 인용한 자료들은 각주와 같이 표기하였으며, 최신 논문과 오픈소스 프로젝트를 망라하여 논의를 전개하였습니다. 특히 NVIDIA 및 학계의 관련 오픈 리소스를 적극 활용하여 실용적인 통찰을 얻었으며, 이를 통해 본 연구의 방향이 이론과 실제를 겸비하고 있음을 강조했습니다.

이상으로 보고서를 마치며, 본 연구 주제가 가진 가능성과 경쟁력을 다시 한 번 확신합니다. 앞으로의 구현과 실험에서 여기 담긴 분석이 그대로 실증되기를 기대합니다.
