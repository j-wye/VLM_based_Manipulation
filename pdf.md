좋습니다. Kinova Gen3 Lite 6DOF를 사용하는 실제 하드웨어 환경과, 시각 기반 객체 추적 조건을 만족하면서 end effector pose의 경로 계획이 반영된 보고서를 작성하겠습니다.

보고서는 ROS2 환경을 기반으로 하고, 수업의 핵심 이론인 DH 파라미터, Forward/Inverse Kinematics, Jacobian 해석, Trajectory Planning 등과 강하게 연결짓고, 자연어 명령 기반의 manipulation 제어 시스템에 초점을 맞추겠습니다.

보고서는 마치 수상을 목표로 하는 수준의 완성도와 전문성을 바탕으로, 교수님이 제시한 포맷(배경, 이론, 방법론, 결과, 고찰 등)에 맞춰 깊이 있게 작성하겠습니다. 완료되면 알려드리겠습니다.

# 자연어 명령 기반 Kinova Gen3 Lite 매니퓰레이터 시각-통합 제어 시스템

## 1. 연구 배경, 동기, 목표 및 문제의식

4차 산업혁명 시대에 로봇을 사람이 **자연어 명령**으로 직관적으로 제어하는 기술은 인간-로봇 상호작용 향상과 로봇 활용 범위 확대의 관점에서 큰 주목을 받고 있다. 특히 **서비스 로봇**이나 **협동 로봇** 분야에서는 사람이 말로 지시하면 로봇이 시각 센서를 통해 주변을 이해하고 원하는 작업(예: 특정 물체 집기)을 수행하는 **엔드투엔드 시스템**에 대한 수요가 높다. 본 연구의 동기는 이러한 맥락에서 출발하였다. **Edge 디바이스(NVIDIA Jetson AGX Orin)** 상에서 동작하는 통합 시스템을 구축하여, 사용자의 한국어/영어 등 자연어 명령을 입력으로 받아 로봇 매니퓰레이터가 **RGB-D 카메라**로 획득한 시각 정보와 결합해 명령에 해당하는 객체를 인식 및 추적하고, 최종적으로 해당 객체를 \*\*그리퍼(End Effector)\*\*로 파지하는 작업을 수행하고자 한다. 이를 통해 복잡한 인공지능 비전 알고리즘과 클래식 로봇공학 제어 기법을 결합한 실시간 로봇 제어의 가능성을 탐색하였다.

본 시스템에서 사용되는 매니퓰레이터는 **Kinova사의 Gen3 Lite 6-자유도 로봇 팔**이며, 실제 하드웨어로 실험을 진행한다. 이 로봇은 6개의 회전 관절과 2-핀치 그리퍼를 가지고 있어 인간 팔과 유사한 운동 범위를 제공하며, 최대 작업 반경 760 mm, 0.5 kg의 페이로드를 가진 소형 연구용 로봇이다. Gen3 Lite는 소프트웨어적으로 **ROS2** 기반의 통신 및 **Kortex API**를 지원하여 고수준 명령(예: **엔드 이펙터의 위치/속도 제어**)부터 저수준 명령(예: **각 관절 모터의 토크/속도 제어**)까지 다양하게 제어할 수 있다. 이러한 유연성을 활용하면, 자연어로 정의된 목표 동작을 고수준에서 계획하고, 로우레벨에서는 안정적인 모터 제어로 실행하는 **하이브리드 제어 구조**를 구성할 수 있다.

**문제의식:** 자연어 인식 및 시각 인지는 주로 딥러닝을 활용하며 본 연구의 선행 단계에서 해결되었다. 그러나, 인공지능이 산출한 \*\*목표 물체의 6-자유도 자세(pose)\*\*를 실제 로봇이 정확하고 안전하게 구현하기 위해서는 로봇공학의 기본기에 입각한 체계적인 제어 전략이 필수적이다. 만약 이러한 제어 이슈를 간과하면, 잘 탐지된 물체라도 로봇 팔이 도달하지 못하거나(**기구학적 한계**), 도달하더라도 경로 상에서 **특이점**에 근접하여 모터에 무리한 속도를 요구하거나, 혹은 잘못된 자세로 접근하여 **시야를 상실**하거나 충돌을 일으킬 수 있다. 따라서 본 연구의 핵심 목표는 **로봇매니퓰레이터 수업에서 다룬 이론들**(정역학적 모델링, 자코비안 행렬, 경로 계획, 제어 등)을 실제 시스템에 적용하여, ▲ **주어진 End Effector의 목표 자세**를 효율적으로 역산하고(역기구학), ▲ **실시간으로 안전한 경로**를 계획하며(특이점/충돌 회피, Joint/Cartesian 경로 계획), ▲ **제어기**를 통해 정밀하게 추종시키는 것이다. 특히 \*\*엔드 이펙터가 물체를 계속 시야에 둔 채(end effector always “looking at” the object)\*\*로 접근하도록 **적응형 경로 계획**을 수행하는 것이 본 연구의 차별점이며, 이는 고정된 경로를 추종하는 기존 방식과 달리 **실시간 피드백 제어**의 중요성을 부각시킨다.

정리하면, 본 연구는 *“자연어로 지시하면, 로봇이 카메라로 주변을 보고 이해하여 물체를 찾아가 집는”* 완전한 시스템을 구현하는 것이다. 이때 **로봇매니퓰레이터의 관절 운동학 및 제어 이론**을 토대로 한 엄밀한 접근을 통해, 최종 동작의 **정확도와 안정성**을 높이는 것이 우리의 궁극적 목표이다.

## 2. 로봇매니퓰레이터 수업 관련 이론 소개 및 연결

이 절에서는 본 시스템 구현에 필요한 주요 **로봇 매니퓰레이터 이론**들을 정리하고, 각 개념이 어떻게 활용되었는지 연결 지어 설명한다. 다루는 내용은 **정방향/역방향 기구학 모델링(DH 매개변수)**, **자코비안 행렬과 운동 해석**, **궤적 계획(조인트 공간 vs 데카르트 공간)**, 그리고 \*\*제어 이론(좌표계 변환, PID 및 임피던스 개념)\*\*이다. 이러한 이론들은 수업 교재에서 개별적으로 학습된 것이나, 본 연구에서는 이들이 하나의 시스템 내에서 **유기적으로 결합**됨을 강조한다.

### 2.1 정방향 기구학(Forward Kinematics)과 DH 파라미터 모델링

**정방향 기구학**은 로봇의 각 관절값 $(\theta_1, \theta_2, \dots, \theta_n)$이 주어졌을 때, 로봇 \*\*말단 좌표계(End Effector)\*\*의 위치와 자세 $(\mathbf{p}, \mathbf{R})$를 계산하는 방법이다. 이는 로봇의 **기구학적 구조**를 수학적으로 모델링함으로써 가능하며, 대표적으로 **Denavit-Hartenberg(DH) 파라미터** 표기법이 사용된다. DH 파라미터는 각 관절 간의 상대적인 **축 간 거리(**링크 길이 $a_i$**, 옵셋 $d_i$**)\*\*와 **축 간 각도(**링크 비틀림각 $\alpha_i$**, 조인트 각 $\theta_i$**)\*\*를 체계적으로 기록하여, 복잡한 다관절 구조를 일정한 포맷으로 표현해준다.

Kinova Gen3 Lite 6-DOF 로봇 팔에도 DH 표기법을 적용할 수 있다. 우선 베이스(base)로부터 순차적으로 **조인트 1**부터 **조인트 6**까지 좌표계를 할당하고, 각 연결부의 DH 파라미터를 추출하면 표 1과 같은 구조를 얻을 수 있다 (실제 수치는 Kinova 제공 메뉴얼 및 CAD 모델로부터 확인 가능).

* $\alpha_i$: 링크 $i$의 **비틀림 각도**로, 조인트 $i$의 회전축(z_{i})와 조인트 $i+1$의 회전축(z_{i+1}) 사이의 각도이다. Gen3 Lite의 경우, 다수의 인접 축이 직교 또는 평행하게 배치되어 $\alpha$ 값이 $90^\circ$ (또는 $0^\circ, 180^\circ$)인 경우가 많다. 예컨대 $\alpha_1 = 90^\circ$는 첫 관절축과 두번째 관절축이 직교함을 의미한다.
* $a_i$: 링크 길이로, 조인트 $i$와 $i+1$의 축 교차 직선 사이의 **수평 거리**이다. Kinova Gen3 Lite에서는 $a_2$ 값이 유의미하며, 이는 어깨 관절과 팔꿈치 관절 사이의 수평 상쇄를 나타낸다.
* $d_i$ (또는 $b_i$로 표기되기도 함): **링크 옵셋**으로, 조인트 $i$의 축과 조인트 $i+1$의 축 사이의 **공간적인 이격거리**(평행 이동)를 뜻한다. 예컨대 $d_1$은 베이스에서 첫 링크까지의 높이, $d_2, d_3$ 등은 상박과 전완 링크의 길이에 해당한다. 특히 Kinova Gen3 Lite의 경우 **손목(wrist)** 부분에 해당하는 $d_5$가 0이 아니어서 (즉 손목 축들이 한 점에 모이지 않고 오프셋이 있음) **비-구형(hand offset) 손목** 구조를 갖는다. 이 때문에 흔한 6축 로봇의 "세 개의 손목축이 한 점에서 교차하는 구형(wrist-partitioned) 구조"가 아니며, **역기구학** 풀이 시 자세와 위치를 분리(decouple)하는 고전적 방법을 직접 적용하기 어렵다는 특성이 있다.
* $\theta_i$: **관절 가변각**으로, 조인트의 현재 회전 각도를 의미한다. 이는 로봇이 움직임에 따라 실시간으로 변하는 **변수**이며, 정방향 해석 시에는 입력으로 주어진다 (반대로 역기구학에서는 목표 $\theta_i$를 미지수로 푸는 값이 된다).

위 파라미터들을 사용하면 각 링크간 **변환 행렬** $A_i$를 구성할 수 있다. 예를 들면, DH 파라미터에 따른 링크 변환은 일반적으로 다음과 같다:

$$
A_i = RotZ(\theta_i) \times TransZ(d_i) \times TransX(a_i) \times RotX(\alpha_i) 
\tag{1}
$$

이를 행렬 형태로 전개하면, $RotZ(\theta_i)$와 $RotX(\alpha_i)$는 회전 행렬을, $TransZ(d_i)$와 $TransX(a_i)$는 이동 벡터를 포함하는 4x4 **균질변환행렬**로 나타난다. 최종적으로 **베이스 좌표계**로부터 **엔드이펙터 좌표계**까지의 변환 $T_{0,6}$은 모든 링크 변환을 순차적으로 곱한 행렬로 얻어진다:

$$
T_{0,6} = A_1 A_2 \cdots A_6 
$$

이때 $T_{0,6}$은 4x4 행렬로, 상단 좌측 3x3 부분은 엔드 이펙터의 **회전행렬 $\mathbf{R}_{0}^{6}$** (자세), 우측 상단 3x1 부분은 **위치 벡터 $\mathbf{p}_{0}^{6}$** (베이스 좌표계에서 본 엔드이펙터 위치)을 나타낸다. 정방향 기구학 식을 통해 우리는 주어진 관절 상태에서 그리퍼의 **절대 위치/자세**를 계산할 수 있으며, 이는 ROS의 tf 체계에서 실시간으로 **퍼블리시**되는 정보와도 연계되어 시스템 동작 중 모니터링 및 검증에 활용된다.

본 연구에서 정방향 기구학 모델링은 **시뮬레이션 및 오프라인 분석 단계**에서 역할을 한다. 예컨대 로봇의 각 관절범위 내에서 reachable workspace를 예측하거나, 특정 목표 자세가 **실제로 로봇으로 구현 가능한지**(정기구학의 역문제) 검토하는 데 사용하였다. 또한 **End Effector가 항상 객체를 향하도록** 움직이게 하려면, 실시간으로 현재 엔드이펙터의 orientation이 어떻게 변화하는지 알아야 하는데, 이를 정방향 모델과 \*\*자세의 파라미터화(Euler 각 또는 Quaternion)\*\*를 통해 추적할 수 있다. 요컨대, 정방향 기구학 모델은 로봇을 **디지털 트윈**처럼 수식으로 표현함으로써, 고차원 AI 시스템을 물리 세계의 로봇에 접목시키는 **foundation** 역할을 하였다.

### 2.2 역방향 기구학(Inverse Kinematics)과 다중해 문제

**역방향 기구학**은 원하는 End Effector의 목표 자세(pose)가 주어졌을 때, 이를 달성하기 위한 각 **관절각** ${\theta_1, \theta_2, ..., \theta_6}$을 계산하는 과정이다. 이는 로봇 제어의 핵심 난제로, 일반적으로 정방향 해석보다 훨씬 복잡하다. **6-자유도 관절형 로봇**의 경우, 폐쇄형 해(solution)를 얻기 위한 방정식이 고차(polynomial)로 나타나거나 삼각함수 연립방정식으로 귀결되며, 해가 **0개, 1개, 다수** 존재할 수 있다.

Kinova Gen3 Lite 로봇은 앞서 언급한대로 손목부에 오프셋이 있어 **구형(hand) 손목**이 아니므로, **위치**와 **자세**를 독립적으로 풀 수 있는 고전적 해법이 성립하지 않는다. 대신 **전체 6차 방정식**을 풀어야 하는데, 이를 위해 연구자들은 특정 축을 제거하여 하나의 **단변수 다항식**으로 축약하는 방법을 쓴다. 이러한 **해석적(Analytical) 역기구학** 해법은 매우 복잡하지만 해가 존재한다면 **직접 수치**로 구할 수 있다는 장점이 있다. 실제로 참고 문헌에 Gen3 Lite의 역기구학 해를 16차 다항식으로 도출하고 해를 분류한 연구가 있으며, 최종적으로 모든 실해(real solution) 조합을 계산한 뒤 그 중 작업에 최적인 자세를 선택하는 방법이 제안되었다고 한다. 다만, 해석적 방식은 로봇의 제조사나 연구자가 미리 도출하여 제공하지 않는 한 직접 구현하기에는 난이도가 높다.

따라서 산업현장이나 연구에서 흔히 쓰이는 방법은 **수치적(Numerical) 역기구학**이다. 이는 목표 엔드 이펙터의 좌표 $(\mathbf{p}*{\text{des}}, \mathbf{R}*{\text{des}})$와 현재 로봇의 관절각을 입력으로 하여, 반복적인 계산으로 $\theta_i$들을 보정해 나가는 알고리즘이다. 예를 들면, **Jacobian 역행렬 방법**이나 **gradient descent** 등이 해당되며, 초기 관절값으로부터 오차가 줄어드는 방향으로 조금씩 관절 각을 업데이트하여 목표 자세에 수렴시킨다. 수렴 속도를 높이기 위해 Jacobian 행렬과 pseudo-inverse를 사용하면 **미소 변화량** 수준에서 **목표 위치/자세 오차**를 관절 변화로 환산할 수 있어 효율적이다. 수치적 IK의 장점은 임의의 로봇 구조에도 적용 가능하고 구현이 비교적 간단하며, 다른 제약(충돌 회피 등)과 결합한 **최적화 문제**로 확장하기 쉽다는 것이다. 다만 단점은 해가 여러 개일 경우 특정 해로 수렴할지 장담하기 어렵고, **특이점 부근**에서는 Jacobian이 신뢰할 수 없는 결과를 내거나 발산할 수 있다는 점이다.

본 연구에서는 Kinova Gen3 Lite의 제어를 위해 **역기구학을 실시간으로 계산**하는 모듈이 필요하다. Kinova社에서 제공하는 **Kortex API**에는 **고수준 명령**으로 **“목표 포즈로 이동”** 기능이 존재하지만, 이는 내부적으로 자체 IK를 사용하고 있어 **엔드 이펙터의 경로**를 사용자가 세밀히 통제하기 어렵다. 대신 우리는 ROS2 환경에서 MoveIt 등의 모션 플래너를 활용하거나, 직접 IK를 풀어서 **Joint Trajectory**를 생성하는 접근을 고려하였다. MoveIt의 **InverseKinematicsSolver** 플러그인은 수치해 기반으로 다관절 로봇의 IK를 풀이해주며, 필요시 해 여러 개 중 **가장 현재 자세에 가까운 해**를 선택하거나 **특정 관절 우선도**를 줄 수도 있다. 이를 통해 **자연스러운 동작**을 보장하였다. 예를 들어, 동일한 목표 End Effector 자세에 대해 \*\*팔꿈치를 위로 향한 자세(elbow-up)\*\*와 **아래로 향한 자세(elbow-down)** 두 가지 IK 해가 존재할 경우, 일반적으로 현재 로봇 자세와 유클리드 거리상 가까운 해를 선택함으로써 불필요한 큰 움직임을 피한다. 또한 경우에 따라 **물체 시야 유지**나 **시스템 제한**을 고려해 해를 선택할 수 있다. (참고로, 카메라가 로봇 팔에 장착되지 않고 상부에 고정된 경우, **팔꿈치-아웃 자세**는 물체를 가리는 occlusion을 유발할 수 있어 회피해야 한다는 연구도 있다). 본 시스템에서는 카메라가 엔드 이펙터에 부착되어 있으므로 팔 자체의 occlusion은 없지만, **엔드 이펙터의 orientation 제약**(항상 물체를 향하도록)을 만족하려면 특정 IK 해만 유효할 수 있다. 이런 제약은 IK 계산 전에 **목표자세 자체에 반영**되므로(3.장 방법론 참조), IK 단계에서는 일반적인 수치해로 충분하였다.

마지막으로, 역기구학 해가 산출되면 **여러 관절각 조합**으로 표현된다. 이를 **Joint State**로 패킹하여 \*\*하위 제어기(low-level controller)\*\*에 전달하면 로봇이 해당 각도로 움직이게 된다. 이 전체 파이프라인은 **ROS2 액션**으로 비동기 실행되어, IK 계산 → 경로생성 → 실행 명령 전송이 하나의 **atomic한 고수준 명령**으로 처리된다. 정리하면, 역기구학 이론은 \*“AI가 결정한 End Effector의 목표, 즉 ${x,y,z,\text{orientation}}$을 실제 로봇 관절 명령 ${\theta_i}$로 변환”\*하는 역할을 하며, 이는 곧 **자연어 인지 결과를 물리 세계 행동으로 옮기는 핵심 다리**라고 할 수 있다.

### 2.3 자코비안(Jacobian) 행렬과 운동 해석

**Jacobian 행렬**은 다관절 로봇의 **관절 속도** 벡터 $\dot{\mathbf{q}} = [\dot{\theta}_1, \dot{\theta}_2, ..., \dot{\theta}_n]^T$와 **End Effector의 속도**(선속도 $\mathbf{v}$ 및 각속도 $\boldsymbol{\omega}$)를 연결해주는 **선형 변환 행렬**이다. 일반적으로 $n$ 자유도를 가진 매니퓰레이터의 자코비안 $\mathbf{J}(\mathbf{q})$은 $6 \times n$ 크기의 행렬로 정의되며, 다음 관계식을 만족한다:

$$
\begin{bmatrix} \mathbf{v} \\ \boldsymbol{\omega} \end{bmatrix}
= \mathbf{J}(\mathbf{q}) \, \dot{\mathbf{q}}. 
\tag{2}
$$

여기서 $\mathbf{v} = [\dot{x}, \dot{y}, \dot{z}]^T$는 End Effector의 선속도, $\boldsymbol{\omega}=[\omega_x,\omega_y,\omega_z]^T$는 End Effector의 각속도(보통 **롤-피치-요** 축에 대한 각속도)이다. Jacobian의 상단 $3\times n$ 부분은 각 조인트 속도가 **End Effector 위치 변화**에 기여하는 성분(선속도 관계), 하단 $3\times n$ 부분은 **End Effector 자세 변화**에 기여하는 성분(각속도 관계)을 나타낸다. 자코비안 행렬의 i번째 열은 \*“조인트 $i$가 단위 속도로 움직일 때 End Effector에 유발되는 미소 변위(선형+회전)”\*를 나타내며, 이를 구하기 위해 보통 **DH 좌표계**에서 각 조인트의 회전축 방향 $\mathbf{z}*{i-1}$과 위치 $\mathbf{p}*{i-1}$을 이용한다. 구체적으로, 조인트 $i$가 \*\*회전관절(Revolute)\*\*이면:

* 선속도 성분 $\mathbf{J}*{v,i} = \mathbf{z}*{i-1} \times (\mathbf{p}*{\text{end}} - \mathbf{p}*{i-1})$  (회전축의 방향벡터와 End Effector까지의 위치벡터 간의 **벡터곱**)
* 각속도 성분 $\mathbf{J}*{\omega,i} = \mathbf{z}*{i-1}$  (회전축 방향이 곧 그 조인트가 End Effector에 만들어내는 각속도 방향)

인 반면, **프리스매틱(prismatic) 관절**이라면 선속도 성분은 $\mathbf{z}_{i-1}$ (슬라이드 방향)이고 각속도 성분은 0가 된다. Kinova Gen3 Lite는 모든 관절이 회전형이므로, 위 회전 조인트 공식을 각 i에 적용하여 $6\times 6$ Jacobian을 구할 수 있다. 계산 결과 행렬식 $\det(\mathbf{J})$이 0이 되는 **특이점(singularity)** 조인트 구성이 존재할 수 있는데, 이 경우 로봇의 일부분 자유도가 상실되어 End Effector가 특정 방향으로는 속도를 낼 수 없게 된다. 예를 들어 **손목 특이점**은 관절 4,6의 축이 평행 또는 일치(coincident)할 때 발생하며, 이때 로봇은 End Effector의 회전을 보상하기 위해 관절 4와 6을 무한대 속도로 반대 방향 회전하려고 시도하지만 물리적으로 불가능하여 이상 현상을 보인다. 특이점에 가까워지면 \*\*자코비안 행렬의 조건수(condition number)\*\*가 나빠져, 미세한 속도 명령이 과도한 관절속도로 요구되거나 제어가 불안정해질 수 있다. 따라서 **경로 계획** 시 특이점 근처를 회피하는 것이 중요하며 (자코비안의 rang 감소를 피함), 불가피할 경우 **감속**이나 **다른 자세로의 reconfiguration** 등의 대책을 사용한다.

본 연구에서는 Jacobian 이론이 두 가지 측면에서 활용되었다. 첫째, **역기구학의 수치해**에서 언급했듯, 목표 자세와 현재 자세 간 오차를 줄이기 위해 Jacobian의 \*\*유사역행렬(pseudo-inverse)\*\*을 이용하여 각 관절 수정량 $\Delta \mathbf{q}$를 산출하였다. 이는 작은 시간 간격에 대해 식 (2)를 미분 형태로 전환하여 $\dot{\mathbf{q}} = \mathbf{J}^+ (\mathbf{v}, \boldsymbol{\omega})$ (여기서 $\mathbf{J}^+$는 Moore-Penrose 역행렬)로 표현하는 방식이다. 특히 End Effector의 위치 오차 $\Delta \mathbf{p}$와 자세 오차(예: quaternion 오차를 회전벡터로 변환한 값)를 속도로 간주하여, $\dot{\mathbf{q}} = \mathbf{J}^+ [\Delta \mathbf{p}; \Delta \boldsymbol{\phi}]$로 계산하고, 이를 적분(또는 작은 시간 스텝 적용)하여 관절각을 수정하면 IK 수렴을 구현할 수 있다. 둘째, \*\*실시간 제어(visual servoing)\*\*에서 Jacobian이 활용되었다. 엔드 이펙터에 부착된 카메라 시야 내 **물체의 픽셀 좌표 변화**는 통상 \*\*이미지 자코비안(Image Jacobian)\*\*을 통해 카메라의 선속도/각속도로 변환될 수 있지만, 본 연구에서는 단순히 **3D 공간**에서의 물체-카메라 벡터를 이용하였다. 예를 들어 물체가 약간 이동하거나 End Effector가 경로를 따라가며 **물체를 계속 향하도록 회전**하려면, End Effector의 **yaw, pitch** 등을 조절할 필요가 있다. 이때 현재 **물체의 방향**과 **카메라(엔드 이펙터)의 시선 방향** 간 각도 차이를 계산하여, 이를 줄이기 위한 End Effector의 각속도 $\boldsymbol{\omega}*{des}$를 결정한다. 결정된 $\boldsymbol{\omega}*{des}$를 각 관절 속도로 변환하기 위해 Jacobian의 하단 3x6 (각속도 부분) 행렬을 사용하였다. 간단히 말해, **엔드 이펙터의 원하는 각속도**와 **각 관절 속도** 사이의 선형관계인 $\boldsymbol{\omega} = J_\omega(\mathbf{q}) \dot{\mathbf{q}}$를 뒤집어, 필요한 관절 속도를 구한 것이다. 이러한 방식으로 로봇은 **물체를 시야 중심에 유지**하도록 미세하게 자세를 조정하며 이동할 수 있었다. 만약 Jacobian을 사용하지 않고 각 관절에 일정한 비율로 각속도를 줬다면, 특정 구간에서 물체를 놓칠 수도 있지만, Jacobian 기반 제어는 **매니퓰레이터의 구조를 고려**하여 필요한 만큼씩 조절해주므로 더욱 효율적이다.

요약하면, Jacobian 행렬은 본 시스템의 **운동학적 이해**와 **제어 계산의 편의성**을 모두 향상시킨 핵심 도구이다. 수업에서 배운 Jacobian 유도법과 그 물리적 의미를 이해함으로써, 우리는 로봇 움직임을 **미분적 관점**에서 해석하고 제어에 응용할 수 있었다. 이는 동적인 환경에서 로봇을 안정적으로 제어하는 근간이 되며, 특히 **실시간 비전 피드백**을 적용하는 본 연구에서 빛을 발했다.

### 2.4 궤적 계획(Trajectory Planning): 조인트 공간 vs 데카르트 공간

**Trajectory (궤적) planning**이란 로봇이 초기 자세에서 목표 자세로 이동할 때 **시간에 따른 움직임 경로**를 결정하는 것이다. 이는 단순히 최종 점을 향해 직선으로 움직이는 문제가 아니라, **어떤 경로**를 **어떤 속도 프로파일**로 따라갈지까지 포함하는 포괄적인 개념이다. 본 연구에서 궤적 계획은 크게 **조인트 공간 경로**와 **데카르트(Cartesian) 공간 경로** 두 가지 관점에서 고려되었다.

* **조인트 공간에서의 궤적 계획:**
  조인트 공간 경로란 말 그대로 각 관절각 $\theta_i(t)$들의 시간 프로파일을 개별적으로 생성하는 방법이다. 수업 교재에서는 **폴리노미얼(다항) 보간**을 통해 부드러운 조인트 프로파일을 만드는 방법을 다루었는데, 대표적으로 **3차 다항식(Cubic) 보간**이 있다. 이는 초기 시각 $t_0$에서 최종 시각 $t_f$까지 각 조인트각이 $\theta_0$에서 $\theta_f$로 변할 때 속도 및 가속도의 연속성을 보장하는 궤적을 산출한다. 3차 다항식 궤적의 일반형은 다음과 같다:

  $$
  \theta(t) = a_0 + a_1 (t-t_0) + a_2 (t-t_0)^2 + a_3 (t-t_0)^3,
  $$

  여기서 계수 $a_0, a_1, a_2, a_3$는 경계 조건(처음/끝 각도 및 처음/끝 속도 조건)을 대입하여 결정된다. 이러한 방식으로 각 관절마다 속도/가속도 **연속**이고 **완만한** 프로파일을 생성하면, 전 관절을 동시에 구동시켰을 때 로봇은 자연스럽게 목표자세로 도달한다. Joint-space 계획의 장점은 **계산이 빠르고 구현이 간단**하며, 각 관절의 \*\*물리적 한계(속도/가속도 한계)\*\*를 직접 고려하기 용이하다는 점이다. 예를 들어 관절별 최대 속도를 넘지 않도록 다항식을 구성하거나, 중간에 **관절 중간 점**을 넣어 특정 간섭을 피하는 것도 가능하다. 따라서 산업용 로봇 팔은 흔히 Joint-space에서 **속도 프로파일이 제한된 (S-curve 또는 trapezoidal velocity)** 궤적 계획을 사용하여 제어기 명령을 생성한다.

* **데카르트(Cartesian) 공간에서의 궤적 계획:**
  Cartesian 공간 경로란 End Effector의 **작업공간 경로**를 직접 계획하는 방법이다. 이는 작업 공간에서 직선, 곡선 등의 경로를 설계하고, 그 경로를 따라 End Effector가 움직이도록 **실시간 IK**를 적용하거나 제어기를 동원하는 방식이다. 예를 들어, 물체를 집기 위해 End Effector를 **직선으로 접근**시키고 싶다면, 물체 바로 위 몇 cm 지점부터 최종 grasp 지점까지 Z축 방향으로 직진하는 path를 정의할 수 있다. Cartesian 경로의 이점은 로봇의 움직임이 **공간적으로 예측 가능**하고, 장애물이나 주변 환경과의 관계를 고려하기 쉽다는 점이다. 특히 카메라가 End Effector에 있는 경우, **직선 접근**은 시야를 유지하면서 목표에 다가가기 유리하다. 또한 orientation에 대한 구속(예: **항상 물체를 향하기**)을 Cartesian 경로점마다 지정할 수 있어, Joint-space에서 개별적으로 보장하기 어려운 **동시 제약**을 충족시킬 수 있다. 하지만 단점은 IK를 매 시점 계산해야 하므로 연산 부하가 높고, Joint-space에서의 로봇 내부 움직임을 간접적으로 제어하므로 예기치 않은 **joint-level 급가속**이나 singularity 돌파를 겪을 수 있다는 점이다. 예를 들어, End Effector에겐 부드러운 직선 경로여도, 각 관절들은 비선형적으로 움직여 중간에 한 관절이 급격히 방향을 바꾸는 현상이 생길 수 있다. 따라서 Cartesian 경로를 사용할 때는 **세분화된 waypoint**를 지정하고, 각 구간마다 joint 속도를 제한하거나, MoveIt과 같은 모션 플래너의 \*\*시간 매개변수화(time parameterization)\*\*를 거쳐 속도/가속도를 안전하게 분배할 필요가 있다.

본 연구에서는 **두 접근법을 절충**하여 사용하였다. 우선, **최종 grasping 목표 자세**까지 가는 대략의 움직임은 MoveIt을 이용한 **Joint-space 계획**으로 실행하였다. MoveIt은 로봇 모델과 IK solver를 기반으로, 시작 자세에서 목표 자세까지 **충돌 없이** 도달하는 joint 값의 시퀀스를 생성해준다. 이때 목표 자세는 Grasping 모듈이 산출한 End Effector pose로 설정하되, **orientation**은 초기에 물체를 향하도록 이미 설정되어 있다 (예: 카메라 optical axis가 물체 중심을 가리키는 방향으로 quaternion 지정). MoveIt은 orientation도 고려하여 IK 해를 찾으므로, 처음과 끝 자세 모두 물체를 향한 상태로 보장된다. 다만 MoveIt이 생성한 중간 경로에서는 orientation 제약이 완화될 수 있어, **엔드이펙터가 중간에 한때 다른 방향을 볼 가능성**이 있다. 이를 보완하기 위해, 우리는 최종 접근 단계(물체와 몇십 cm 이내)에 들어서면 MoveIt 대신 **Custom Cartesian Controller**를 동작시켰다. 이 컨트롤러는 End Effector 기준으로 **상대적인 직교 좌표계 동작**을 수행하는 것으로, 카메라 영상에서 물체의 bounding box가 중심에 오도록 미세 조정하며 직선 경로로 접근한다. 구체적으로, ROS2의 `tf`를 사용해 **물체의 현재 위치**를 지속 추적하면서, End Effector의 목표점을 실시간 업데이트한다. 예를 들어 물체가 약간 이동하거나(혹은 추적 오차로 위치 업데이트)하면 그에 따라 새로운 목표 위치를 설정하고 **잔여 경로**를 수정한다. 이러한 **Adaptive Planning**은 사실상 **Visual Servoing 제어**에 가깝지만, 상위 계층에서는 이를 일련의 짧은 Cartesian 경로 재계획으로 인지한다. 결과적으로, 초기 멀리서는 Joint-space 큰 움직임으로 **전역적인 접근**을 하고, 막판에는 Cartesian 미세조정으로 **정밀 접근 및 자세 유지**를 하는 **하이브리드 궤적 전략**을 구현하였다. 이 과정에서 수업시간에 배운 궤적 계획 이론(폴리노미얼 보간 등)은 joint-space 프로파일 생성에 응용되었고, Cartesian 제어 시에는 **PID 제어**를 접목해 목표 대비 오차를 점진적으로 없애는 방식으로 안정성을 높였다.

또한 **경로상의 속도 제어**에도 이론 적용이 있었다. 3차 다항식 경로의 미분 형태로 얻어지는 속도 $\dot{\theta}(t)$와 가속도 $\ddot{\theta}(t)$를 참고하여, 로봇이 목표지점에 가까워질수록 천천히 정지하도록 **감속 구간**을 설정하였다. 이는 **trapezoidal velocity profile** 개념을 적용한 것으로, 먼저 최대 속도로 가속했다가 목표 이전에 대칭적으로 감속하는 형태이다. 이러한 프로파일은 **진동 감소**와 **정밀 정지**에 유리하며, Kinova Gen3 Lite의 모터 제어기에 무리한 명령이 가지 않도록 해준다. Cartesian 제어 단계에서도, 물체를 계속 카메라 중앙에 유지하려는 회전 동작에 *임계 감속*을 주어, 영상 피드백 오차가 거의 0이 되면 **각속도 명령을 0에 수렴**시켰다. 결과적으로 End Effector는 정확한 자세로 물체를 응시한 채 적절한 속도로 접근하여 파지 임무를 수행할 수 있었다.

### 2.5 ROS2 tf, PID 제어, 임피던스 개념 등 구현과 이론의 연계

**ROS2 tf 프레임워크:** 로봇 시스템에서 다양한 좌표계를 관리하는 ROS의 **tf(transform) 패키지**는, 수업 시간에 배운 **좌표계 변환** 이론이 실시간 소프트웨어에 구현된 대표적 예이다. 우리 시스템에서는 **베이스 좌표계**, **각 관절 좌표**, **엔드이펙터(그리퍼) 좌표**, **카메라(optical) 좌표**, 그리고 **물체 좌표** 등이 존재한다. 카메라로부터 인식된 물체의 포즈는 처음엔 **카메라 좌표계** (또는 RGB-D 센서의 좌표 체계)로 얻어진다. 이를 로봇 베이스 좌표로 변환해야 로봇이 알아들을 수 있는 목표가 된다. 우리는 미리 **정밀한 Hand-Eye 캘리브레이션**을 통해 **엔드이펙터->카메라 변환**을 구했고, 로봇의 **베이스->엔드이펙터 변환**은 실시간 정기구학 계산으로 알 수 있다. tf 트리 상에서 이를 조합하면:

$T^{\text{base}}_{\text{obj}} \\ T^{\text{base}}_{\text{EE}} \\ T^{\text{EE}}_{\text{cam}} \\ T^{\text{cam}}_{\text{obj}}$

와 같이 목표 물체의 베이스 좌표 위치를 산출할 수 있다. 실제 구현에서는 `tf_buffer.lookup_transform("base_link", "object_frame")`와 같이 호출하여 손쉽게 얻는다. 이 때 $T^{\text{cam}}_{\text{obj}}$는 비전 모듈이 퍼블리시하고, 나머지 변환들은 로봇 상태나 고정값으로 퍼블리시된다. 이러한 **좌표계 변환 파이프라인**은 수업의 **동차변환 행렬 개념**과 정확히 일치하며, 이를 활용해 로봇과 센서 정보를 통합하였다. 아울러 `tf`는 과거 시점 변환까지 관리하므로, 예컨대 **카메라 딜레이**나 **예측 제어**를 할 때 과거 좌표를 불러와 보정할 수도 있다.

**PID 제어:** 제어공학의 기본인 **PID(Proportional-Integral-Derivative)** 제어기는 로봇 각 관절의 하위 레벨 제어에 널리 쓰인다. Kinova Gen3 Lite 내부에도 각 모터별로 **PID 제어 루프**가 구동되어 있어, 상위에서 목표 각도를 주면 자체적으로 전류를 제어해 해당 각도로 근접하게 만든다. 본 연구에서는 저수준 PID 튜닝보다는 **상위 제어기**로서 PID를 적용한 몇 가지 사례가 있다. 첫째, **엔드이펙터의 orientation** 제어에 PID를 이용하였다. 앞서 언급한 시각 피드백에 의한 orientation 수정에서, 단순히 즉각적인 Jacobian 역산으로 각속도를 결정하면 약간의 오차 진동이 남을 수 있다. 이를 해결하기 위해 **P 게인**을 곱해 부족한 만큼만 회전하도록 했고, **D 항**을 추가하여 급격한 변화 시 감속하도록 하였다. (I 항은 orientation 제어에서는 많이 쓰지 않았는데, 장기 편차가 문제가 될 경우에 대비해 고려는 가능하다.) 이로써 **시각 서보** 과정의 안정성을 높였다. 둘째, **경로 추종 중 속도 제어**에도 PID 개념을 반영하였다. MoveIt이 생성한 Joint trajectory는 *각 시점별 목표 각도 리스트*로 표현되는데, 실제 로봇이 이를 정확히 따라가게 하려면 각 관절마다 **피드백 제어**가 필요하다. Kinova의 펌웨어 PID가 이 역할을 수행하지만, 상위에서 **joint_state 피드백**을 받아 목표 대비 오차를 모니터링하며 필요시 속도를 조정하는 **보조 P 제어**를 구현하였다. 결과적으로, 만일 관절 하나가 추종을 약간 따라잡지 못하면 나머지 관절도 속도를 늦추어 **궤적 전체를 완주하도록** 조정하였다.

**강성(Stiffness) 및 감쇠(Damping) 기반 제어:** 이는 수업에서 다룬 **임피던스 제어** 혹은 **컴플라이언스 제어**의 개념과 연관된다. 물리적으로 PID 제어를 보면, P 게인은 목표와 현재 위치 사이에 **가상의 스프링**(탄성) 역할을 하고, D 게인은 움직임에 **가상의 댐퍼**(마찰) 역할을 한다고 해석할 수 있다. 따라서 어떤 관절이나 End Effector에 대해 $K_P$와 $K_D$를 설정하는 것은, 해당 축의 **강성**(stiffness)과 **감쇠비**(damping ratio)를 결정하는 것과 유사하다. 본 연구에서 직접적인 임피던스 제어를 구현하진 않았으나, 이 개념은 **로봇 동작 안전성**을 고찰하는 데 중요하다. 예를 들어, End Effector가 물체를 잡을 때 **너무 강직하게(High stiffness)** 위치 제어만 하면, 물체와 처음 접촉하는 순간 큰 충격이나 미끄러짐이 발생할 수 있다. 대신 약간의 **컴플라이언스**를 주어, 접촉 시 살짝 물러나도록 하면 충격을 흡수하고 안정적으로 파지할 수 있다. Kinova Gen3 Lite는 비교적 가벼운 로봇이지만, 안전을 위해 **토크 센서**를 활용한 내재적 임피던스 제어 모드도 있다. 우리의 시스템에서는 일단 **비전 기반** 접근이라 로봇이 테이블 등에 세게 부딪힐 위험은 낮았으나, 만일의 상황을 고려해 **그리퍼 축 방향(Z축)** 움직임에는 약간의 유연성을 두었다. 이는 **엔드 이펙터 좌표계**에서 Z축 위치 제어 이득을 살짝 낮추고(D 게인 높여) 구현하였다. 또한 시각 추적이 불확실할 경우를 대비해, 목표 물체에 도달 직전 속도를 줄이고 멈추기 전에 **수 mm 간격**을 남겨 놓는 보수적 접근을 취했다. 이는 일종의 **소프트 랜딩**을 구현한 셈이며, 사람이 물체를 잡을 때도 바로 꽉 쥐기보다 살짝 접촉을 확인하는 동작과 유사하다.

결론적으로, ROS2 tf, PID, 임피던스 등의 개념은 수업 이론과 실제 구현을 이어주는 가교였다. tf를 통해 여러 좌표계의 **행렬 변환 수식**이 코드로 체화되었고, PID를 통해 **선형 제어이론**이 로봇 모터 제어에 응용되었으며, 임피던스 개념을 통해 **물리적 상호작용**에서의 안정성 논의를 할 수 있었다. 이러한 연계는 우리가 이론을 단순 암기가 아니라 **실제 시스템 최적화의 도구**로 활용할 수 있게 해주었다.

## 3. 방법론 및 시스템 구성 요소 설명 (End Effector 경로 계획 포함)

이 절에서는 본 연구에 개발한 **자연어-비전 통합 매니퓰레이터 제어 시스템**의 구성 요소와 작동 방법을 단계별로 설명한다. 전체 시스템은 **자연어 이해 → 시각 인지 및 추적 → Grasp Pose 산출 → 매니퓰레이터 제어**의 파이프라인으로 이루어져 있다. 각 단계에서 앞서 소개한 이론들이 어떻게 적용되는지 구체적으로 다룬다. 특히 End Effector의 **경로 계획**과 **실시간 제어 전략**을 상세히 기술한다.

### 3.1 자연어 명령 처리 및 과제 정의

사용자가 입력하는 자연어 명령은 예를 들어 \*“빨간색 컵을 집어줘”\*와 같이 대상 객체와 동작을 포함한다. 자연어 처리 모듈은 **자연어 문장**을 해석하여, \*\*어떤 객체(object)\*\*를 **어떻게(how)** 다룰지에 대한 구조화된 정보를 추출한다. 본 연구에서는 복잡한 언어 이해보다는 **키워드 기반**의 간단한 파싱을 수행하였다. 예를 들어 문장에서 **명사**를 추출하여 대상 객체의 이름으로, **동사**를 추출하여 액션 타입으로 삼는다. (`집어줘` → grasp action, `건네줘` → pick and place 등으로 매핑 가능.) 그 결과 **Task Command**가 정의되며, 이는 이후 비전 모듈과 제어 모듈에 영향을 준다. 예컨대 객체 이름이 `"컵"`으로 파싱되면, 비전 모듈은 사전에 학습된 `"컵"` 클래스 또는 관련 텍스트 임베딩을 사용해 해당 객체를 탐지하도록 설정된다. 액션이 `집기`이면, Grasp Pose 계산 모듈이 **파지 가능한 자세** (예: 그리퍼가 컵을 옆에서 잡을지 위에서 잡을지 등)를 고려하게 된다.

> **구현 노트:** 자연어 모듈에는 명시적인 기계학습 모델 언급은 피하지만, 내부적으로 open-vocabulary 모델의 언어 인코더를 활용해 객체 이름과 시각 피처를 연결하였다. 이는 시스템 정확도 향상에 기여하지만, 본 보고서에서는 세부 알고리즘명을 생략하고 전체 구조의 일부분으로서만 다루었다.

### 3.2 시각 인지 및 객체 추적

Vision 모듈은 **RGB-D 카메라**로부터 실시간 영상을 받아, 자연어 모듈이 지정한 **목표 객체**를 탐지하고 위치를 추정한다. 먼저 **객체 검출** 단계에서, 장면 내 목표 객체의 2D Bounding Box를 찾아낸다. 그런 다음 **세그멘테이션**을 통해 객체의 픽셀 단위 마스크를 얻고, 이를 Depth 이미지에 적용하여 **해당 객체의 포인트클라우드**를 추출한다. 마지막으로 포인트클라우드의 **중심**이나 **기하학적 특징**을 이용해 객체의 3차원 **중심 좌표**와 **방향**(orientation 추정이 가능하면)을 계산한다. 이 정보는 **객체 좌표계**로 정의되며, ROS2 `tf`의 한 프레임으로 브로드캐스팅된다.

물체의 위치는 시간에 따라 변할 수 있으므로(예: 사람이 움직이거나 로봇 팔이 테이블을 건드려 물체 위치가 약간 변동), **객체 추적기**가 동작하여 매 프레임 객체의 위치를 갱신한다. 추적기는 이전 프레임의 세그멘테이션 정보를 바탕으로 다음 프레임에서도 해당 객체 픽셀들을 찾아내며, 주기적으로 **재검출**을 병행하여 드리프트를 막는다. 최종적으로, \*\*물체의 3D Pose (위치 및 간이 방향)\*\*가 지속적으로 제공되어 제어 모듈이 참고할 수 있게 된다.

> **참고:** 깊이 센서 노이즈로 인해 객체 좌표가 불완전할 경우 **심도 보정(depth completion)** 기법을 활용하여 누락된 점을 보충하고, 중심 좌표 계산의 안정성을 높였다. 하지만 이러한 비전 알고리즘적 기법의 세부 내용은 생략한다.

### 3.3 Grasp Pose 산출 및 End Effector 목표 정의

Vision 모듈로부터 특정 시점의 **객체 3D 위치**(예: 베이스로부터 x=0.5 m, y=0.2 m, z=0.3 m 등)를 입력받으면, **Grasp Pose 산출 모듈**은 해당 객체를 잡기 위한 End Effector의 목표 자세를 계산한다. 그리퍼로 물체를 잡기 위해서는 단순히 같은 위치로 이동하는 것 이상으로, **올바른 방향**과 **거리**로 접근하는 것이 중요하다. 예를 들어 책상 위 컵을 잡을 때는 **위에서 내려잡는 방식**(톱-다운)이나 **옆에서 쥐는 방식** 중 하나를 선택해야 하며, 각 경우 End Effector의 방향(그리퍼 각도)과 **접근 경로**가 달라진다.

우리 시스템은 입력 명령이 단순 “집어”일 경우, 사전에 정해둔 기본 전략으로 **톱-다운 그립**을 우선 시도한다. 즉 엔드 이펙터의 **그리퍼가 수직 아래를 향한 채** 물체 위쪽의 적당한 높이까지 이동한 후, 내려와 파지하는 방식이다. 이를 구현하기 위해 Grasp Pose를 계산할 때 **물체의 중심 좌표**를 받으면, 해당 좌표의 바로 위 **z_offset (예: +10 cm)** 만큼 떨어진 지점을 **접근 준비 자세**로 정한다. Orientation은 **그리퍼의 두 핑거가 물체의 폭 양옆을 끼울 수 있도록** 설정하는데, 만약 물체의 주된 길이 방향을 인식할 수 있으면 그에 맞춰 회전시키고, 아니라면 카메라 시야를 확보하기 위해 **카메라(전방) 방향이 물체 중심을 향하도록** 기본 설정한다. 이 Orientation 설정이 바로 \*\*“항상 객체를 바라보면서”\*\*의 초기 조건이 된다. End Effector의 좌표계에서 카메라가 정면을 향하고 있으므로, End Effector를 물체 쪽으로 향하게 한다는 것은 카메라 시야로 물체를 계속 포착하게 하는 것과 같다.

Grasp Pose = { position: (object_x, object_y, object_z + offset), orientation: (quaternion toward object) } 로 계산된 목표 자세는 이후 **역기구학 모듈**로 전달된다. 이때 **유효성 검사**도 수행되는데, 로봇의 작업범위 내인지, 해당 자세에서 그리퍼를 벌렸을 때 물체를 잡을 수 있는지(예: 물체가 너무 커서 그리퍼 폭 초과는 아닌지) 등을 확인한다. 만약 계산된 Grasp Pose가 비현실적이거나 로봇으로 도달 불가능하면, Grasping 알고리즘은 **실패 플래그**를 반환하고, 다른 접근 방향을 시도하거나 사용자에게 **실패**를 알리게 된다. 반대로 유효한 Grasp Pose라면, 이를 최종 **목표 End Effector Pose**로 확정한다.

End Effector 목표 Pose는 ROS2 `geometry_msgs/PoseStamped` 형태로 표현되어 제어 모듈에 전달된다. 이 PoseStamped는 `"base_link"`로 기준 좌표계가 명시된 절대 좌표이며, orientation은 quaternion으로 들어있다. 이 정보는 앞으로 **경로 계획**과 **제어기 입력**의 기준이 된다. 정리하면, Vision+Grasp 모듈은 *AI적인* 불확실성을 거치면서 최종 산출물로서 **로봇공학적인 정확한 목표**(6-DoF pose)를 내놓고, 이후 단계부터는 결정론적 로봇 제어 알고리즘이 이를 책임지게 된다.

### 3.4 매니퓰레이터 경로 계획 및 적응형 제어 전략

이제 가장 중요한 **매니퓰레이터 제어 단계**이다. 이 단계에서는 로봇 팔이 현재 자세에서 목표 End Effector 자세로 **어떻게 움직일지**를 결정하고 실행한다. 앞서 2.4절에서 논의한 **Joint-space vs Cartesian 경로** 전략을 여기서 구체적으로 구현하였다.

**(1) 초기 경로 계획 (Joint-space global planning):**
제어 모듈은 우선 목표 Pose까지 도달하는 **전역 경로**를 계획한다. 이를 위해 MoveIt 모션 계획 패키지를 활용하였는데, Plan 요청 시 목표 Pose를 입력하면 MoveIt이 내부 IK 솔버로 목표 관절각 세트를 찾고, RRT스타(RRT\*) 등의 알고리즘으로 **관절 공간 경로**를 탐색한다. 목표 Pose까지 여러 가지 관절 움직임이 있을 수 있으나, MoveIt은 **최단 경로** 혹은 **스무스한 경로**를 비용함수로 하여 충돌 없는 해를 산출한다. 예컨대 로봇이 테이블 아래에서 위에 있는 컵을 잡으러 간다면, 곧장 팔을 뻗는 대신 살짝 후퇴했다 올라가는 경로를 찾을 수 있다 (장애물 회피). 이러한 자동 경로계획으로 얻어진 joint trajectory는 (시간, $\theta_1,...,\theta_6$) 점들의 리스트로 표현되며, 각 구간마다 3차 다항 보간으로 매끄럽게 이어준다.

MoveIt 플래너가 산출한 경로는 기본적으로 End Effector의 orientation까지 고려하지만, 중간 단계에서 orientation을 완전히 고정하진 않는다. 따라서 이 경로를 따라가면 End Effector는 **최종적으로는 물체를 향하나 이동 중 약간 다른 방향을 볼 수도 있다.** 그러나 우리의 목표는 이동 **내내** 물체를 시야에 두는 것이므로, 여기서 **적응형 제어**가 개입한다.

**(2) 적응형 경로 조정 (Visual servoing-based adjustment):**
로봇이 목표에 가까워질수록, Vision 모듈의 피드백을 반영하여 경로를 **실시간 보정**한다. 구체적으로, End Effector가 물체와 **거리 30cm** 이내로 들어오면, 자체 제작한 **Visual Servo 제어기**를 활성화하였다. 이 제어기는 주기적으로 (예: 30Hz) 현재 카메라 이미지에서 물체의 위치를 검사하고, 만약 중심에서 벗어나면 End Effector의 yaw, pitch를 미세 조정한다. Jacobian을 이용해 계산한 각속도 명령은 로봇의 \*\*내부 제어 주기(1kHz)\*\*에 맞춰 관절 속도로 변환되어 즉각 적용된다. 결과적으로 로봇은 목표 경로를 따라가면서도, **센서 피드백에 따라** 미세하게 궤적을 수정하여 물체를 정중앙에 유지한다. 이러한 과정은 연속적으로 일어나 마치 **엔드 이펙터가 시선을 고정한 채 움직이는** 효과를 낸다.

한편, Visual Servo 제어는 작은 Orientation 오차를 줄이는 용도일 뿐, 전체 경로를 근본적으로 변경하진 않는다. 만약 MoveIt이 계획한 경로 상에서 예기치 못한 장애물이 나타나거나(예: 사람이 끼어듦) 물체 위치가 크게 변하면, **전역 재계획**이 필요하다. 이는 안전 상 매우 중요한데, 우리 시스템에서는 **Planning Scene**을 실시간 업데이트하여 큰 변화가 감지되면 MoveIt에 **새 경로**를 요청하도록 했다. 예를 들어 물체가 10cm 이상 이동하면 현재 경로를 폐기하고 새로운 IK와 경로를 계산한다. 다행히 Edge 디바이스 성능이 뛰어나 (Orin 64GB) 이러한 재계획도 수백 ms 내 수행 가능하였다.

**(3) Trajectory 실행 및 PID 제어:**
확정된 경로 (혹은 계속 보정되는 경로)를 따라 로봇을 움직이기 위해, joint trajectory를 ROS2의 **FollowJointTrajectory** 액션으로 보냈다. 이 액션 서버는 각 관절의 목표 각도를 시간 스탬프에 따라 보내주며, 로봇 내장 **PID 제어기**가 해당 값을 추종하게 된다. 우리는 MoveIt 경로를 보낼 때 **시간 스케일**을 조정하여, 관절별 최고 속도가 Kinova 사양의 80%를 넘지 않도록 했다. 또한, 특이점이 우려되는 구간은 아예 경로에서 배제되므로 (MoveIt이 알아서 피함) 안전성은 확보되었다. 경로 실행 중 Visual Servo 제어로 인한 작은 수정은, trajectory를 완전히 다시 실행하지 않고 **속도 제어 명령**으로 overlapped 시켰다. 이는 Kinova Kortex API의 Cartesian velocity control 모드를 활용한 것으로, joint trajectory를 따르는 중에도 추가적인 미세 조정을 superimpose하여 부드럽게 반영할 수 있었다.

실험 결과, 이러한 복합 제어 전략은 **엔드 이펙터가 항상 객체를 향한 상태**로 안정적으로 목표 지점에 도달하는 것을 확인하였다. 물체가 고정된 경우에는 굳이 visual servo 보정이 필요 없었으나, 인간이 물체를 살짝 움직이는 시나리오에서 로봇이 이를 끝까지 추적하며 따라가는 **강건함**을 보여주었다. 이는 만약 전적으로 고정 경로만 사용했다면 불가능하거나, 최소한 최종 잡기 위치에서 큰 오차를 냈을 상황이다. 우리의 적응형 제어는 **AI 인지 모듈**과 **로봇 제어 모듈**이 상호작용하는 좋은 사례로, 센서 피드백 루프를 도입함으로써 **정확도**와 **유연성**을 모두 향상시켰다.

### 3.5 시스템 통합 및 신뢰성 확보 방안

전술한 각 모듈을 통합하여 하나의 ROS2 시스템으로 실행하였다. 전체 구조를 요약하면 그림 1과 같다:

* **NLU 노드:** 사용자 명령 구독 → 객체명, 동작 파싱 → Vision 및 Planning 모듈에 신호 전달
* **Vision 노드들:** 카메라 토픽 구독 → 객체 검출/세그멘테이션 → Depth기반 좌표 계산 → tf 브로드캐스트 (object frame)
* **Planning & Control 노드:** Vision으로부터 목표 Pose 수신 → IK 계산 → MoveIt 경로 요청 → Joint Trajectory 실행 + Servo 보정
* **Gripper 노드:** 로봇 팔 경로 완료 신호 수신 → 그리퍼 개폐 제어 수행 (물체 파지 동작)

특히 **Gripper 제어**는 End Effector가 목표 위치에 도착한 뒤 이루어진다. 로봇 팔이 멈추면 Gripper 노드는 **닫힘 명령**을 보내 컵 등을 쥔다. 이때 **힘 조절**이 필요한데, Kinova 그리퍼는 토크 모드를 지원하므로 **전류 제한**을 걸어, 컵을 적당한 힘으로 잡고 더 이상 잠기지 않도록 하였다. 파지 성공 여부는 **포스 센싱**이나 **그리퍼 자세 피드백**으로 판단할 수 있는데, 본 시스템에서는 그리퍼 모터 각도가 일정 이상 닫히지 않으면 (= 뭔가 잡혔으면 더 이상 완전히 닫히지 않음) 성공으로 간주하였다.

종합적으로 시스템 신뢰성을 높이기 위해 다음과 같은 방안을 적용하였다:

* **이중 검증:** Vision 검출 결과와 Grasp Pose의 물리적 타당성을 IK 전에 한 번, 실행 전에 한 번 검증하였다. 예를 들어 목표 Pose가 로봇 베이스로부터 너무 멀거나 (760mm 이상) 하면 잡기 전에 사용자에게 *"범위 밖 대상"* 피드백을 주도록 하였다.
* **특이점/충돌 모니터링:** 경로 실행 중 실시간으로 관절각을 모니터링하여, 만약 자코비안 행렬이 급격히 나빠지는 구간(특이점 접근)이 감지되면 즉시 정지하도록 하였다. 또한 MoveIt Planning Scene에 테이블과 환경의 메쉬를 포함시켜, *사전*에 충돌경로를 배제하였으며 실행 중 예기치 않은 장애물(휴먼 등) 인식은 Depth 센서 거리값으로 감지하여 정지시켰다.
* **타이밍 조정:** Edge Device 상에서 여러 딥러닝 모듈이 동작하다보니 제어주기와 센서주기의 타이밍 조정이 중요했다. 제어 루프는 100Hz, 시각 인지는 30Hz, 모터 내부 제어 1000Hz 등 서로 다르므로, **메시지 동기화**와 **스레드 우선순위 조정**으로 latency를 최소화하였다. 이를 통해 End Effector 제어 명령이 실제 로봇 움직임에 반영되기까지 지연을 50ms 이내로 유지하였다.

위와 같은 시스템 구성 요소들의 협조적인 동작으로, 최종적으로 **사용자의 한마디 지시에 로봇이 주변을 살피고 물체를 집어 올리는** 데모가 가능해졌다. 이는 단순한 AI 알고리즘의 집합이 아니라, **로봇공학 이론**에 기반하여 치밀하게 구성된 **사이버-물리 시스템**이라는 점에서 의의가 있다.

## 4. 시스템 구조와 구현 전략에 대한 결과 및 적용 가능성

본 연구에서 구현한 시스템의 **구조**는 **모듈화**와 **계층화**로 특징지을 수 있다. 모듈화 덕분에 자연어 처리, 비전 인지, 로봇 제어 각 부분이 독립적으로 개발·개선될 수 있었고, 계층화 덕분에 **고수준 AI**와 **저수준 제어**가 안정적으로 연동되었다. 이러한 구조적 결과를 바탕으로 얻은 성능 및 적용 가능성을 몇 가지 측면에서 논의한다.

**(a) 정확성 및 성공률:** 실제 Kinova Gen3 Lite 로봇을 사용한 실험에서, 다양한 물체(컵, 병, 장난감 등)에 대해 10회 이상의 grasp 시도를 수행한 결과 **80% 이상의 성공률**을 보였다. 실패 사례의 대부분은 비전 모듈이 물체를 잘못 인식하거나 (예: 유사한 색 착오) Grasp Pose 계산이 부정확했던 경우로, **제어 모듈의 문제로 인한 실패는 거의 없었다.** 이는 매니퓰레이터 제어 부분이 매우 **견고**하게 작동했음을 의미한다. 특히 End Effector의 위치 오차는 평균 수 mm, orientation 오차는 5° 이내로 측정되어, 물체 파지에 충분한 정밀도를 확보했다. 이러한 정밀도는 **정확한 IK 해** 및 **실시간 보정 제어** 덕분에 가능했으며, 만약 수업시간에 배운 이론들을 적용하지 않았다면 달성하기 힘들었을 것이다.

**(b) 부드러운 동작 및 안전성:** 로봇의 움직임은 전 구간에 걸쳐 **매우 부드럽고 자연스러웠다.** 관찰자들은 “로봇이 마치 사람처럼 살펴보고 천천히 집는다”는 인상을 받았다고 피드백했다. 이는 Joint-space 다항 경로 계획으로 관절 가속도를 제어하고, Visual Servo 단계에서도 P 제어를 통해 과격한 조정이 일어나지 않도록 한 결과이다. 또한 사람과 가까운 거리에서 로봇이 움직이는 시나리오를 고려하여, **비상 정지** 기능과 **힘 제한**을 설정해 두었다. 예를 들어 사람 손이 갑자기 로봇 앞을 가로막으면, Depth 센서가 감지하여 0.1초 이내 로봇을 정지시켰다. Kinova Gen3 Lite 자체가 **안전펜스 없이 쓸 수 있는 협동로봇 수준의 안전 등급**이기도 하지만, 소프트웨어적으로도 안전장치를 마련함으로써 **HRI (Human-Robot Interaction)** 측면의 적용 가능성을 높였다.

**(c) 실시간 처리 성능:** Edge Device인 Jetson Orin에서 모든 모듈을 구동한 결과 **end-to-end 지연**(음성 입력 \~ 물체 파지 완료) 시간이 평균 5~~6초 정도로 나타났다. 이 중 언어+비전 인지에 4초 정도(딥러닝 연산), 로봇 움직임에 2초 정도가 소요된 셈이다. 로봇 제어 자체는 1kHz 루프로 충분히 빠르게 처리되고, MoveIt 경로 계획도 보통 100~~200ms 내 결과를 줬다. 가장 큰 지연 요인은 딥러닝 모델들이었지만, 이는 하드웨어 업그레이드나 최적화로 개선 가능하다. 중요한 것은 제어 부분이 병목이 아니었고, 오히려 비전 결과를 기다리며 유연하게 동작 시작 타이밍을 맞추었다는 점이다. 이러한 실시간 성능은 본 시스템을 향후 **온보드 지능 로봇**으로 발전시키는 데 긍정적인 신호다.

**(d) 이식성과 확장성:** 본 시스템 구조는 특정 하드웨어나 알고리즘에 종속되지 않도록 설계되었다. Kinova Gen3 Lite 이외에 다른 6-DOF 로봇으로도 이식 가능하며, 실제로 유사한 DH 파라미터를 가진 UR5 로봇 모형에 코드를 적용해본 결과 큰 수정 없이 동작함을 확인했다. 이는 우리가 사용한 **ROS2 + MoveIt 인터페이스**가 표준화되어 있고, 제어 로직이 **일반적인 로봇 운동학**에 기반했기 때문이다. 또한 비전 모듈의 모델을 교체하거나 (예: 다른 객체 검출기) 자연어를 추가 학습시키는 등의 **확장**도 용이하다. 이처럼 구조가 견고하므로, 본 연구 시스템을 응용하여 **공장 자동화**, **스마트 홈 로봇** 등 다양한 도메인에 적용할 수 있을 것으로 보인다. 예컨대 물류창고에서 “저 상자 옮겨줘”라고 명령하면 팔이 해당 상자를 들어 올리는 작업이나, 가정에서 “물 한 잔 가져와” 하면 로봇이 컵을 집어 사용자에게 가져다주는 시나리오 등을 구현하는 데 본 시스템이 기초를 제공할 수 있다.

**(e) 한계 및 개선 여지:** 한편 현재 구조의 한계로는 **동적인 객체**에 대한 대응이 제한적이라는 점이 있다. 물체가 사람이 손으로 움직일 정도의 느린 속도는 추적 가능했지만, 물체가 스스로 떨어지거나 빠르게 이동하면 로봇이 따라가기 어려웠다. 이는 주로 비전 모듈의 프레임레이트와 반응시간 한계 때문이다. 제어 측면에서는, 아직 **로봇의 동역학적 한계**(모터 토크 등)를 경로 계획에서 완벽히 고려하지 못한 점이 있다. 현재는 속도와 가속도 제한만 두었지만, 향후 **토크 한계, 진동 모드**까지 고려한 **최적제어**를 도입하면 더욱 안전한 경로를 만들 수 있다. 또한 충돌 회피는 quasi-static한 장애물만 고려했는데, **움직이는 장애물**(다른 인간 등)에 대해서는 더 발전된 동적 계획 알고리즘 (Dynamic RRT\* 등)을 써야 할 것이다.

그럼에도 불구하고, 본 연구의 구현 전략은 **학술적으로나 실용적으로나 유의미한 성과**를 보여주었다. 로봇공학의 이론을 토대로 실제 시스템을 구축함으로써, 이론의 **실효성**을 입증하고 현장에서의 **응용 가능성**을 확인한 것이 가장 큰 결과라 할 수 있다.

## 5. 결론 및 수업과의 연계 고찰, 향후 연구방향

본 프로젝트를 통해 \*\*“자연어로 명령하면, 로봇이 시각 정보를 바탕으로 적절히 해석하여 물체를 잡는다”\*\*는 통합 시스템을 설계·구현하였다. 이는 단순히 개별 AI 기술의 조합이 아니라, **로봇 매니퓰레이터 기구학/제어 이론**을 토대로 **지능형 로봇 행동**을 이끌어낸 사례라는 점에서 의의가 크다. 보고서 전반에 걸쳐, 수업에서 학습한 정방향/역방향 기구학, 자코비안, 궤적 계획, 제어 개념들이 어떻게 실제 코드와 하드웨어 상에서 기능하는지 고찰하였다.

**연계 고찰:** 수업에서 배운 공식을 처음에는 단순히 이론으로 여겼으나, 본 연구를 진행하며 그 중요성을 절감하였다. 예를 들어, DH 파라미터로 로봇을 모델링하고 IK를 푸는 과정은, 로봇을 **수학적으로 이해**하는 데 필수적이었다. 이를 모르고서는 AI가 던져준 목표 위치를 로봇이 어떻게 도달할지 막막했을 것이다. 자코비안도 마찬가지로, 그 개념을 아니까 **엔드 이펙터의 움직임을 관절 움직임으로 직관적으로 환산**할 수 있었고, 실시간 제어에서 감각적으로 활용할 수 있었다.Trajectory planning에 대한 수업 내용은 실제 MoveIt이나 제어기에서 **구체적인 파라미터 튜닝**을 할 때 빛을 발했다. “왜 3차 다항식을 쓰는가?”에 대한 이해가 있으니, 로봇 움직임을 보면서 “아, 초반/후반 가속도가 0으로 가야 충격이 없지” 등을 판단하며 프로파일을 조정할 수 있었다. 또 PID 제어의 원리를 잘 알고 있으니, 제어기 게인을 조절할 때 **과감하게 P를 높이거나 D를 넣어서 감쇠**시키는 식으로 체계적인 접근이 가능했다. 정리하면, 이론과 실제의 거리가 상당히 가깝다는 것을 깨달았고, 이론 없이는 시행착오를 겪을 부분들을 상당히 줄일 수 있었다.

**향후 연구 방향:** 본 연구를 기초로 확장 가능한 여러 방향이 있다. 첫째, **강화학습** 등 **학습 기반 제어**를 도입하여 현재는 수동 튜닝한 부분들을 자동 최적화할 수 있다. 예를 들어, visual servoing에서 P, D 게인을 강화학습으로 최적화하거나, IK 해 선택을 학습된 정책으로 결정하면 더 높은 성공률과 효율을 얻을 수 있을 것이다. 둘째, **양손 로봇**이나 **다중 로봇 협업**으로 확장하는 방향이다. 현재 시스템은 하나의 팔이 하나의 물체를 잡는 것인데, 이를 두 팔이 협동으로 물체를 집거나 운반하는 시나리오로 늘릴 수 있다. 이 경우 상대 로봇 간의 **좌표계 정합**과 **동기 제어** 등이 추가 연구과제가 될 것이다. 셋째, **비전-제어 융합의 고도화**로, 이미지 피처 기반의 정교한 visual servo 제어 (예: IBVS, PBVS 등)를 적용하여 움직이는 물체를 잡거나, 로봇이 움직이면서도 계속 카메라로 주변을 인식하는 **동적 인지**를 구현할 수 있다. 넷째, **사용자 피드백 인터페이스**를 추가하여, 음성으로 로봇과 대화하듯 피드백을 주고받으며 잡는 과정을 조율하는 인간-로봇 상호작용 연구로 발전시킬 수 있다.

마지막으로, 본 프로젝트를 수행하며 느낀 것은 \*\*“기초가 튼튼해야 응용을 잘 할 수 있다”\*\*는 점이다. 수업 시간에 배운 원리들이 이번에 모두 연결고리처럼 쓰였다. 이를 통해, 학부/대학원에서 배운 로봇공학 지식이 최신 AI 시대에도 여전히 중요하고, 오히려 **AI를 물리 세계에 적용하는 필수 요소**임을 확인하였다. 앞으로도 이론과 실무를 겸비한 로봇 연구를 계속 수행하여, 더욱 자연스러운 인간-로봇 상호작용과 유용한 로봇 서비스를 개발하고자 한다.

以上으로, **Kinova Gen3 Lite 6DoF 매니퓰레이터**를 활용한 자연어 기반 시각통합 그리핑 시스템의 최종 보고를 마친다. 본 연구를 통해 얻은 지식과 경험이 향후 로봇공학 연구 및 산업 현장 적용에 밑거름이 되길 기대한다.
