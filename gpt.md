# Jetson AGX Orin 기반 실시간 로봇 그리핑 시스템 연구 분석

## 1. 경량 Visual Object Tracking 알고리즘 비교 및 추천

Jetson AGX Orin과 같은 엣지 디바이스에서 **실시간**으로 동작 가능한 Object Tracker를 선택하려면, 정확도와 속도의 균형이 중요합니다. NanoSAM 자체의 tracking 기능은 제한적이므로, 별도의 **경량 추적 알고리즘**을 도입해 detection 결과를 지속적으로 추적하는 것이 필요합니다. 대표적인 tracking 접근법으로는 전통적인 **Correlation Filter 기반 추적**과 딥러닝 기반 **Siamese 네트워크 추적**이 있습니다. 또한 detection 결과를 프레임마다 연계하는 **단순 IoU 기반 추적**도 사용할 수 있습니다. 각 방법의 장단점을 비교하면 다음과 같습니다:

* **IoU 기반 Tracker (예: NVidia DeepStream의 IoU tracker)** – **장점:** 연산이 거의 들지 않을 정도로 빠르며, detector가 내는 bounding box를 그대로 연속 프레임에 연계합니다. 추가 모델 없이 **지연 시간을 거의 증가시키지 않으면서** tracking ID 부여 정도의 역할을 수행합니다. **단점:** Object의 appearance(외형) 변화나 occlusion에 대응하지 못하며, detection이 없는 프레임에서는 대상 위치를 예측하지 못합니다. 정밀하게 객체 위치를 쫓는 데 한계가 있어 **정확도가 낮고 drift**가 발생할 수 있습니다.

* **Correlation Filter 기반 Tracker (예: KCF, CSRT 등)** – **장점:** 객체 주변 패치를 잘라 HOG 등의 특징으로 학습한 필터를 통해 빠르게 위치를 업데이트합니다. **KCF**의 경우 매우 빠르며 CPU만으로도 수십\~수백 FPS 구현이 가능해 **자원 소모가 낮습니다**. **CSRT**는 채널/공간 신뢰도를 반영한 개선형으로 **KCF 대비 정확도가 높으며** scale 변화에도 비교적 강인합니다. **단점:** KCF는 occlusion이나 극심한 모습 변화 시 쉽게 위치를 잃고 drift합니다. CSRT는 정확도는 높지만 KCF보다 **속도가 느려** CPU 환경에서 실시간 처리 시 FPS가 떨어집니다. 예를 들어, OpenCV 구현 기준 CSRT는 KCF보다 느려 30 FPS 미만일 수 있습니다. 둘 다 객체가 급격히 변형되거나 3D 회전하면 추적 성능이 저하됩니다.

* **Siamese 네트워크 기반 Tracker (예: SiamFC, SiamRPN++, SiamMask 등)** – **장점:** 딥러닝으로 학습된 임베딩 특징을 사용하여 초기 대상(template)과 현재 프레임을 매칭함으로써 **높은 정확도와 견실한 추적**을 달성합니다. 특히 SiamRPN계열은 Bounding box 회귀를 함께 수행해 scale 변화에도 대응하고 정확한 위치 예측이 가능합니다. **Siamese 기반 추적기는 속도-정확도 면에서 균형이 뛰어나**, 단일 객체 추적에 적합하다는 평가를 받습니다. Jetson Orin의 GPU에서 TensorRT 최적화를 통해 SiamRPN++ 등을 실행할 경우 실시간에 근접한 속도를 얻을 수 있습니다. **단점:** 모델 크기와 연산량이 커서(수십~~수백 MB) 전통 기법보다 \*\*자원 소모(GPU 메모리, 연산)\*\*가 큽니다. 미세 튜닝 없이 기본 SiamRPN++ PyTorch 모델을 사용하면 Jetson에서 10~~20 FPS 수준에 머물 수 있어 최적화가 필요합니다. 또한 딥러닝 특성상 추적 실패 시 재탐지 로직이 없으면 recover가 어렵습니다.

* **경량 딥러닝 Tracker (예: LightTrack, NanoTrack)** – **장점:** 최근 연구들은 모바일/임베디드용으로 **초경량 Siamese 추적기**를 개발하고 있습니다. NanoTrack의 경우 MobileNetV3 기반으로 모델 크기 수 MB 수준에 불과하며, Apple M1 CPU에서 **200 FPS 이상**을 달성할 만큼 **매우 빠릅니다**. 경량화하면서도 VOT 챌린지에서 기존 Siamese 방법 대비 준수한 정확도를 보여, 임베디드 시스템에 적합합니다. **단점:** 절대적인 SOTA 모델들보다는 약간 낮은 정확도나 robustness를 보일 수 있고, 개발 초기단계이므로 지원되는 프레임워크나 튜토리얼이 한정적일 수 있습니다. 그래도 연산량이 작아 **Jetson의 GPU/CPU 리소스 점유를 최소화**하면서 작동한다는 큰 장점이 있습니다.

위 내용을 토대로 주요 Tracker들의 특징을 **FPS (속도)** 및 **자원 사용량** 측면에서 비교하면 다음과 같습니다:

| **Tracking 알고리즘**           | **방식 및 특징**                    | **장점**                             | **단점**                                  | **추론 속도 / 리소스**                                                    |
| --------------------------- | ------------------------------ | ---------------------------------- | --------------------------------------- | ------------------------------------------------------------------ |
| **IoU Matching** (Detect마다) | Detection 결과를 IoU로 연계 (MOT 방식) | 매우 간단하고 초고속 (딥러닝 없음)               | detection 의존도 매우 높음; 외형 변화 대응 불가        | ▲ **FPS:** Detection만큼 (추가 부하 무시가능)<br>▲ **리소스:** GPU/CPU 부담 매우 낮음 |
| **KCF** (OpenCV 구현)         | HOG 기반 Correlation Filter      | CPU에서도 실시간 (수백 FPS 가능)             | scale/조명 변화 약함; occlusion 발생 시 drift    | ▲ **FPS:** 100+ (해상도 의존)<br>▲ **리소스:** CPU 1코어 위주                  |
| **CSRT** (OpenCV 구현)        | 채널&공간 신뢰 필터 (Correlation 개선)   | KCF 대비 정확도 향상                      | KCF보다 느려 일부 응용엔 과부하                     | ▲ **FPS:** \~30 (CPU)<br>▲ **리소스:** CPU 1코어 중간점유                   |
| **SiamRPN++** (딥러닝 Siamese) | ResNet 등 CNN 기반 Region 추적      | 정확도 높고 scale 변화 대응<br>단일 객체 추적에 정밀 | 모델 무거워 Jetson 최적화 필요;<br>추적 실패 시 복구 어려움 | ▲ **FPS:** \~20-30 (TensorRT 최적화 시)<br>▲ **리소스:** GPU 연산 집중        |
| **NanoTrack** (경량 Siamese)  | MobileNetV3 기반 초경량 추적기         | 매우 빠르고 모델 크기 작음;<br>임베디드에 최적       | SOTA 대비 약간 정확도 저하<br>(하지만 실용적 수준)       | ▲ **FPS:** 200+ (CPU에서도 가능)<br>▲ **리소스:** GPU 부담 매우 적음             |

*주: 상기 FPS 수치는 대략적인 추정치이며, 입력 해상도와 최적화 정도에 따라 달라집니다. Jetson AGX Orin의 경우 1792 CUDA 코어와 8\~16GB 메모리를 갖추어, 적절히 최적화된 Siamese 계열도 30FPS 근접 가능성이 있습니다. KCF/CSRT 등은 CPU로 구동해도 무리 없으며, NanoTrack은 CPU만으로도 충분히 real-time이 가능합니다.*

**결론적으로**, **Jetson AGX Orin 기반의 본 시스템에는 경량화된 Siamese 계열 추적기를 추천**합니다. NVidia 제공 IoU tracker처럼 detection에 전적으로 의존하는 방식은 속도는 훌륭하지만 정밀 추적이 어려워 grasping과 같이 정밀한 object 상태 파악이 필요한 작업에는 부적절합니다. 반면 **Siamese 기반 추적**은 **속도와 정확도를 모두 만족**시키며 single object에 집중할 수 있습니다. 특히 MobileSAM 등 segmentation 모델의 처리 간격을 벌리고 그 사이를 tracker로 메꾸는 본 구조에서는, **추적기의 추가 지연이 매우 적고**(수 ms 수준) **Object 위치를 안정적으로 예측**해주는 것이 중요합니다. NanoTrack/LightTrack 같은 최신 경량 추적기를 도입하면 **시스템 전체 latency에 큰 영향 없이** NanoSAM의 한계를 보완할 수 있을 것입니다.

또한 **Detect-then-Track vs 주기적 Detection**에 대한 전략도 고려해야 합니다. 일반적으로 **tracking을 연속적으로 수행**하면 detection을 매 프레임 돌리는 것보다 연산 부하는 줄지만, 오래 추적할수록 오차 누적(drift) 우려가 있습니다. 이를 보완하기 위해 **일정 주기마다 다시 detection으로 보정**하는 접근이 유효합니다. 실제로 Panico 등(2020)은 **추적 신뢰도에 따라 detection을 필요할 때만 수행하는 Adaptive Detection Tracking** 기법을 제안하여 **새 탐지와 추적 간 균형을 최적화함으로써 시스템 속도를 높였다**고 보고하였습니다. 이러한 **Refine-on-Demand** 방식은, 추적기의 confidence가 떨어지거나 객체 상실 징후가 있을 때만 NanoOWL로 재탐지/세분화하여 오차를 바로잡는 접근입니다. 이 방법은 불필요한 매 프레임 detection을 피하면서도, **추적 실패나 occlusion 발생 시 신속히 복구**하여 **강인성을 높일 수 있다는 장점**이 있습니다. 따라서 **권장 전략**으로는: **초기**에 NanoOWL+NanoSAM으로 정확한 인식/세분화를 수행한 뒤, **연속 프레임**에서는 권장한 **경량 tracker**로 빠르게 객체를 추적하고, **주기적으로 (예: N프레임마다)** 또는 **tracker 신뢰도 하락 시** detection을 다시 호출하는 **Detect-then-Track (DtT) + Refine-on-Demand (RoD)** 체계를 사용하는 것입니다. 이처럼 **tracking과 detection을 적절히 혼용**하면, **시스템 평균 지연시간을 최소화**하면서도 **높은 추적 정확도와 신뢰도**를 유지할 수 있습니다.

## 2. NanoOWL 속도 향상을 위한 이미지 Downsampling 전략 평가

NanoOWL의 inference가 병목이라면, **입력 이미지를 다운샘플링**하여 detector 연산을 가볍게 하고, 대신 **detection 결과(bounding box)만 원본 해상도에 매핑**하여 NanoSAM으로 정밀 분할(segmentation)을 수행하는 방법이 제안되었습니다. 이 접근의 **효과**와 **trade-off**를 분석하면 다음과 같습니다.

**(1) Latency 절감 효과:** 입력 해상도를 낮추면 CNN 기반 detector의 연산량이 **대폭 감소**합니다. 일반적으로 **이미지 해상도를 50%로 줄이면 픽셀 수는 1/4로 줄어들어**, 컨볼루션 연산 등에서 **이론상 4배까지 속도 향상**을 기대할 수 있습니다. 실제 YOLOv3에 대한 실험에서도 **해상도를 낮추면 추론 속도가 개선**되어, 적절한 수준의 압축에서는 **다른 성능 지표를 크게 저해하지 않고도** inference time을 줄일 수 있음을 보였습니다. 예컨대, 자율주행 객체 탐지에서 이미지 품질을 약간 낮추거나 크기를 축소하면 **추론 속도가 개선**되지만, 정확도(metric) 하락은 **경미한 수준**에 그친다고 보고되었습니다. 따라서 NanoOWL을 구동할 때 **전역 화각(FoV)은 유지한 채 해상도만 감소**시키면, **대상 객체를 놓치지 않으면서** 연산 시간을 단축할 수 있습니다. Jetson AGX Orin 상에서 구체적인 수치를 가정하면, 1280×720 영상을 640×360으로 줄일 경우 NanoOWL의 FPS가 예를 들어 5fps에서 15fps로 향상되는 식의 **현격한 속도 개선**을 기대할 수 있습니다 (모델 종류와 최적화에 따라 상이).

**(2) Segmentation 정밀도 영향:** 다운샘플링으로 detector가 출력한 bounding box는 원본 대비 다소 **오차**가 있을 수 있지만, 그 영역을 원본 이미지에 대응시켜 **세밀 분할**을 수행하면 **경계 정확도를 회복**할 수 있습니다. 즉, NanoSAM은 원본 고해상도에서 동작하므로 객체의 **미세한 윤곽까지 정확히 분할**해낼 수 있습니다. 이 방식은 1단계에서 해상도 저하로 **탐지 상의 일부 세부정보 손실**을 **2단계에서 만회**하는 효과가 있습니다. 중요한 전제는 detector가 다운샘플 이미지에서 **대략적인 bounding box를 정확히 잡아낼 수 있을 정도로만 해상도를 유지**하는 것입니다. 너무 작은 해상도로 줄이면 작은 객체의 특징이 소실되어 **탐지 누락**이나 오분류가 발생할 수 있고, 이는 이후 segmentation이 아예 틀린 대상에 수행되는 위험을 낳습니다. 다행히도 본 연구 시나리오에서는 로봇이 명령으로 찾는 대상 객체가 **상대적으로 크고 뚜렷**한 물체일 가능성이 높으므로, \*\*적당한 다운샘플링(예: 50\~75% 스케일)\*\*에서도 detection엔 큰 문제가 없을 것입니다. 또한, segmentation은 detector의 box를 초기 프롬프트로 받아 동작하므로, box가 약간 부정확해도 SAM이 내부적으로 객체 경계를 조정해줄 여지가 있습니다 (SAM은 prompt에 근접한 영역의 객체를 자율 분할하는 능력이 있음).

**(3) 유사 연구 및 실험 근거:** 두 단계로 해상도를 달리 쓰는 아이디어는 **일반적인 실시간 인식 파이프라인에서 자주 활용**됩니다. 예를 들어 **Mask R-CNN** 같은 2-stage 모델도 **전역적 object proposal은 저해상도 feature 맵에서** 찾고, **정교한 마스크 분할은 ROI Align으로 일부 고해상도 특징을 활용**하는 구조입니다. 이는 **전역 연산량을 줄이면서도 국부적 정확도는 높이는** 전략입니다. 또한 **FastSAM** 등의 최근 방법도 **이미지 전체에 대한 segmentation을 빠르게 하기 위해, 먼저 저해상도로 candidate masks를 생성한 후 원하는 객체의 마스크를 선택**하는 방식을 취합니다. 이처럼 \*\*한 번에 모든 정보를 고해상도로 처리하지 않고, 필요한 부분만 원본 해상도로 정제(refine)\*\*하는 것은 **실시간 시스템에서 효과적인 최적화**임이 보고되고 있습니다. 한 연구에서는 **초고해상도 이미지를 균일 다운샘플링할 경우 객체 경계 부근 정보가 손실돼 정확도가 떨어질 수 있으나, 중요한 영역을 선별해 세밀처리하면 정확도를 회복할 수 있다**고 제안하였습니다. 본 접근은 바로 detector가 관심 객체 영역을 선별해 주고, SAM이 그 부근을 정밀 처리하는 형태이므로 이러한 **selective high-res refinement**의 이점을 얻을 수 있습니다.

한편, **이미지 화각(FoV)은 유지한 채 해상도만 낮추는 전략**의 장점과 한계를 요약하면:

* **장점:** 카메라 시야 내 **모든 물체를 계속 관찰**하면서 연산량만 감소시키므로, **감지 누락 가능성을 최소화**합니다. (화각을 크롭으로 줄이는 경우 중요한 대상이 화면 밖으로 사라질 위험이 있지만, 해상도 축소는 단지 정보밀도만 낮출 뿐입니다.) 또한 배경 픽셀까지 전부 연산해야 하는 segmentation을 매 프레임 돌리지 않고 **필요할 때만 ROI에 대해 수행**하게 되어 전체 파이프라인이 효율화됩니다. Depth 처리 등 후속 모듈도 작은 ROI에 집중하니 추가 효율이 생깁니다.

* **한계:** **해상도 축소로 인해 작은 객체에 대한 detection 정밀도 감소**는 피하기 어렵습니다. 따라서 모든 경우에 통용될 수 있는 것은 아니며, 상황에 맞는 해상도 선택이 중요합니다. 또한 detection이 약간 부정확한 bounding box를 낼 경우, segmentation 단계에서 원본 이미지를 보더라도 **초기 box 위치가 틀리면 엉뚱한 곳을 분할**할 수 있습니다. 이를 완화하려면, 가능하면 \*\*다운샘플 배율을 크게 무리하지 않는 수준(예: >50% 원본 크기)\*\*으로 유지하고, **detection box에 약간의 margin을 두어** SAM에 입력하는 것이 좋습니다. 이렇게 하면 객체가 박스 경계에 살짝 걸쳤더라도 segmentation 때 완전히 포착할 수 있습니다.

**권장 Downsampling 비율 및 구현:** 실시간 로보틱스 시스템에서는 **경험적으로 원본의 50% 내지 67% 크기(즉, 면적 25\~44%)** 정도로 입력 영상을 줄이는 것을 흔히 고려합니다. 이 정도면 **연산량이 획기적으로 감소**하면서도, 대부분의 중형 물체는 detector가 충분히 인지할 수 있습니다. 예를 들어 1280×720 → 640×360 (50%) 또는 854×480 (67%)으로 축소를 테스트해보고, NanoOWL의 mAP 저하가 허용 가능한 수준인지 확인해야 합니다. **구현 측면**에서는, 카메라에서 입력받은 이미지를 곧바로 해당 크기로 리사이즈하여 NanoOWL에 투입하고, detection으로 나온 bounding box 좌표를 **1/스케일 비율로 환산**하여 원본 이미지 좌표에 맵핑합니다. 그 후 NanoSAM에는 \*\*원본 이미지 크롭 (bounding box 영역)\*\*과 **텍스트/박스 프롬프트**를 전달하여 해당 영역만 정밀 분할하도록 합니다. (SAM 모델은 이미지 전체를 입력받는 구조이므로 가능하다면 해당 ROI만 잘라 입력하는 것이 계산량 면에서 유리합니다. SAM의 연산 역시 입력 사이즈에 선형적으로 비례하기 때문에, ROI가 작으면 훨씬 빠르게 동작할 것입니다.) 마지막으로, 얻어진 세분화 마스크는 원본 좌표계에서 취급되므로 후속 depth 처리 등에 바로 이용하면 됩니다. 이 방식으로 **NanoOWL의 느린 속도를 보완**하면서도 **NanoSAM의 높은 분할 정밀도**를 유지할 수 있을 것으로 예상됩니다.

요약하면, **제안된 다운샘플링 + 원본 매핑 전략은** 실제로 **시스템 지연을 상당히 완화**해 줄 것으로 기대됩니다. 기존 연구와 논문들도 **해상도 감소가 추론 시간에 미치는 긍정적 효과**를 반복해서 보고하고 있습니다. 그리고 segmentation 정밀도는 detection 단계에서 **대상 포착만 제대로 되면 원본 기반으로 복원**되므로 **큰 저하 없이 유지**될 것입니다. 다만, 최적의 비율은 **대상 물체 크기와 카메라 해상도에 의존**하므로, 0.5×, 0.67× 등 몇 가지 후보에서 **탐지 성능 vs. FPS**를 측정하여 결정하는 것이 좋습니다. 실험적으로 검증하면서 **다운샘플링 정도를 adaptive**하게 조절하는 것도 가능할 것입니다. 전체적으로, 입력 영상을 **과도하게 줄이지 않는 한**, 이 두단계 처리 전략은 **실제 실시간 성능 향상에 매우 유효**하며 segmentation 성능 저하도 **미미**할 것으로 분석됩니다.

## 3. 연구 수준 평가 및 적합 학회/저널 제안

&#x20;*그림 1. 자연어 명령 기반 로봇 그리핑 시스템 개요: 사용자가 지시한 객체를 NanoOWL (오픈 vocab 감지)로 찾고 NanoSAM으로 정밀 분할하여 마스크를 획득한다. 부분적 Depth 이미지에 대해 보완(업샘플)하여 Grasping 알고리즘이 엔드 이펙터 자세를 계산하고, 이를 검증 후 로봇 관절 명령으로 변환한다. 이러한 Detect-then-Track + Refine-on-Demand 구조와 Depth Completion 융합이 본 연구의 특징이다.*

본 연구는 \*\*“자연어 명령 기반 로봇 grasping”\*\*이라는 최신 추세의 주제를 다루며, **Vision과 Robotics, AI가 융합**된 시스템을 제안하고 있습니다. 주요 기술 구성은 **NanoOWL (오픈-도메인 객체 검출)**, **NanoSAM (세그먼트 에니싱 기반 분할)**, **Tracking-분할 결합 (DtT & RoD)**, 그리고 **Depth Completion**을 통한 3D 정보 보완 등으로 요약됩니다. 이러한 구성 요소와 기여도를 AI 로보틱스 관점에서 평가하면 다음과 같습니다:

* **Edge Device 기반 실시간 처리:** 대부분의 기존 연구는 고성능 GPU(workstation) 상에서 오픈도메인 인식과 grasping을 구현하는데, 본 연구는 Jetson AGX Orin이라는 엣지 플랫폼에서 **실시간 동작**을 목표로 합니다. 이는 **공학적 도전**이 크지만, 성공 시 **현장 적용 가능성**을 크게 높인다는 의미에서 **실용적 임팩트**가 있습니다. Reviewer들은 “과연 제한된 자원에서 모든 모듈을 실시간으로 돌릴 수 있는가”를 중점 볼 것이므로, 본 연구에서 **Nano 모델 경량화, 최적화 기법, 파이프라인 병렬화** 등으로 이를 달성했다면 **기술적 유의미함**이 있다고 평가할 것입니다.

* **Multimodal 입력 (언어 + 비전 + Depth):** 로봇에게 **텍스트/음성 명령으로 목표를 지정**하고, Vision으로 인식하며, Depth 정보를 활용해 grasping까지 하는 **멀티모달 통합**은 매우 흥미로운 분야입니다. 최근 **open-vocabulary grasping**이나 **language-conditioned manipulation** 연구들이 등장하고 있으며, 해당 분야의 **기술적 난이도**는 높습니다. 본 연구는 복잡한 여러 모듈을 연결하여 이러한 멀티모달 기능을 시연했다는 점에서 **시스템 통합적 기여**가 있습니다. 다만 Reviewer들은 “각 모듈이 기존 기법들(OpenAI CLIP, Owl-ViT, SAM 등)의 단순 조합인지, 아니면 새로운 알고리즘적 개선이 있는지”를 살펴볼 것입니다. **NanoOWL/SAM의 커스터마이징**이나 **Depth Completion의 독창성** 등이 강조된다면 **novelty**를 어필할 수 있습니다.

* **Tracking-Segmentation 통합 (Detect-then-Track & Refine-on-Demand):** 제안된 구조는 **항상 detection+segmentation하는 것이 아닌**, **추적기로 보간하다 필요 시 정밀분할을 수행**하는 **하이브리드 방식**입니다. 이러한 구조 자체가 학술적으로 새로운 알고리즘은 아닐지라도, **실제 로봇 grasping에 이를 적용**한 사례는 드뭅니다. 특히 SAM과 open-vocab 검출기 같은 최신 모델들을 실시간 추적과 엮은 것은 **최신 연구 동향과 맞물린 응용**으로서 reviewer들의 관심을 끌 수 있습니다. Novelty 측면에서는 “추적과 분할을 결합하여 Latency와 Precision을 모두 잡았다”는 **시스템 개선**으로서 평가될 것입니다. 이는 공학적 창의성으로 인정받을 수 있으나, **Top-tier 학회**에서는 보다 정량적인 **성능 향상 입증**(예: 동일 환경 대비 처리 속도 X% 향상, grasp 성공률 Y% 향상 등)이 필요합니다.

* **Depth Completion 적용을 통한 성능 향상:** Depth 센서 (Realsense D435i)의 특성상 **취득하지 못한 영역의 깊이 보완** 또는 **해상도 업샘플링**은 grasp 정확도를 높이는 데 중요합니다. 본 연구가 부분적인 depth 맵에 대해 AI 기반 보간/완성 기술을 적용했다면, 이는 grasping 맥락에서 **실용적 기여**입니다. 과거에 3D 위치 추정 개선을 위해 TSDF나 딥러닝 completion을 사용한 연구들이 있으나, open-vocab 인식과 결합한 사례는 드물기에 **새로운 조합**입니다. Reviewer들은 Depth completion 모듈이 **얼마나 효율적으로 구현되었는지, 실제 grasp 성공률에 기여했는지**를 볼 것입니다. 새로운 네트워크 설계가 아닌 알려진 기법 적용이라면 novelty는 제한적일 수 있지만, **Edge에서 실시간 처리**했다는 점은 어필할 만합니다.

**전반적인 연구 수준 평가:** 해당 연구는 \*\*최신 Vision 모델(SAM, CLIP 계열 등)\*\*을 로봇에 적용하고, **자연어→행동**이라는 도전적인 과제를 다루고 있어 **의의가 큽니다**. 특히 \*\*“임의의 물체를 사람 지시로 잡게 한다”\*\*는 목표는 현재 로보틱스 분야의 뜨거운 관심사이며, 이를 **실시간 임베디드 시스템으로 구현**했다면 **학술적으로도 충분히 의미있는 성과**입니다. Novelty를 엄밀히 따지면, 각 구성요소는 기존에 존재하는 것들의 응용/개선인 측면이 강합니다. 예컨대, NanoOWL/SAM 자체가 새로운 backbone이나 학습법을 제안한 것은 아닐 것입니다 (아마도 기존 모델의 경량 구현). 따라서 **이 연구의 novelty는 “시스템 통합과 최적화” 측면**에 있습니다. 이러한 **시스템 지향 연구**는 \*\*로봇 종합 학회(ICRA/IROS)\*\*에서 환영받는 경향이 있습니다. 반면 **CVPR/NeurIPS 같은 이론 중심 학회**에서는 개별 모듈의 알고리즘적 새로움이 부족하면 임팩트가 낮다고 판단할 수 있습니다. **Reviewer 입장**에서 본 연구의 **novelty**는 “특정 새로운 알고리즘을 발명했다”보다는 “여러 최첨단 기법을 조합하여 새로운 기능을 로봇에 실현했다”에 가깝습니다. **Impact** 면에서는, 성공적으로 구현 및 실험되었다면 **로봇의 인식/행동 능력을 한 단계 확장**한 유용한 사례로 인식될 것입니다. 특히 Edge에서 돌아가는 open-vocab grasping 데모는 **따라하기 어려운 성취**이므로, **실용적 파급력**도 있다고 봅니다. 이런 유형의 기여는 **Robotics 커뮤니티**에서 높이 평가하지만, **Vision/ML 커뮤니티**에서는 상대적으로 낮게 볼 수 있습니다.

이제, 적합한 **국제 학회/저널**을 검토해보겠습니다. 후보로 제시된 ICRA, IROS, RA-L (RA Letters), CoRL, NeurIPS, CVPR를 **요구 기여 수준과 본 연구의 부합도** 측면에서 비교하면 다음과 같습니다:

| **학회/저널** | **초점 및 요구되는 기여** | **본 연구와의 적합성 분석** |
| ------------ | ----------------------- | -------------------------- |
| **ICRA (IEEE)** | 세계 최대 규모 로봇학회. 로봇 시스템, 알고리즘 전반 다룸. <br>**요구:** 신뢰성 있는 실험과 (작더라도) 새로운 시도. 통합 시스템도 환영 | **높음:** 본 연구의 **멀티모달 실시간 로봇 시스템 데모**는 ICRA에 잘 맞습니다. 혁신적 알고리즘이 아니어도, **종합적인 로봇 성능 향상**을 보여주면 ICRA에서는 충분한 기여로 인정받습니다. Edge에서 구동한 점도 **실험적 설계의 창의성**으로 어필 가능 |
| **IROS (IEEE/RSJ)** | ICRA와 쌍벽을 이루는 종합 로봇학회. 범위가 넓고 적용 사례도 중시. <br>**요구:** 참신한 접근 또는 기존 방법의 새로운 적용, 다수의 실험 | **높음:** IROS 역시 **시스템 지향 연구**를 포용하며, 본 연구처럼 **여러 기술 통합**으로 새로운 로봇 능력을 달성한 사례를 환영합니다. ICRA 대비 약간 acceptance rate이 높아 **투고 성공 가능성도 양호**합니다. 다만 ICRA보다 주목도는 약간 낮을 수 있으나, **무난한 선택**입니다 |
| **RA-L (IEEE)** | Robotics and Automation Letters (저널 형식, 짧은 레터). <br>**요구:** 명확하고 참신한 기여를 간결히 제시. 신속한 리뷰/출판. (채택 시 ICRA/IROS 발표 가능) | **중간:** RA-L은 **5\~6페이지 내**에 novelty를 강조해야 하므로, 본 연구처럼 **다소 복합적인 내용을 담기엔 제약**이 있습니다. 그러나 “**Edge기반 실시간 open-vocab grasping**” 등 핵심 키워드로 novelty를 포커싱하면 도전해볼 만합니다. **모듈 통합보다 한두 가지 핵심 기술적 개선**에 초점을 맞춰 작성해야 합니다. 성공하면 ICRA/IROS에서 발표도 가능하므로, 일정이 맞는다면 고려해볼 수 있습니다 |
| **CoRL (NeurIPS 계열)** | Conference on Robot Learning. 로봇+러닝 융합, 알고리즘 혁신 중시. <br>**요구:** 강화학습, 모션학습 등 **학습적 기여** 또는 성능 향상 | **낮음:** CoRL은 **학습 기반 방법론**에 초점이 있으며, 시스템 구현 자체보다는 **학습 알고리즘의 novelty**를 봅니다. 본 연구가 만약 Vision-Language 모델을 **fine-tune**하거나 RL로 grasping policy를 학습했다면 몰라도, 현재 구성은 주로 **모델 응용**입니다. CoRL에 투고하면 **학술적 참신성 어필이 어려울 것**으로 보입니다 |
| **CVPR (IEEE/CVF)** | 컴퓨터비전 최고권위. **새로운 모델, SOTA 성능, 대규모 데이터셋** 등이 요구됨. <br>**요구:** **뚜렷한 비전 알고리즘 기여** (예: 새로운 네트워크, 새로운 벤치마크).          | **낮음:** CVPR 심사위원은 **로봇 어플리케이션**보다는 **비전 알고리즘 혁신**을 찾습니다. 본 연구는 최첨단 모델들을 활용했지만 자체적인 CV 알고리즘 혁신 (예: 새로운 segmentation 네트워크 등)은 없습니다. 시스템 통합만으로는 CVPR **acceptance 가능성 낮습니다**. (만약 NanoSAM 등을 개발한 핵심 알고리즘적인 내용이 있다면 별개이지만, 현재로선 robotics venue가 더 적합) |
| **NeurIPS (Neural Info Proc Sys)** | 기계학습 최고권위. **이론적 또는 범용 ML 기여** 요구. 비전/로봇 응용은 철저히 ML 관점에서 새로워야 | **매우 낮음:** NeurIPS는 **학습 이론, 모델 혁신**이 핵심입니다. 본 연구는 응용 중심이므로 NeurIPS에서 novelty를 인정받기 힘듭니다. 특별한 end-to-end 학습기법이나 새로운 문제정의(benchmark 제시 등)가 없는 한 **부적합**합니다 |

위 비교를 종합하면, **로보틱스 분야의 ICRA나 IROS가 가장 적합한 투고 대상**으로 판단됩니다. 특히 ICRA는 **학계 및 산업계 모두 인지도가 높아**, 본 연구의 **임베디드 실시간 멀티모달 로봇 시스템**이라는 결과를 널리 알리는 데 유리합니다. IROS도 비슷한 성격으로 안전한 선택입니다. **Novelty 측면**에서 다소 시스템 종합에 가깝더라도, ICRA/IROS에서는 \*\*“\~을 구현하여 \~을 가능하게 했다”\*\*는 식의 **실험적 성취**를 높이 사줄 가능성이 큽니다. 반대로 CVPR이나 NeurIPS처럼 **Vision/ML 최상위 학회는 요구 수준이 본 연구 기여와 mismatch**되므로 피하는 것이 좋습니다. 한 가지 전략으로, **RA-L에 투고하여 accept될 경우 ICRA/IROS 발표**로 이어가는 것을 고려할 수 있습니다. RA-L은 빠른 공개가 장점이지만 글자 수 제한 내에 임팩트를 압축해야 하므로, **기술적 핵심 (예: “최초로 Edge에서 \~ 구현”이나 “\~% 성능 향상”)을 부각**하는 서술이 필요합니다. 마지막으로, 분야 특성상 **시연 영상**이나 **데모**가 중요하므로, 어느 학회든 **실제 로봇이 자연어 지시에 따라 물체를 인식하고 집는 영상**을 잘 준비하면 높은 평가를 받을 것입니다. Overall, **ICRA**를 1순위로 추천하며, 준비 기간과 논문 완성도를 고려해 **IROS 또는 RA-L**을 선택지로 삼는 것을 제안합니다.
